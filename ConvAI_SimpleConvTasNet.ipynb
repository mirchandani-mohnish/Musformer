{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN759nzWO3WUnOvADpx12j6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirchandani-mohnish/Musformer/blob/main/ConvAI_SimpleConvTasNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {
        "id": "fJ4msDwbSfnh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1c1LIWJQWk1",
        "outputId": "a4ffa129-8878-4acb-c304-6c2043408f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/musdb18/musdb18 (2)/musdb18'\n",
        "reduced_dataset_path = '/content/drive/MyDrive/musdb18/musdb18'\n"
      ],
      "metadata": {
        "id": "OLb0Shy9SMaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install musdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6PYqRrsSSYQ",
        "outputId": "df91a8dc-a777-46c4-837d-22910ef04dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting musdb\n",
            "  Downloading musdb-0.4.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from musdb) (2.0.2)\n",
            "Collecting stempeg>=0.2.3 (from musdb)\n",
            "  Downloading stempeg-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyaml (from musdb)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from musdb) (4.67.1)\n",
            "Collecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml->musdb) (6.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\n",
            "Downloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyaml, ffmpeg-python, stempeg, musdb\n",
            "Successfully installed ffmpeg-python-0.2.0 musdb-0.4.2 pyaml-25.1.0 stempeg-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "import musdb\n",
        "\n",
        "MUS_DB_PATH = reduced_dataset_path\n",
        "\n",
        "mus = musdb.DB(root=MUS_DB_PATH)\n",
        "mus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\n",
        "mus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\n",
        "mus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\n",
        "print(mus_train[0])\n",
        "print(mus_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1WvqTQqSU0Q",
        "outputId": "61ebf595-8fb4-4184-ec8b-dacf93fad641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANiMAL - Easy Tiger\n",
            "Arise - Run Run Run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installing SpeechBrain via pip\n",
        "BRANCH = 'develop'\n",
        "!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n",
        "\n",
        "# Clone SpeechBrain repository\n",
        "!git clone https://github.com/speechbrain/speechbrain/"
      ],
      "metadata": {
        "id": "xwPWLJHmSePZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "gc7eknq1SqNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torchaudio\n",
        "from speechbrain.utils.data_utils import get_all_files\n",
        "\n",
        "train_files = []\n",
        "valid_files = []\n",
        "test_files = []\n",
        "\n",
        "def create_json(json_file, mus_obj):\n",
        "\n",
        "  json_dict = {}\n",
        "  for i, track in enumerate(mus_obj):\n",
        "    if i % 10 == 0:\n",
        "      print(i)\n",
        "\n",
        "    file_name = track.name\n",
        "    file_path = track.path\n",
        "    file_rate = track.rate\n",
        "    # file_audio = track.audio\n",
        "    # file_vocal = track.targets['vocals'].audio\n",
        "    # print(file_name)\n",
        "    json_dict[file_name] = {\n",
        "              \"file_path\": file_path,\n",
        "              \"file_index\": i,\n",
        "              \"rate\": file_rate\n",
        "      }\n",
        "    # print(json_dict[file_name])\n",
        "\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "# 80% for training\n",
        "create_json(\"train.json\", mus_train)\n",
        "create_json(\"valid.json\", mus_valid)\n",
        "create_json(\"test.json\", mus_test)"
      ],
      "metadata": {
        "id": "bwbQ1BScSpYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f37d10b-90b0-4133-f116-47a3d334ef0d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple ConvTasNet Model\n"
      ],
      "metadata": {
        "id": "ajhBc4M2TSCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file models.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Conv1d, ConvTranspose1d, LayerNorm, Sequential, Dropout, LeakyReLU\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.cnn = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=10, kernel_size=17, stride=2,padding=8),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Conv1d(in_channels=10, out_channels=5, kernel_size=9, stride=8, padding=4),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Conv1d(in_channels=5, out_channels=40, kernel_size=5, stride=10, padding=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code here. Aim for 1-2 lines.\n",
        "        # x = x.permute(0,2, 1)\n",
        "        out = self.cnn(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, inp_channels=40):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.cnn = torch.nn.Sequential(\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.ConvTranspose1d(in_channels=40, out_channels=5, kernel_size=5, stride=10,padding=2, output_padding=9),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.ConvTranspose1d(in_channels=5, out_channels=10, kernel_size=9, stride=8, padding=4, output_padding=7),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.ConvTranspose1d(in_channels=10, out_channels=1, kernel_size=17, stride=2, padding=8, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code here. Aim for 1-2 lines.\n",
        "        out = self.cnn(x)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "OqL0cpAAU3mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fedbe3d-2d61-47af-fb15-83a4e342cb71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file models.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Conv1d, ConvTranspose1d, LayerNorm, Sequential, Dropout, LeakyReLU\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=10, kernel_size=17, stride=2, padding=8),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Conv1d(in_channels=10, out_channels=5, kernel_size=9, stride=8, padding=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Conv1d(in_channels=5, out_channels=40, kernel_size=5, stride=10, padding=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [batch, 1, time]\n",
        "        # Output shape: [batch, 40, time_reduced]\n",
        "        return self.cnn(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels=40, out_channels=5, kernel_size=5, stride=10, padding=2, output_padding=9),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=5, out_channels=10, kernel_size=9, stride=8, padding=4, output_padding=7),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=10, out_channels=1, kernel_size=17, stride=2, padding=8, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [batch, 40, time_reduced]\n",
        "        # Output shape: [batch, 1, time]\n",
        "        return self.cnn(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogOjqCSllgNs",
        "outputId": "9fc91756-4a90-4d09-af12-16acae8b1a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file hparams_convtasnet.yaml\n",
        "\n",
        "# #################################\n",
        "# Training a Convolutional Autoencoder\n",
        "# #################################\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1986\n",
        "__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "output_folder: !ref ./results/Musdb/ConvTasNet/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Path where data manifest files are stored\n",
        "train_annotation: train.json\n",
        "valid_annotation: valid.json\n",
        "test_annotation: test.json\n",
        "\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "\n",
        "# Training Parameters\n",
        "number_of_epochs: 20\n",
        "batch_size: 64\n",
        "lr_start: 0.002\n",
        "lr_final: 0.0002\n",
        "\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "\n",
        "# Encoder\n",
        "encoder: !new:models.Encoder\n",
        "\n",
        "# Decoder\n",
        "decoder: !new:models.Decoder\n",
        "\n",
        "\n",
        "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
        "# which is saved by the Checkpointer so that training can be resumed\n",
        "# if it gets interrupted at any point.\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
        "# device, as well as having train()/eval() called on them by the Brain class.\n",
        "modules:\n",
        "    encoder: !ref <encoder>\n",
        "    decoder: !ref <decoder>\n",
        "\n",
        "# This optimizer will be constructed by the Brain class after all parameters\n",
        "# are moved to the correct device. Then it will be added to the checkpointer.\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_start>\n",
        "\n",
        "# This function manages learning rate annealing over the epochs.\n",
        "# We here use the simple lr annealing method that linearly decreases\n",
        "# the lr from the initial value to the final one.\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr_start>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "loss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n",
        "\n",
        "\n",
        "# This object is used for saving the state of training both so that it\n",
        "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
        "# can be later loaded for evaluation or inference.\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        encoder: !ref <encoder>\n",
        "        decoder: !ref <decoder>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "id": "J76ekIGaTN7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267bc9e0-2596-48c7-e17b-248ba588fe27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hparams_convtasnet.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Recipe for training a convolutional autoencoder with audioMNIST\n",
        "\n",
        "To run this recipe, do the following:\n",
        "> python train.py hparams.yaml\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "import speechbrain as sb\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "import musdb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Brain class for signal reconstruction training\n",
        "class SignalReconstructionBrain(sb.Brain):\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Runs all the computations that transform the input into the output.\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : Tensor\n",
        "            Tensor that contains the predicted signal.\n",
        "        \"\"\"\n",
        "\n",
        "        mix_sig = batch.mix_sig\n",
        "        vocals_sig = batch.vocals_sig\n",
        "\n",
        "\n",
        "\n",
        "        # We first move the batch to the appropriate device.\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "\n",
        "        mix_sig = mix_sig.unsqueeze(1)  # Shape: [batch, 1, time]\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        features = self.modules.encoder(mix_sig)  # Shape: [batch, channels, time]\n",
        "\n",
        "        # Forward pass through the decoder\n",
        "        predictions = self.modules.decoder(features)  # Shape: [batch, 1, time]\n",
        "\n",
        "        # Remove the channel dimension\n",
        "        predictions = predictions.squeeze(1)\n",
        "\n",
        "\n",
        "        wavs = wavs.unsqueeze(1)\n",
        "        # print(wavs.shape)\n",
        "        features = self.modules.encoder(wavs)\n",
        "        # print(features.shape)\n",
        "        predictions = self.modules.decoder(features).squeeze()\n",
        "        print(predictions.shape)\n",
        "\n",
        "\n",
        "        # At test time, we store the samples into the output folder.\n",
        "        # This way we can listen to the reconstructed samples.\n",
        "        if stage == sb.Stage.TEST:\n",
        "          save_path = self.hparams.output_folder+'/samples/'\n",
        "          if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "          for i, id in enumerate(batch.id):\n",
        "            save_file = save_path + id\n",
        "            torchaudio.save(save_file, predictions[i].cpu().unsqueeze(0),16000)\n",
        "            torchaudio.save(save_file.replace('.wav','_original.wav'), wavs[i].cpu(),16000)\n",
        "\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
        "        Arguments\n",
        "        ---------\n",
        "        predictions : tensor\n",
        "            The output tensor from `compute_forward`.\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            A one-element tensor used for backpropagating the gradient.\n",
        "        \"\"\"\n",
        "        target, _ = batch.vocals_sig\n",
        "\n",
        "\n",
        "        # Loss computation.\n",
        "        # Your code here. Aim for 1 line\n",
        "        loss = sb.nnet.losses.mse_loss(predictions, target)\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of each epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "        # Set up statistics trackers for this stage\n",
        "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
        "            metric=sb.nnet.losses.mse_loss\n",
        "        )\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
        "        stage_loss : float\n",
        "            The average loss for all of the dataset processed in this stage.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "        # Store the train loss until the validation stage.\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_loss = stage_loss\n",
        "\n",
        "        # Summarize the statistics from the stage for record-keeping.\n",
        "        else:\n",
        "            stats = {\n",
        "                \"loss\": stage_loss,\n",
        "            }\n",
        "\n",
        "        # At the end of validation...\n",
        "        if stage == sb.Stage.VALID:\n",
        "\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            # The train_logger writes a summary to stdout and to the logfile.\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats={\"loss\": self.train_loss},\n",
        "                valid_stats=stats,\n",
        "            )\n",
        "\n",
        "            # Save the current checkpoint and delete previous checkpoints,\n",
        "            self.checkpointer.save_and_keep_only(meta=stats, min_keys=[\"error\"])\n",
        "\n",
        "        # We also write statistics about test dataset to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "\n",
        "\n",
        "\n",
        "    # # Define audio pipeline\n",
        "    # @sb.utils.data_pipeline.takes(\"file_path\")\n",
        "    # @sb.utils.data_pipeline.provides(\"mix_sig\")\n",
        "    # def audio_pipeline(file_path):\n",
        "    #     stems, rate = stempeg.read_stems(file_path)\n",
        "\n",
        "    #     mix_sig = torch.from_numpy(stems.sum(axis=0)).float()\n",
        "    #     mix_sig = torchaudio.functional.resample(mix_sig, rate, 16000).squeeze(dim=0)\n",
        "    #     sig = torchaudio.functional.resample(sig, fs, 16000).squeeze(dim=0)\n",
        "    #     mix_sig = mix_sig / mix_sig.abs().max()\n",
        "    #     return mix_sig\n",
        "\n",
        "    # @sb.utils.data_pipeline.takes(\"file_path\")\n",
        "    # @sb.utils.data_pipeline.provides(\"mix_sig\")\n",
        "    # def target_pipeline(file_path):\n",
        "    #     stems, rate = stempeg.read_stems(file_path)\n",
        "\n",
        "    #     mix_sig = torch.from_numpy(stems.sum(axis=0)).float()\n",
        "    #     vocals_sig = torch.from_numpy(stems[0]).float()\n",
        "\n",
        "    #     vocals_sig = torchaudio.functional.resample(vocals_sig, rate, 16000).squeeze(dim=0)\n",
        "\n",
        "    #     vocals_sig = vocals_sig / vocals_sig.abs().max()\n",
        "    #     return vocals_sig\n",
        "\n",
        "    # Define audio pipeline for the mixture (source)\n",
        "    @sb.utils.data_pipeline.takes(\"file_index\",\"file_path\")\n",
        "    @sb.utils.data_pipeline.provides(\"mix_sig\", \"vocals_sig\")\n",
        "    def audio_pipeline(file_index,file_path):\n",
        "        \"\"\"Load the mixture and vocals signals from the MUSDB track.\"\"\"\n",
        "        # Load the track using musdb\n",
        "        track = musdb.DB(root='/content/drive/MyDrive/musdb18/musdb18').tracks[file_index]\n",
        "        print(track)\n",
        "\n",
        "        # Get the mixture and vocals signals\n",
        "        mix_sig = torch.from_numpy(track.audio).float()  # Shape: [num_channels, num_samples]\n",
        "        vocals_sig = torch.from_numpy(track.targets['vocals'].audio).float()  # Shape: [num_channels, num_samples]\n",
        "\n",
        "        # Resample to 16 kHz\n",
        "        mix_sig = torchaudio.functional.resample(mix_sig, track.rate, 16000)\n",
        "        vocals_sig = torchaudio.functional.resample(vocals_sig, track.rate, 16000)\n",
        "\n",
        "        # Normalize the waveforms to be between 0 and 1\n",
        "        mix_sig = mix_sig / mix_sig.abs().max()\n",
        "        vocals_sig = vocals_sig / vocals_sig.abs().max()\n",
        "\n",
        "        return mix_sig, vocals_sig\n",
        "\n",
        "\n",
        "    datasets = {}\n",
        "    data_info = {\n",
        "        \"train\": hparams[\"train_annotation\"],\n",
        "        \"valid\": hparams[\"valid_annotation\"],\n",
        "        \"test\": hparams[\"test_annotation\"],\n",
        "    }\n",
        "    hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
        "    for dataset in data_info:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=data_info[dataset],\n",
        "            dynamic_items=[audio_pipeline],\n",
        "            output_keys=[\"id\", \"mix_sig\", \"vocals_sig\"],\n",
        "        )\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Recipe begins!\n",
        "if __name__ == \"__main__\":\n",
        "    # Reading command line arguments.\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training).\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides.\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
        "    datasets = dataio_prep(hparams)\n",
        "\n",
        "    # Initialize the Brain object to prepare for mask training.\n",
        "    brain = SignalReconstructionBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    brain.fit(\n",
        "        epoch_counter=brain.hparams.epoch_counter,\n",
        "        train_set=datasets[\"train\"],\n",
        "        valid_set=datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"error\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk3NsZIIW8R3",
        "outputId": "664a5928-7b7b-47ef-bf9b-1b25afc14f8f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To start from scratch, you need to remove the output folder.\n",
        "# Otherwise, speechbrain starts from the last valid checkpoint.\n",
        "#!rm -rf ./results/AudioMNIST/Autoencoder/\n",
        "\n",
        "!python train.py hparams_convtasnet.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-uTTXHIX1HI",
        "outputId": "9b3c6980-37b7-48aa-e030-f35f4d913f91"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
            "speechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ./results/Musdb/ConvTasNet/1986\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "/usr/local/lib/python3.11/dist-packages/speechbrain/core.py:798: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=gradscaler_enabled)\n",
            "speechbrain.core - SignalReconstructionBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 3.3k\n",
            "* Total Number of Parameters: 3.3k\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "  0% 0/1 [00:00<?, ?it/s]ANiMAL - Easy Tiger\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archives\n"
      ],
      "metadata": {
        "id": "QxyPC3dN2z21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/pyenv/pyenv.git ~/.pyenv\n",
        "\n",
        "# # Set the root for pyenv\n",
        "# os.environ['PYENV_ROOT'] = os.path.expanduser(\"~/.pyenv\")\n",
        "# # Prepend pyenv's bin folder to PATH\n",
        "# os.environ['PATH'] = os.environ['PYENV_ROOT'] + '/bin:' + os.environ['PATH']\n",
        "\n",
        "# # Confirm pyenv is callable\n",
        "# !pyenv --version\n",
        "# !apt-get install libffi-dev\n",
        "# !pyenv install 3.8.13\n",
        "# !pyenv global 3.8.13\n",
        "# !pyenv exec pip install spleeter\n",
        "# !pyenv exec spleeter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "25N5uVuwl0PD",
        "outputId": "173dd609-6b62-492a-8f67-a827cbe943f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/root/.pyenv'...\n",
            "remote: Enumerating objects: 25664, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 25664 (delta 22), reused 19 (delta 15), pack-reused 25622 (from 3)\u001b[K\n",
            "Receiving objects: 100% (25664/25664), 5.90 MiB | 23.53 MiB/s, done.\n",
            "Resolving deltas: 100% (17226/17226), done.\n",
            "pyenv 2.5.4-1-gc579b636\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libffi-dev is already the newest version (3.4.2-4).\n",
            "libffi-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Downloading Python-3.8.13.tar.xz...\n",
            "-> https://www.python.org/ftp/python/3.8.13/Python-3.8.13.tar.xz\n",
            "Installing Python-3.8.13...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8eae14fdfc97>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyenv --version'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'apt-get install libffi-dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyenv install 3.8.13'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyenv global 3.8.13'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyenv exec pip install spleeter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirchandani-mohnish/Musformer/blob/main/ConvAI_SimpleConvTasNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {
        "id": "fJ4msDwbSfnh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1c1LIWJQWk1",
        "outputId": "edd03b12-aa66-41b4-ad26-9c1b23657de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvBA6tWY3Ret",
        "outputId": "500dd83d-fd17-4b37-d66c-71934e0a63f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/musdb18/musdb18 (2)/musdb18'\n",
        "reduced_dataset_path = '/content/drive/MyDrive/musdb18/musdb18'\n"
      ],
      "metadata": {
        "id": "OLb0Shy9SMaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install musdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6PYqRrsSSYQ",
        "outputId": "6e142958-8419-4c14-c46c-c84a25163d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting musdb\n",
            "  Downloading musdb-0.4.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from musdb) (2.0.2)\n",
            "Collecting stempeg>=0.2.3 (from musdb)\n",
            "  Downloading stempeg-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyaml (from musdb)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from musdb) (4.67.1)\n",
            "Collecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml->musdb) (6.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\n",
            "Downloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyaml, ffmpeg-python, stempeg, musdb\n",
            "Successfully installed ffmpeg-python-0.2.0 musdb-0.4.2 pyaml-25.1.0 stempeg-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gll2yXE8No1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "import musdb\n",
        "\n",
        "MUS_DB_PATH = reduced_dataset_path\n",
        "\n",
        "mus = musdb.DB(root=MUS_DB_PATH)\n",
        "mus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\n",
        "mus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\n",
        "mus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\n",
        "print(mus_train[0])\n",
        "print(mus_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "h1WvqTQqSU0Q",
        "outputId": "4263e954-8f23-43de-cfb0-fbee5703d2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2f525a60b315>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmus_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmusdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMUS_DB_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmus_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmusdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMUS_DB_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmus_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmus_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/musdb/__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (NVIDIA GPU) is available\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "# Check if MPS (Apple Metal GPU) is available\n",
        "mps_available = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = torch.device(\n",
        "    \"cuda\" if cuda_available else\n",
        "    \"mps\" if mps_available else\n",
        "    \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY6yxCfTgvfg",
        "outputId": "421eb7c6-cc40-4eb8-b6ea-f933e5748baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installing SpeechBrain via pip\n",
        "BRANCH = 'develop'\n",
        "!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n",
        "\n",
        "# Clone SpeechBrain repository\n",
        "!git clone https://github.com/speechbrain/speechbrain/"
      ],
      "metadata": {
        "id": "xwPWLJHmSePZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "gc7eknq1SqNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import torchaudio\n",
        "# from speechbrain.utils.data_utils import get_all_files\n",
        "\n",
        "# train_files = []\n",
        "# valid_files = []\n",
        "# test_files = []\n",
        "\n",
        "# def create_json(json_file, mus_obj):\n",
        "\n",
        "#   json_dict = {}\n",
        "#   for i, track in enumerate(mus_obj):\n",
        "#     if i % 10 == 0:\n",
        "#       print(i)\n",
        "\n",
        "#     file_name = track.name\n",
        "#     file_path = track.path\n",
        "#     file_rate = track.rate\n",
        "#     # file_audio = track.audio\n",
        "#     # file_vocal = track.targets['vocals'].audio\n",
        "#     # print(file_name)\n",
        "#     json_dict[file_name] = {\n",
        "#               \"file_path\": file_path,\n",
        "#               \"file_index\": i,\n",
        "#               \"rate\": file_rate\n",
        "#       }\n",
        "#     # print(json_dict[file_name])\n",
        "\n",
        "#     with open(json_file, mode=\"w\") as json_f:\n",
        "#         json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "# # 80% for training\n",
        "# create_json(\"train.json\", mus_train)\n",
        "# create_json(\"valid.json\", mus_valid)\n",
        "# create_json(\"test.json\", mus_test)"
      ],
      "metadata": {
        "id": "bwbQ1BScSpYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torchaudio\n",
        "from speechbrain.utils.data_utils import get_all_files\n",
        "\n",
        "train_files = []\n",
        "valid_files = []\n",
        "test_files = []\n",
        "\n",
        "def create_json(json_file, files_path):\n",
        "\n",
        "  json_dict = {}\n",
        "  for track in get_all_files(files_path):\n",
        "\n",
        "    # file_audio = track.audio\n",
        "    # file_vocal = track.targets['vocals'].audio\n",
        "    # print(file_name)\n",
        "    file_name = track.split('/')[-1].split(\"_\")[0]\n",
        "\n",
        "    json_dict[file_name] = {\n",
        "              \"mix_path\": files_path + '/' + file_name + '_mix.wav',\n",
        "              \"vocal_path\": files_path + '/' + file_name + '_vocals.wav'\n",
        "              # \"rate\": file_rate\n",
        "    }\n",
        "    # print(json_dict[file_name])\n",
        "\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "# 80% for training\n",
        "create_json(\"train.json\", reduced_dataset_path + \"/combined_train\")\n",
        "# create_json(\"valid.json\", mus_valid)\n",
        "create_json(\"test.json\", reduced_dataset_path+ \"/combined_test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjFlbPgKQOX_",
        "outputId": "462fed4b-fc45-4bcd-f091-42542ff3e2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eI-hvqQ-JWXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple ConvTasNet Model\n"
      ],
      "metadata": {
        "id": "ajhBc4M2TSCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file models.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Conv1d, ConvTranspose1d, LayerNorm, Sequential, Dropout, LeakyReLU\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.cnn = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=10, kernel_size=17, stride=2,padding=8),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Conv1d(in_channels=10, out_channels=5, kernel_size=9, stride=8, padding=4),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Conv1d(in_channels=5, out_channels=40, kernel_size=5, stride=10, padding=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code here. Aim for 1-2 lines.\n",
        "        # x = x.permute(0,2, 1)\n",
        "        out = self.cnn(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, inp_channels=40):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.cnn = torch.nn.Sequential(\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.ConvTranspose1d(in_channels=40, out_channels=5, kernel_size=5, stride=10,padding=2, output_padding=9),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.ConvTranspose1d(in_channels=5, out_channels=10, kernel_size=9, stride=8, padding=4, output_padding=7),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.ConvTranspose1d(in_channels=10, out_channels=1, kernel_size=17, stride=2, padding=8, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code here. Aim for 1-2 lines.\n",
        "        out = self.cnn(x)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "OqL0cpAAU3mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a22d3d-2833-48b4-f0a7-9bc78ddeb667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file models.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Conv1d, ConvTranspose1d, LayerNorm, Sequential, Dropout, LeakyReLU\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=10, kernel_size=17, stride=2, padding=8),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Conv1d(in_channels=10, out_channels=5, kernel_size=9, stride=8, padding=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Conv1d(in_channels=5, out_channels=40, kernel_size=5, stride=10, padding=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [batch, 1, time]\n",
        "        # Output shape: [batch, 40, time_reduced]\n",
        "        return self.cnn(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels=40, out_channels=5, kernel_size=5, stride=10, padding=2, output_padding=9),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=5, out_channels=10, kernel_size=9, stride=8, padding=4, output_padding=7),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=10, out_channels=1, kernel_size=17, stride=2, padding=8, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [batch, 40, time_reduced]\n",
        "        # Output shape: [batch, 1, time]\n",
        "        return self.cnn(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogOjqCSllgNs",
        "outputId": "70181170-b235-44cc-9a06-e03d922c7886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file hparams_convtasnet.yaml\n",
        "\n",
        "# #################################\n",
        "# Training a Convolutional Autoencoder\n",
        "# #################################\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1986\n",
        "__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "output_folder: !ref ./results/Musdb/ConvTasNet/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Path where data manifest files are stored\n",
        "train_annotation: train.json\n",
        "valid_annotation: valid.json\n",
        "test_annotation: test.json\n",
        "\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "\n",
        "# Training Parameters\n",
        "number_of_epochs: 10\n",
        "batch_size: 5\n",
        "lr_start: 0.0002\n",
        "lr_final: 0.00002\n",
        "max_samples: 12000\n",
        "\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "\n",
        "# Encoder\n",
        "encoder: !new:models.Encoder\n",
        "\n",
        "# Decoder\n",
        "decoder: !new:models.Decoder\n",
        "\n",
        "\n",
        "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
        "# which is saved by the Checkpointer so that training can be resumed\n",
        "# if it gets interrupted at any point.\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
        "# device, as well as having train()/eval() called on them by the Brain class.\n",
        "modules:\n",
        "    encoder: !ref <encoder>\n",
        "    decoder: !ref <decoder>\n",
        "\n",
        "# This optimizer will be constructed by the Brain class after all parameters\n",
        "# are moved to the correct device. Then it will be added to the checkpointer.\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_start>\n",
        "\n",
        "# This function manages learning rate annealing over the epochs.\n",
        "# We here use the simple lr annealing method that linearly decreases\n",
        "# the lr from the initial value to the final one.\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr_start>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "loss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n",
        "\n",
        "\n",
        "# This object is used for saving the state of training both so that it\n",
        "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
        "# can be later loaded for evaluation or inference.\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        encoder: !ref <encoder>\n",
        "        decoder: !ref <decoder>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "id": "J76ekIGaTN7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24cc394-6377-4659-af1b-e0665018ef20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hparams_convtasnet.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Recipe for training a convolutional autoencoder with audioMNIST\n",
        "\n",
        "To run this recipe, do the following:\n",
        "> python train.py hparams.yaml\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "import speechbrain as sb\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "import musdb\n",
        "import stempeg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Brain class for signal reconstruction training\n",
        "class SignalReconstructionBrain(sb.Brain):\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Runs all the computations that transform the input into the output.\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : Tensor\n",
        "            Tensor that contains the predicted signal.\n",
        "        \"\"\"\n",
        "\n",
        "        batch = batch.to(self.device)\n",
        "        mix_sig = batch.mix_sig.data  # Access the tensor data\n",
        "        # vocals_sig = batch.vocals_sig.data  # Access the tensor data\n",
        "\n",
        "        # print(mix_sig.shape)\n",
        "        # print(vocals_sig.shape)\n",
        "\n",
        "        # # We first move the batch to the appropriate device.\n",
        "        # # wavs, lens = batch.sig\n",
        "        mix_sig = mix_sig.permute(0,2,1)\n",
        "        # vocals_sig = vocals_sig.permute(0,2,1)\n",
        "        print(mix_sig.shape)\n",
        "\n",
        "        # print(vocals_sig.shape)\n",
        "        # print(mix_sig)\n",
        "        # mix_sig = mix_sig.unsqueeze(1)\n",
        "        # mix_sig = mix_sig.unsqueeze(1)  # Shape: [batch, 1, time]\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        features = self.modules.encoder(mix_sig)  # Shape: [batch, channels, time]\n",
        "\n",
        "        # Forward pass through the decoder\n",
        "        predictions = self.modules.decoder(features)  # Shape: [batch, 1, time]\n",
        "\n",
        "        # Remove the channel dimension\n",
        "        predictions = predictions.squeeze(1)\n",
        "\n",
        "\n",
        "        # wavs = wavs.unsqueeze(1)\n",
        "        # # print(wavs.shape)\n",
        "        # features = self.modules.encoder(wavs)\n",
        "        # # print(features.shape)\n",
        "        # predictions = self.modules.decoder(features).squeeze()\n",
        "        # print(predictions.shape)\n",
        "\n",
        "\n",
        "        # At test time, we store the samples into the output folder.\n",
        "        # This way we can listen to the reconstructed samples.\n",
        "        if stage == sb.Stage.TEST:\n",
        "          save_path = self.hparams.output_folder+'/samples/'\n",
        "          if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "          for i, id in enumerate(batch.id):\n",
        "            save_file = save_path + id\n",
        "            torchaudio.save(save_file, predictions[i].cpu().unsqueeze(0),16000)\n",
        "            torchaudio.save(save_file.replace('.wav','_original.wav'), wavs[i].cpu(),16000)\n",
        "\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
        "        Arguments\n",
        "        ---------\n",
        "        predictions : tensor\n",
        "            The output tensor from `compute_forward`.\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            A one-element tensor used for backpropagating the gradient.\n",
        "        \"\"\"\n",
        "        target, _ = batch.vocals_sig\n",
        "        print(\"compute obj\")\n",
        "        target = target.permute(0,2,1)\n",
        "        target = target.squeeze(1)\n",
        "        print(target.shape)\n",
        "        print(predictions.shape)\n",
        "\n",
        "        # Loss computation.\n",
        "        # Your code here. Aim for 1 line\n",
        "        # loss = self.hparams.loss(target, predictions)\n",
        "        # loss = torch.nn.MSELoss(predictions,target)\n",
        "\n",
        "        criterion = torch.nn.MSELoss()  # Create the loss function\n",
        "        loss = criterion(predictions, target)\n",
        "        print(loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of each epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "        # Set up statistics trackers for this stage\n",
        "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
        "            metric=sb.nnet.losses.mse_loss\n",
        "        )\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
        "        stage_loss : float\n",
        "            The average loss for all of the dataset processed in this stage.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "        # Store the train loss until the validation stage.\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_loss = stage_loss\n",
        "\n",
        "        # Summarize the statistics from the stage for record-keeping.\n",
        "        else:\n",
        "            stats = {\n",
        "                \"loss\": stage_loss,\n",
        "            }\n",
        "\n",
        "        # At the end of validation...\n",
        "        if stage == sb.Stage.VALID:\n",
        "\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            # The train_logger writes a summary to stdout and to the logfile.\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats={\"loss\": self.train_loss},\n",
        "                valid_stats=stats,\n",
        "            )\n",
        "\n",
        "            # Save the current checkpoint and delete previous checkpoints,\n",
        "            self.checkpointer.save_and_keep_only(meta=stats, min_keys=[\"error\"])\n",
        "\n",
        "        # We also write statistics about test dataset to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "\n",
        "\n",
        "\n",
        "    # # Define audio pipeline\n",
        "    # @sb.utils.data_pipeline.takes(\"file_path\")\n",
        "    # @sb.utils.data_pipeline.provides(\"mix_sig\")\n",
        "    # def audio_pipeline(file_path):\n",
        "    #     print(\"just started\")\n",
        "    #     stems, rate = stempeg.read_stems(file_path)\n",
        "    #     print('reached stems')\n",
        "\n",
        "    #     mix_sig = torch.from_numpy(stems.sum(axis=0)).float()\n",
        "    #     print(mix_sig)\n",
        "    #     mix_sig = torchaudio.functional.resample(mix_sig, rate, 16000).squeeze(dim=0)\n",
        "    #     print(mix_sig)\n",
        "    #     mix_sig = mix_sig / mix_sig.abs().max()\n",
        "    #     print(mix_sig)\n",
        "    #     return mix_sig\n",
        "\n",
        "    # @sb.utils.data_pipeline.takes(\"file_path\")\n",
        "    # @sb.utils.data_pipeline.provides(\"vocals_sig\")\n",
        "    # def target_pipeline(file_path):\n",
        "    #     stems, rate = stempeg.read_stems(file_path)\n",
        "\n",
        "\n",
        "    #     vocals_sig = torch.from_numpy(stems[0]).float()\n",
        "\n",
        "    #     vocals_sig = torchaudio.functional.resample(vocals_sig, rate, 16000).squeeze(dim=0)\n",
        "\n",
        "    #     vocals_sig = vocals_sig / vocals_sig.abs().max()\n",
        "    #     print(vocals_sig)\n",
        "    #     return vocals_sig\n",
        "\n",
        "    # Define audio pipeline for the mixture (source)\n",
        "    # @sb.utils.data_pipeline.takes(\"file_index\",\"file_path\")\n",
        "    # @sb.utils.data_pipeline.provides(\"mix_sig\", \"vocals_sig\")\n",
        "    # def audio_pipeline(file_index,file_path):\n",
        "    #     \"\"\"Load the mixture and vocals signals from the MUSDB track.\"\"\"\n",
        "    #     # Load the track using musdb\n",
        "\n",
        "    #     # Get the mixture and vocals signals\n",
        "    #     mix_sig = torch.from_numpy(track.audio).float()  # Shape: [num_channels, num_samples]\n",
        "    #     vocals_sig = torch.from_numpy(track.targets['vocals'].audio).float()  # Shape: [num_channels, num_samples]\n",
        "\n",
        "    #     # Resample to 16 kHz\n",
        "    #     mix_sig = torchaudio.functional.resample(mix_sig, track.rate, 16000)\n",
        "    #     vocals_sig = torchaudio.functional.resample(vocals_sig, track.rate, 16000)\n",
        "\n",
        "    #     # Normalize the waveforms to be between 0 and 1\n",
        "    #     mix_sig = mix_sig / mix_sig.abs().max()\n",
        "    #     vocals_sig = vocals_sig / vocals_sig.abs().max()\n",
        "\n",
        "    #     return mix_sig, vocals_sig\n",
        "\n",
        "    @sb.utils.data_pipeline.takes(\"mix_path\",\"vocal_path\")\n",
        "    @sb.utils.data_pipeline.provides(\"mix_sig\", \"vocals_sig\")\n",
        "    def audio_pipeline(mix_path, vocal_path):\n",
        "        \"\"\"Load the mixture and vocals signals from .wav files.\"\"\"\n",
        "        # Load the mixture signal\n",
        "        mix_sig, mix_rate = torchaudio.load(mix_path)  # Shape: [num_channels, num_samples]\n",
        "        mix_sig = mix_sig.float()  # Convert to float32\n",
        "\n",
        "        # Load the vocals signal (assuming the vocals file has a similar naming convention)\n",
        "        vocals_sig, vocals_rate = torchaudio.load(vocal_path)  # Shape: [num_channels, num_samples]\n",
        "        vocals_sig = vocals_sig.float()  # Convert to float32\n",
        "\n",
        "        # Resample to 4 kHz (if necessary)\n",
        "        # if mix_rate != 16000:\n",
        "        mix_sig = torchaudio.functional.resample(mix_sig, mix_rate, 4000)\n",
        "        # if vocals_rate != 16000:\n",
        "        vocals_sig = torchaudio.functional.resample(vocals_sig, vocals_rate, 4000)\n",
        "\n",
        "        # Normalize the waveforms to be between 0 and 1\n",
        "        mix_sig = mix_sig / mix_sig.abs().max()\n",
        "        vocals_sig = vocals_sig / vocals_sig.abs().max()\n",
        "\n",
        "\n",
        "        # print(mix_rate)\n",
        "        # print(mix_sig.shape)\n",
        "        # print(vocals_rate)\n",
        "        # print(vocals_sig.shape)\n",
        "\n",
        "        mix_sig = _cap_length(mix_sig, hparams[\"max_samples\"])\n",
        "        vocals_sig = _cap_length(vocals_sig, hparams[\"max_samples\"])\n",
        "\n",
        "        return mix_sig.T, vocals_sig.T\n",
        "\n",
        "\n",
        "\n",
        "    def _cap_length(waveform, max_samples):\n",
        "        \"\"\"Truncate or pad waveform to exactly max_samples.\"\"\"\n",
        "        num_samples = waveform.shape[1]  # Get current number of samples\n",
        "        if num_samples > max_samples:\n",
        "            return waveform[:, :max_samples]  # Truncate\n",
        "        else:\n",
        "            pad = max_samples - num_samples\n",
        "            return torch.nn.functional.pad(waveform, (0, pad))  # Pad with zeros\n",
        "\n",
        "    datasets = {}\n",
        "    data_info = {\n",
        "        \"train\": hparams[\"train_annotation\"],\n",
        "        # \"valid\": hparams[\"valid_annotation\"],\n",
        "        \"test\": hparams[\"test_annotation\"],\n",
        "    }\n",
        "    hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
        "    for dataset in data_info:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=data_info[dataset],\n",
        "            dynamic_items=[audio_pipeline],\n",
        "            output_keys=[\"id\", \"mix_sig\", \"vocals_sig\"],\n",
        "        )\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Recipe begins!\n",
        "if __name__ == \"__main__\":\n",
        "    # Reading command line arguments.\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training).\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides.\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
        "    datasets = dataio_prep(hparams)\n",
        "\n",
        "    # Initialize the Brain object to prepare for mask training.\n",
        "    brain = SignalReconstructionBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    brain.fit(\n",
        "        epoch_counter=brain.hparams.epoch_counter,\n",
        "        train_set=datasets[\"train\"],\n",
        "        # valid_set=datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"error\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk3NsZIIW8R3",
        "outputId": "99d46261-010b-47f0-e944-c4af45906445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To start from scratch, you need to remove the output folder.\n",
        "# Otherwise, speechbrain starts from the last valid checkpoint.\n",
        "#!rm -rf ./results/AudioMNIST/Autoencoder/\n",
        "\n",
        "!python train.py hparams_convtasnet.yaml --device \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-uTTXHIX1HI",
        "outputId": "43c2a663-5417-4aa2-86ce-e3f2ae942c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
            "speechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ./results/Musdb/ConvTasNet/1986\n",
            "speechbrain.core - Gradscaler enabled: `False`\n",
            "speechbrain.core - Using training precision: `--precision=fp32`\n",
            "speechbrain.core - Using evaluation precision: `--eval_precision=fp32`\n",
            "speechbrain.core - SignalReconstructionBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 3.3k\n",
            "* Total Number of Parameters: 3.3k\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:04<00:04,  4.80s/it, train_loss=0.00278]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:13<00:00,  6.61s/it, train_loss=0.00272]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:02<00:02,  2.52s/it, train_loss=0.00255]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:04<00:00,  2.45s/it, train_loss=0.00249]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:03<00:03,  3.46s/it, train_loss=0.00233]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:05<00:00,  2.91s/it, train_loss=0.00228]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:02<00:02,  2.49s/it, train_loss=0.00213]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:04<00:00,  2.41s/it, train_loss=0.00208]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:02<00:02,  2.51s/it, train_loss=0.00194]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:05<00:00,  2.92s/it, train_loss=0.00189]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:02<00:02,  2.52s/it, train_loss=0.00176]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:04<00:00,  2.43s/it, train_loss=0.00171]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:02<00:02,  2.52s/it, train_loss=0.00159]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:04<00:00,  2.44s/it, train_loss=0.00155]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:03<00:03,  3.56s/it, train_loss=0.00143]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:05<00:00,  2.98s/it, train_loss=0.0014]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:02<00:02,  2.53s/it, train_loss=0.00129]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:04<00:00,  2.45s/it, train_loss=0.00126]\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "  0% 0/2 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            " 50% 1/2 [00:02<00:02,  2.48s/it, train_loss=0.00116]torch.Size([5, 1, 12000])\n",
            "compute obj\n",
            "torch.Size([5, 12000])\n",
            "torch.Size([5, 12000])\n",
            "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "100% 2/2 [00:05<00:00,  2.89s/it, train_loss=0.00113]\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "  0% 0/1 [00:00<?, ?it/s]torch.Size([5, 1, 12000])\n",
            "  0% 0/1 [00:10<?, ?it/s]\n",
            "speechbrain.core - Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 346, in <module>\n",
            "    test_stats = brain.evaluate(\n",
            "                 ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1755, in evaluate\n",
            "    loss = self.evaluate_batch(batch, stage=Stage.TEST)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1364, in evaluate_batch\n",
            "    out = self.compute_forward(batch, stage=stage)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/train.py\", line 85, in compute_forward\n",
            "    torchaudio.save(save_file, predictions[i].cpu().unsqueeze(0),16000)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/utils.py\", line 313, in save\n",
            "    return backend.save(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\", line 316, in save\n",
            "    save_audio(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\", line 247, in save_audio\n",
            "    s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torio/io/_streaming_media_encoder.py\", line 199, in __init__\n",
            "    self._s = ffmpeg_ext.StreamingMediaEncoder(str(dst), format)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Failed to open output \"results/Musdb/ConvTasNet/1986/samples/Arise - Run Run Run\" (Invalid argument).\n",
            "Exception raised from get_output_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_writer/stream_writer.cpp:24 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d657b36c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7d657b315a76 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #2: <unknown function> + 0x53464 (0x7d64f7cdb464 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #3: torio::io::StreamingMediaEncoder::StreamingMediaEncoder(std::string const&, std::optional<std::string> const&) + 0x11 (0x7d64f7cdcc61 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #4: <unknown function> + 0x3bac6 (0x7d649d5f2ac6 in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #5: <unknown function> + 0x330c7 (0x7d649d5ea0c7 in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #6: python3() [0x55559b]\n",
            "<omitting python frames>\n",
            "frame #8: python3() [0x58536d]\n",
            "frame #9: python3() [0x56e229]\n",
            "frame #10: python3() [0x52fa60]\n",
            "frame #11: <unknown function> + 0xfc6b (0x7d657bb8dc6b in /usr/local/lib/python3.11/dist-packages/torchaudio/lib/_torchaudio.so)\n",
            "frame #15: python3() [0x56df52]\n",
            "frame #21: python3() [0x6135e4]\n",
            "frame #23: python3() [0x633abb]\n",
            "frame #24: python3() [0x62fd14]\n",
            "frame #25: python3() [0x644375]\n",
            "frame #30: <unknown function> + 0x29d90 (0x7d657c635d90 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #31: __libc_start_main + 0x80 (0x7d657c635e40 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archives\n"
      ],
      "metadata": {
        "id": "QxyPC3dN2z21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/pyenv/pyenv.git ~/.pyenv\n",
        "\n",
        "# # Set the root for pyenv\n",
        "# os.environ['PYENV_ROOT'] = os.path.expanduser(\"~/.pyenv\")\n",
        "# # Prepend pyenv's bin folder to PATH\n",
        "# os.environ['PATH'] = os.environ['PYENV_ROOT'] + '/bin:' + os.environ['PATH']\n",
        "\n",
        "# # Confirm pyenv is callable\n",
        "# !pyenv --version\n",
        "# !apt-get install libffi-dev\n",
        "# !pyenv install 3.8.13\n",
        "# !pyenv global 3.8.13\n",
        "# !pyenv exec pip install spleeter\n",
        "# !pyenv exec spleeter"
      ],
      "metadata": {
        "id": "25N5uVuwl0PD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
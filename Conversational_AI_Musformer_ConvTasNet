{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10005918,"sourceType":"datasetVersion","datasetId":6159348}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T17:17:07.143425Z","iopub.execute_input":"2025-03-31T17:17:07.143689Z","iopub.status.idle":"2025-03-31T17:17:07.522807Z","shell.execute_reply.started":"2025-03-31T17:17:07.143668Z","shell.execute_reply":"2025-03-31T17:17:07.521978Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/musdb18-music-source-separation-dataset/The Long Wait - Dark Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Raft Monk - Tiring.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Too Much.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Georgia Wonder - Siren.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Sunshine Garcia Band - For I Am The Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Enda Reilly - Cur An Long Ag Seol.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Buitraker - Revo X.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/We Fell From The Sky - Not You.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Mountaineering Club - Mallory.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Skelpolu - Resurrection.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Over The Top.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Bobby Nobody - Stitch Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Arise - Run Run Run.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Carlos Gonzalez - A Place For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Forkupines - Semantics.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises - Falcon 69.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Lyndsey Ollard - Catching Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Triviul feat. The Fiend - Widow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Louis Cressy Band - Good Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Motor Tapes - Shore.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/AM Contra - Heart Peripheral.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Signe Jakobsen - What Have You Done To Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Moosmusic - Big Dummy Shake.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/M.E.R.C. Music - Knockout.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Doppler Shift - Atrophy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Detsky Sad - Walkie Talkie.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Happy Daze.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Timboz - Pony.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Oh No.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Mu - Too Bright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Hollow Ground - Ill Fate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises (Baumi) - SDRNR.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Like Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Sambasevam Shanmugam - Kaathaadi.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Tom McKenzie - Directions.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Borderline.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Side Effects Project - Sing With Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Nerve 9 - Pray For The Rain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Zeno - Signs.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Girls Under Glass - We Feel Alright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Cristina Vane - So Easy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Broken Man.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Bulldozer.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Punkdisco - Oral Hygiene.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Al James - Schoolboy Facination.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dark Ride - Burning Bridges.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Drumtracks - Ghost Bitch.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Aimee Norwich - Child.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - If You Say.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rockabilly.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Steven Clark - Bounty.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Giselle - Moss.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Strand Of Oaks - Spacestation.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - Set Me Free.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Bill Chudziak - Children Of No-one.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Angela Thomas Wade - Milk Cow Blues.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Grants - PunchDrunk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Grunge.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Once More (With Feeling).stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Beatles.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Auctioneer - Our Future Faces.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Air Traffic.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - A Reason To Leave.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Districts - Vermont.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Come Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/North To Alaska - All The Same.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Human Mistakes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dreamers Of The Ghetto - Heavy Love.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Rockshow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Faces On Film - Waiting For Ga.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Snowmine - Curfews.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Swinging Steaks - Lost My Way.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Dorothy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Gospel.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Stella.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Disco.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Reggae.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The So So Glos - Emergency.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Wicked.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/St Vitus - Word Gets Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Celestial Shore - Die For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Facade.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/AvaLuna - Waterduct.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Punk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - One Minute Smile.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Blood To Bone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Tim Taler - Stalker.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Hendrix.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Summerghost.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hop Along - Sister Cities.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - All Souls Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - You Listen.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country2.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Clinic A.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Sirens.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Britpop.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Chris Durban - Celebrate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Angelsaint.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - On The Line.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/A Classic Education - NightOwl.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Together Alone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Titanium - Haunted Age.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Goodbye Bolero.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Secret Mountains - High Horse.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Wall Of Death - Femme.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - The Wind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Velvet Curtain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Johnny Lokke - Whisper To A Scream.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - Take A Step.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Jay Menon - Through My Eyes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Flags - 54.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Waltz For My Victims.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Easy Tiger.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Back From The Start.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hollow Ground - Left Blind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Sweet Lights - You Let Me Down.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Port St Willow - Stay Even.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Helado Negro - Mitad Del Mundo.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Black Bloc - If You Want Success.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Pennies.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Voelund - Comfort Lives In Belief.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Nos Palpitants.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Creepoid - OldTree.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - South Of The Water.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Lushlife - Toynbee Suite.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Matthew Entwistle - Dont You Ever.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Scarlet Brand - Les Fleurs Du Mal.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country1.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - Dont Let Go.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - 80s Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Atlantis Bound - It Was My Fault For Waiting.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Invisible Familiars - Disturbing Wildlife.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Cnoc An Tursa - Bannockburn.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hezekiah Jones - Borrowed Heart.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/BigTroubles - Phantom.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Remember December - C U Next Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Night Panther - Fire.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Long Wait - Back Home To Blue.stem.mp4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install musdb\n!pip install mir_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T17:17:07.523908Z","iopub.execute_input":"2025-03-31T17:17:07.524414Z","iopub.status.idle":"2025-03-31T17:17:16.327150Z","shell.execute_reply.started":"2025-03-31T17:17:07.524381Z","shell.execute_reply":"2025-03-31T17:17:16.326304Z"}},"outputs":[{"name":"stdout","text":"Collecting musdb\n  Downloading musdb-0.4.2-py2.py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from musdb) (1.26.4)\nCollecting stempeg>=0.2.3 (from musdb)\n  Downloading stempeg-0.2.3-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pyaml in /usr/local/lib/python3.10/dist-packages (from musdb) (25.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from musdb) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2.4.1)\nCollecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb)\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->musdb) (6.0.2)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.7->musdb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.7->musdb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->musdb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.7->musdb) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.7->musdb) (2024.2.0)\nDownloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\nDownloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nInstalling collected packages: ffmpeg-python, stempeg, musdb\nSuccessfully installed ffmpeg-python-0.2.0 musdb-0.4.2 stempeg-0.2.3\nCollecting mir_eval\n  Downloading mir_eval-0.8.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.26.4)\nRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.13.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mir_eval) (4.4.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.15.4->mir_eval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.15.4->mir_eval) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.15.4->mir_eval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.15.4->mir_eval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.15.4->mir_eval) (2024.2.0)\nDownloading mir_eval-0.8.2-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mir_eval\nSuccessfully installed mir_eval-0.8.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n%%capture\n# Installing SpeechBrain via pip\nBRANCH = 'develop'\n!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n\n# Clone SpeechBrain repository\n!git clone https://github.com/speechbrain/speechbrain/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T17:17:16.329431Z","iopub.execute_input":"2025-03-31T17:17:16.329678Z","iopub.status.idle":"2025-03-31T17:17:36.050380Z","shell.execute_reply.started":"2025-03-31T17:17:16.329657Z","shell.execute_reply":"2025-03-31T17:17:36.049196Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"db_path = '/kaggle/input/musdb18-music-source-separation-dataset'\noutput_path = '/kaggle/working'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T17:17:36.051944Z","iopub.execute_input":"2025-03-31T17:17:36.052253Z","iopub.status.idle":"2025-03-31T17:17:36.055875Z","shell.execute_reply.started":"2025-03-31T17:17:36.052229Z","shell.execute_reply":"2025-03-31T17:17:36.055129Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nnp.float_ = np.float64\nimport musdb\n\nMUS_DB_PATH = db_path\n\nmus = musdb.DB(root=MUS_DB_PATH)\nmus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\nmus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\nmus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\nprint(mus_train[0])\nprint(mus_test[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T17:17:36.056644Z","iopub.execute_input":"2025-03-31T17:17:36.056999Z","iopub.status.idle":"2025-03-31T17:18:12.172715Z","shell.execute_reply.started":"2025-03-31T17:17:36.056962Z","shell.execute_reply":"2025-03-31T17:18:12.171786Z"}},"outputs":[{"name":"stdout","text":"A Classic Education - NightOwl\nAM Contra - Heart Peripheral\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%file hparams.yaml\n# ################################\n# Model: SepFormer for source separation\n# https://arxiv.org/abs/2010.13154\n# Dataset : WSJ0-2mix and WSJ0-3mix\n# ################################\n# Basic parameters\n# Seed needs to be set at top of yaml, before objects with parameters are made\n#\n\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params\n\n# e.g. '/yourpath/wsj0-mix/2speakers'\n# end with 2speakers for wsj0-2mix or 3speakers for wsj0-3mix\ndata_folder: !PLACEHOLDER\n\n# the path for wsj0/si_tr_s/ folder -- only needed if dynamic mixing is used\n# e.g. /yourpath/wsj0-processed/si_tr_s/\nbase_folder_dm: /yourpath/wsj0-processed/si_tr_s/\n\nexperiment_name: convtasnet\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\n\n# Experiment params\nprecision: fp16 # bf16, fp16 or fp32\nnum_spks: 2 # set to 3 for wsj0-3mix\nnoprogressbar: False\nsave_audio: True # Save estimated sources on disk\nsample_rate: 8000\n\n####################### Training Parameters ####################################\nN_epochs: 10\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999  # this is the upper limit for an acceptable loss\n# if True, the training sequences are cut to a specified length\nlimit_training_signal_len: True\n# this is the length of sequences if we choose to limit\n# the signal length of training sequences\ntraining_signal_len: 240000 # shoudl give 30 seconds of audio\n\n# Set it to True to dynamically create mixtures at training time\ndynamic_mixing: False\n\n# Parameters for data augmentation\nuse_wavedrop: False\nuse_speedperturb: True\nuse_rand_shift: False\nmin_shift: -8000\nmax_shift: 8000\n\n# Speed perturbation\nspeed_changes: [95, 100, 105]  # List of speed changes for time-stretching\n\nspeed_perturb: !new:speechbrain.augment.time_domain.SpeedPerturb\n    orig_freq: !ref <sample_rate>\n    speeds: !ref <speed_changes>\n\n# Frequency drop: randomly drops a number of frequency bands to zero.\ndrop_freq_low: 0  # Min frequency band dropout probability\ndrop_freq_high: 1  # Max frequency band dropout probability\ndrop_freq_count_low: 1  # Min number of frequency bands to drop\ndrop_freq_count_high: 3  # Max number of frequency bands to drop\ndrop_freq_width: 0.05  # Width of frequency bands to drop\n\ndrop_freq: !new:speechbrain.augment.time_domain.DropFreq\n    drop_freq_low: !ref <drop_freq_low>\n    drop_freq_high: !ref <drop_freq_high>\n    drop_freq_count_low: !ref <drop_freq_count_low>\n    drop_freq_count_high: !ref <drop_freq_count_high>\n    drop_freq_width: !ref <drop_freq_width>\n\n# Time drop: randomly drops a number of temporal chunks.\ndrop_chunk_count_low: 1  # Min number of audio chunks to drop\ndrop_chunk_count_high: 5  # Max number of audio chunks to drop\ndrop_chunk_length_low: 1000  # Min length of audio chunks to drop\ndrop_chunk_length_high: 2000  # Max length of audio chunks to drop\n\ndrop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n    drop_length_low: !ref <drop_chunk_length_low>\n    drop_length_high: !ref <drop_chunk_length_high>\n    drop_count_low: !ref <drop_chunk_count_low>\n    drop_count_high: !ref <drop_chunk_count_high>\n\n# loss thresholding -- this thresholds the training loss\nthreshold_byloss: True\nthreshold: -30\n\n# Encoder parameters\nN_encoder_out: 256\n# out_channels: 256\nkernel_size: 16\nkernel_stride: 8\n\n# Dataloader options\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 1\n\n\n# Specifying the network\nEncoder: !new:speechbrain.lobes.models.dual_path.Encoder\n    kernel_size: !ref <kernel_size>\n    out_channels: !ref <N_encoder_out>\n\n# intra: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n#    num_layers: 1\n#    input_size: !ref <out_channels>\n#    hidden_channels: !ref <out_channels>\n#    dropout: 0\n#    bidirectional: True\n\n# inter: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n#    num_layers: 1\n#    input_size: !ref <out_channels>\n#    hidden_channels: !ref <out_channels>\n#    dropout: 0\n#    bidirectional: True\n\nMaskNet: !new:speechbrain.lobes.models.conv_tasnet.MaskNet\n    N: 256\n    B: 256\n    H: 512\n    P: 3\n    X: 6\n    R: 4\n    C: !ref <num_spks>\n    norm_type: 'gLN'\n    causal: True\n    mask_nonlinear: 'relu'\n\nDecoder: !new:speechbrain.lobes.models.dual_path.Decoder\n    in_channels: !ref <N_encoder_out>\n    out_channels: 1\n    kernel_size: !ref <kernel_size>\n    stride: !ref <kernel_stride>\n    bias: False\n\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder: !ref <Encoder>\n    decoder: !ref <Decoder>\n    masknet: !ref <MaskNet>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder: !ref <Encoder>\n        decoder: !ref <Decoder>\n        masknet: !ref <MaskNet>\n        counter: !ref <epoch_counter>\n        lr_scheduler: !ref <lr_scheduler>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:15:50.153788Z","iopub.execute_input":"2025-03-31T19:15:50.154172Z","iopub.status.idle":"2025-03-31T19:15:50.160133Z","shell.execute_reply.started":"2025-03-31T19:15:50.154146Z","shell.execute_reply":"2025-03-31T19:15:50.159457Z"}},"outputs":[{"name":"stdout","text":"Overwriting hparams.yaml\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"import musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\n\nclass LazyMusDBDataset(Dataset):\n    def __init__(self, root, subset=\"train\", split=None, target_sr=8000, chunk_size=15):\n        \"\"\"\n        True lazy-loading for MUSDB\n        :param chunk_size: in seconds\n        \"\"\"\n        self.db = musdb.DB(root=root, subsets=subset, split=split, is_wav=False)\n        self.target_sr = target_sr\n        self.chunk_size = chunk_size\n        self.tracks = [{\n            \"path\": track.path,\n            \"duration\": track.duration,\n            \"rate\": track.rate,\n            \"stem_id\": track.stem_id  # Needed for STEM access\n        } for track in self.db.tracks]\n\n    def __len__(self):\n        return len(self.tracks)\n\n    def __getitem__(self, idx):\n        track_info = self.tracks[idx]\n        \n        # Load chunk directly from disk without full track loading\n        def load_stem_chunk(stem_name, random_chunk=True):\n            # MUSDB's internal lazy loading\n            track = self.db.tracks[idx]\n            if stem_name == \"mix\":\n                source = track\n            else:\n                source = track.targets[stem_name]\n            \n            # Calculate chunk bounds\n            if random_chunk:\n                max_start = int(track_info[\"duration\"] * track_info[\"rate\"]) - self.chunk_size * track_info[\"rate\"]\n                start = np.random.randint(0, max(max_start, 1))\n            else:\n                start = 0\n            stop = start + self.chunk_size * track_info[\"rate\"]\n            \n            # Load only the needed segment\n            audio = source.audio[start:stop]\n            \n            # Convert and resample\n            audio_tensor = torch.from_numpy(audio).float().permute(1, 0)\n            return torchaudio.functional.resample(\n                audio_tensor,\n                orig_freq=track_info[\"rate\"],\n                new_freq=self.target_sr\n            ).mean(dim=0, keepdim=False)\n\n        \n        # orig_sr = track.rate  # Original sample rate\n        \n        # chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n    \n        # resampled_chunks = []\n    \n        # for i in range(0, audio_tensor.shape[1], chunk_size):\n        #     chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n        #     resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n        #     resampled_chunks.append(resampled_chunk)\n    \n        # # Concatenate back the processed chunks\n        # # print(\"PROCESSING CHUNKS\")\n        # resampled_audio = torch.cat(resampled_chunks, dim=1)\n        # # print(resampled_audio.shape)\n        # resampled_audio = resampled_audio.mean(dim=0, keepdim=False)\n        # # print(resampled_audio.shape)\n        # # print(resampled_audio.shape)\n        # return resampled_audio\n\n        return {\n            \"mix_sig\": load_stem_chunk(\"mix\"),\n            \"voc_sig\": load_stem_chunk(\"vocals\"),\n            \"inst_sig\": load_stem_chunk(\"accompaniment\"),\n            \"track_id\": track_info[\"stem_id\"]\n        }\n\n# Usage with SpeechBrain\ntrain_data = LazyMusDBDataset(db_path, subset=\"train\", split=\"train\")\nvalid_data = LazyMusDBDataset(db_path, subset=\"train\", split=\"valid\")\ntest_data = LazyMusDBDataset(db_path, subset=\"test\")\n\n# Create DataLoader\ntrain_loader = sb.dataio.dataloader.make_dataloader(\n    train_data,\n    batch_size=1,\n    collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:15:11.586280Z","iopub.execute_input":"2025-03-31T19:15:11.586673Z","iopub.status.idle":"2025-03-31T19:15:29.141712Z","shell.execute_reply.started":"2025-03-31T19:15:11.586641Z","shell.execute_reply":"2025-03-31T19:15:29.141000Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"%%file train.py\n#!/usr/bin/env/python3\n\"\"\"Recipe for training a neural speech separation system on the wsjmix\ndataset. The system employs an encoder, a decoder, and a masking network.\n\nTo run this recipe, do the following:\n> python train.py hparams/sepformer.yaml\n> python train.py hparams/dualpath_rnn.yaml\n> python train.py hparams/convtasnet.yaml\n\nThe experiment file is flexible enough to support different neural\nnetworks. By properly changing the parameter files, you can try\ndifferent architectures. The script supports both wsj2mix and\nwsj3mix.\n\n\nAuthors\n * Cem Subakan 2020\n * Mirco Ravanelli 2020\n * Samuele Cornell 2020\n * Mirko Bronzi 2020\n * Jianyuan Zhong 2020\n\"\"\"\n\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nimport pdb\n\nimport musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.core import AMPConfig\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nimport time\nfrom torch.utils.data import DataLoader\n\nimport musdb\n\n\nclass LazyMusDBDataset(Dataset):\n    def __init__(self, root, subset=\"train\", split=None, target_sr=8000, chunk_size=15):\n        \"\"\"\n        True lazy-loading for MUSDB\n        :param chunk_size: in seconds\n        \"\"\"\n        self.db = musdb.DB(root=root, subsets=subset, split=split, is_wav=False)\n        self.target_sr = target_sr\n        self.chunk_size = chunk_size\n        self.tracks = [{\n            \"path\": track.path,\n            \"duration\": track.duration,\n            \"rate\": track.rate,\n            \"stem_id\": track.stem_id  # Needed for STEM access\n        } for track in self.db.tracks]\n\n    def __len__(self):\n        return len(self.tracks)\n\n    def __getitem__(self, idx):\n        track_info = self.tracks[idx]\n        \n        # Load chunk directly from disk without full track loading\n        def load_stem_chunk(stem_name, random_chunk_val=0):\n            # MUSDB's internal lazy loading\n            track = self.db.tracks[idx]\n            if stem_name == \"mix\":\n                source = track\n            else:\n                source = track.targets[stem_name]\n            \n            # Calculate chunk bounds\n            start = random_chunk_val\n            stop = start + self.chunk_size * track_info[\"rate\"]\n            \n            # Load only the needed segment\n            audio = source.audio[start:stop]\n            \n            # Convert and resample\n            audio_tensor = torch.from_numpy(audio).float().permute(1, 0)\n            return torchaudio.functional.resample(\n                audio_tensor,\n                orig_freq=track_info[\"rate\"],\n                new_freq=self.target_sr\n            ).mean(dim=0, keepdim=False)\n            # chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n        \n            # resampled_chunks = []\n        \n            # for i in range(0, audio_tensor.shape[1], chunk_size):\n            #     chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n            #     resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n            #     resampled_chunks.append(resampled_chunk)\n        \n            # # Concatenate back the processed chunks\n            # # print(\"PROCESSING CHUNKS\")\n            # resampled_audio = torch.cat(resampled_chunks, dim=1)\n            # # print(resampled_audio.shape)\n            # resampled_audio = resampled_audio.mean(dim=0, keepdim=False)\n            # # print(resampled_audio.shape)\n            # # print(resampled_audio.shape)\n            # return resampled_audio\n        # if random_chunk_start_val:\n        max_start = int(track_info[\"duration\"] * track_info[\"rate\"]) - self.chunk_size * track_info[\"rate\"]\n        random_chunk_start_val = np.random.randint(0, max(max_start, 1))\n        \n        return {\n            \"mix_sig\": load_stem_chunk(\"mix\", random_chunk_start_val),\n            \"voc_sig\": load_stem_chunk(\"vocals\",random_chunk_start_val),\n            \"inst_sig\": load_stem_chunk(\"accompaniment\",random_chunk_start_val),\n            \"track_id\": track_info[\"stem_id\"]\n        }\n\n# Define training procedure\nclass Separation(sb.Brain):\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        # print(\"compute forward\")\n        # print(mix)\n        mix, mix_lens = mix\n        \n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)\n        \n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n            dim=-1,\n        ).to(self.device)\n\n        # Add speech distortions\n        if stage == sb.Stage.TRAIN:\n            with torch.no_grad():\n        #         if self.hparams.use_speedperturb:\n        #             mix, targets = self.add_speed_perturb(targets, mix_lens)\n\n        #             mix = targets.sum(-1)\n\n                if self.hparams.use_wavedrop:\n                    mix = self.hparams.drop_chunk(mix, mix_lens)\n                    mix = self.hparams.drop_freq(mix)\n\n                if self.hparams.limit_training_signal_len:\n                    mix, targets = self.cut_signals(mix, targets)\n\n        # Separation\n        mix_w = self.hparams.Encoder(mix)\n        est_mask = self.hparams.MaskNet(mix_w)\n        mix_w = torch.stack([mix_w] * self.hparams.num_spks)\n        sep_h = mix_w * est_mask\n\n        # Decoding\n        est_source = torch.cat(\n            [\n                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n                for i in range(self.hparams.num_spks)\n            ],\n            dim=-1,\n        )\n\n        # T changed after conv1d in encoder, fix it here\n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        # pdb.set_trace()\n        # print(\"compute_forward ended\")\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n        # print(\"INSIDE FIT BATCH\")\n        \n        amp = AMPConfig.from_name(self.precision)\n        should_step = (self.step % self.grad_accumulation_factor) == 0\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig] #mix_sig, voc_sig, inst_sig\n        \n        # time.sleep(1000000) # debugging\n        # with self.no_sync(not should_step):\n        #     if self.use_amp:\n        #         with torch.autocast(\n        #             dtype=amp.dtype, device_type=torch.device(self.device).type\n        #         ):\n        #             predictions, targets = self.compute_forward(\n        #                 mixture, targets, sb.Stage.TRAIN\n        #             )\n        #             print(\"compute forward\")\n        #             loss = self.compute_objectives(predictions, targets)\n\n        #             # hard threshold the easy dataitems\n        #             if self.hparams.threshold_byloss:\n        #                 th = self.hparams.threshold\n        #                 loss = loss[loss > th]\n        #                 if loss.nelement() > 0:\n        #                     loss = loss.mean()\n        #             else:\n        #                 loss = loss.mean()\n\n        #         if (\n        #             loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        #         ):  # the fix for computational problems\n        #             self.scaler.scale(loss).backward()\n        #             if self.hparams.clip_grad_norm >= 0:\n        #                 self.scaler.unscale_(self.optimizer)\n        #                 torch.nn.utils.clip_grad_norm_(\n        #                     self.modules.parameters(),\n        #                     self.hparams.clip_grad_norm,\n        #                 )\n        #             self.scaler.step(self.optimizer)\n        #             self.scaler.update()\n        #         else:\n        #             self.nonfinite_count += 1\n        #             logger.info(\n        #                 \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n        #                     self.nonfinite_count\n        #                 )\n        #             )\n        #             loss.data = torch.tensor(0.0).to(self.device)\n        #     else:\n       \n        \n        predictions, targets = self.compute_forward(\n            mixture, targets, sb.Stage.TRAIN\n        )\n        loss = self.compute_objectives(predictions, targets)\n\n        if self.hparams.threshold_byloss:\n            th = self.hparams.threshold\n            loss = loss[loss > th]\n            if loss.nelement() > 0:\n                loss = loss.mean()\n        else:\n            loss = loss.mean()\n\n        if (\n            loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        ):  # the fix for computational problems\n            loss.backward()\n            if self.hparams.clip_grad_norm >= 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.optimizer.step()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n        # if self.hparams.num_spks == 3:\n        #     targets.append(batch.s3_sig)\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n    # def add_speed_perturb(self, targets, targ_lens):\n    #     \"\"\"Adds speed perturbation and random_shift to the input signals\"\"\"\n\n    #     min_len = -1\n    #     recombine = False\n\n    #     if self.hparams.use_speedperturb or self.hparams.use_rand_shift:\n    #         # Performing speed change (independently on each source)\n    #         new_targets = []\n    #         recombine = True\n\n    #         for i in range(targets.shape[-1]):\n    #             new_target = self.hparams.speed_perturb(targets[:, :, i])\n    #             new_targets.append(new_target)\n    #             if i == 0:\n    #                 min_len = new_target.shape[-1]\n    #             else:\n    #                 if new_target.shape[-1] < min_len:\n    #                     min_len = new_target.shape[-1]\n\n    #         if self.hparams.use_rand_shift:\n    #             # Performing random_shift (independently on each source)\n    #             recombine = True\n    #             for i in range(targets.shape[-1]):\n    #                 rand_shift = torch.randint(\n    #                     self.hparams.min_shift, self.hparams.max_shift, (1,)\n    #                 )\n    #                 new_targets[i] = new_targets[i].to(self.device)\n    #                 new_targets[i] = torch.roll(\n    #                     new_targets[i], shifts=(rand_shift[0],), dims=1\n    #                 )\n\n    #         # Re-combination\n    #         if recombine:\n    #             if self.hparams.use_speedperturb:\n    #                 targets = torch.zeros(\n    #                     targets.shape[0],\n    #                     min_len,\n    #                     targets.shape[-1],\n    #                     device=targets.device,\n    #                     dtype=torch.float,\n    #                 )\n    #             for i, new_target in enumerate(new_targets):\n    #                 targets[:, :, i] = new_targets[i][:, 0:min_len]\n\n    #     mix = targets.sum(-1)\n    #     return mix, targets\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_data):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n        test_loader = sb.dataio.dataloader.make_dataloader(\n            test_data, **self.hparams.dataloader_opts\n        )\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n                   \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_spks, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n\n                    # Compute SDR\n                    sdr, _, _, _ = bss_eval_sources(\n                        targets[0].t().cpu().numpy(),\n                        predictions[0].t().detach().cpu().numpy(),\n                    )\n\n                    sdr_baseline, _, _, _ = bss_eval_sources(\n                        targets[0].t().cpu().numpy(),\n                        mixture_signal[0].t().detach().cpu().numpy(),\n                    )\n\n                    sdr_i = sdr.mean() - sdr_baseline.mean()\n\n                    # Saving on a csv file\n                    row = {\n                        \"snt_id\": snt_id[0],\n                        \"sdr\": sdr.mean(),\n                        \"sdr_i\": sdr_i,\n                        \"si-snr\": -sisnr.item(),\n                        \"si-snr_i\": -sisnr_i.item(),\n                    }\n                    writer.writerow(row)\n\n                    # Metric Accumulation\n                    all_sdrs.append(sdr.mean())\n                    all_sdrs_i.append(sdr_i.mean())\n                    all_sisnrs.append(-sisnr.item())\n                    all_sisnrs_i.append(-sisnr_i.item())\n\n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_spks):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n\n\ndef dataio_prep(hparams):\n    \"\"\"Creates data processing pipeline\"\"\"\n\n    # 1. Define datasets\n\n    # datasets = {}\n    # data_info = {\n    #     \"train\": hparams[\"train_annotation\"],\n    #     \"valid\": hparams[\"valid_annotation\"],\n    #     \"test\": hparams[\"test_annotation\"],\n    # }\n\n        \n    MUS_DB_PATH = hparams[\"db_path\"]\n    \n    mus = musdb.DB(root=MUS_DB_PATH)\n    \n    mus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\n    mus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\n    mus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\n\n\n        \n    def create_json(mus_obj):\n      json_dict = {}\n      for i, track in enumerate(mus_obj):\n        \n        file_name = track.name\n        file_path = track.path\n        file_rate = track.rate\n        \n        json_dict[file_name] = {\n                  \"track\": track\n          }\n        \n        return json_dict\n          \n    train_obj = create_json(mus_train)\n    test_obj = create_json(mus_test)\n    valid_obj = create_json(mus_valid)\n    \n   \n    \n    \n  \n    def convert_musdb_to_torch(track, target_sr=8000, chunk_size_seconds=1):\n        \"\"\"\n        Converts a musdb track to a PyTorch tensor with efficient resampling.\n    \n        Args:\n            track: A musdb track object (e.g., `mus_train[0]`).\n            target_sr (int): The target sampling rate for resampling.\n            chunk_size_seconds (int): Number of seconds per processing chunk.\n    \n        Returns:\n            torch.Tensor: The resampled waveform tensor of shape (num_channels, num_samples).\n        \"\"\"\n        # Convert to tensor and move channels first (PyTorch format)\n        audio_tensor = torch.from_numpy(track.audio).float().permute(1, 0)  # Shape: (num_channels, num_samples)\n        \n        orig_sr = track.rate  # Original sample rate\n        \n        chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n    \n        resampled_chunks = []\n    \n        for i in range(0, audio_tensor.shape[1], chunk_size):\n            chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n            resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n            resampled_chunks.append(resampled_chunk)\n    \n        # Concatenate back the processed chunks\n        # print(\"PROCESSING CHUNKS\")\n        resampled_audio = torch.cat(resampled_chunks, dim=1)\n        # print(resampled_audio.shape)\n        resampled_audio = resampled_audio.mean(dim=0, keepdim=False)\n        # print(resampled_audio.shape)\n        # print(resampled_audio.shape)\n        return resampled_audio\n    \n    \n    @sb.utils.data_pipeline.takes(\"track\")\n    @sb.utils.data_pipeline.provides(\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\")\n    def audio_pipeline_mix(track):\n        # mix_sig = torchaudio.functional.resample(torch.from_numpy(track.audio), track.rate, hparams[\"sample_rate\"])\n\n        # voc_sig = torchaudio.functional.resample(torch.from_numpy(track.targets['vocals'].audio), track.rate, hparams[\"sample_rate\"])\n\n        # inst_sig = torchaudio.functional.resample(torch.from_numpy(track.targets['accompaniment'].audio), track.rate, hparams[\"sample_rate\"])\n         #.squeeze(dim=0) \n        mix_sig = convert_musdb_to_torch(track, hparams[\"sample_rate\"], chunk_size_seconds=1)\n        voc_sig = convert_musdb_to_torch(track.targets[\"vocals\"], hparams[\"sample_rate\"], chunk_size_seconds=1)\n        inst_sig = convert_musdb_to_torch(track.targets[\"accompaniment\"], hparams[\"sample_rate\"], chunk_size_seconds=1)\n        track_id = track.name\n        \n        return track_id, mix_sig, voc_sig, inst_sig\n\n\n    \n    \n    train_data = sb.dataio.dataset.DynamicItemDataset(train_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    valid_data = sb.dataio.dataset.DynamicItemDataset(valid_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    test_data = sb.dataio.dataset.DynamicItemDataset(test_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    datasets = [train_data, valid_data, test_data]\n    \n    \n    return datasets\n    \n    # return\n    # datasets = [train_data, valid_data, test_data]\n\n    # valid_data = sb.dataio.dataset.DynamicItemDataset.from_json(\n    #     json_path=hparams[\"valid_data\"],\n    #     replacements={\"data_root\": hparams[\"data_folder\"]},\n    # )\n\n    # test_data = sb.dataio.dataset.DynamicItemDataset.from_json(\n    #     json_path=hparams[\"test_data\"],\n    #     replacements={\"data_root\": hparams[\"data_folder\"]},\n    # )\n\n    # hparams[\"dataloader_options\"][\"shuffle\"] = False\n    \n\n\n    # @sb.utils.data_pipeline.takes(\"mix_obj\")\n    # @sb.utils.data_pipeline.provides(\"voc\")\n    # def audio_pipeline_vocals(mix_obj):\n    #     voc = torchaudio.functional.resample(mix_obj.targets['vocals'].audio, rate, 16000) #.squeeze(dim=0) \n    #     return voc\n\n    # @sb.utils.data_pipeline.takes(\"mix_obj\")\n    # @sb.utils.data_pipeline.provides(\"inst\")\n    # def audio_pipeline_instrumentals(mix_obj):\n    #     inst = torchaudio.functional.resample(mix_obj.audio, rate, 16000) #.squeeze(dim=0) \n    #     return inst\n\n    # if hparams[\"num_spks\"] == 3:\n\n    #     @sb.utils.data_pipeline.takes(\"s3_wav\")\n    #     @sb.utils.data_pipeline.provides(\"s3_sig\")\n    #     def audio_pipeline_s3(s3_wav):\n    #         s3_sig = sb.dataio.dataio.read_audio(s3_wav)\n    #         return s3_sig\n\n    # sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_mix)\n    # sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_vocals)\n    # sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_instrumentals)\n    # if hparams[\"num_spks\"] == 3:\n    #     sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_s3)\n    #     sb.dataio.dataset.set_output_keys(\n    #         datasets, [\"id\", \"mix_sig\", \"s1_sig\", \"s2_sig\", \"s3_sig\"]\n    #     )\n    # else:\n    # sb.dataio.dataset.set_output_keys(\n    #     datasets, [\"id\", \"mix_sig\", \"voc_sig\", \"inst_sig\"]\n    # )\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n    # Check if wsj0_tr is set with dynamic mixing\n    if hparams[\"dynamic_mixing\"] and not os.path.exists(\n        hparams[\"base_folder_dm\"]\n    ):\n        raise ValueError(\n            \"Please, specify a valid base_folder_dm folder when using dynamic mixing\"\n        )\n\n    # Data preparation\n    # from prepare_data import prepare_wsjmix  # noqa\n\n    # run_on_main(\n    #     # prepare_wsjmix,\n    #     kwargs={\n    #         \"datapath\": hparams[\"data_folder\"],\n    #         \"savepath\": hparams[\"save_folder\"],\n    #         \"n_spks\": hparams[\"num_spks\"],\n    #         \"skip_prep\": hparams[\"skip_prep\"],\n    #         \"fs\": hparams[\"sample_rate\"],\n    #     },\n    # )\n\n    # Create dataset objects\n    # if hparams[\"dynamic_mixing\"]:\n    #     from dynamic_mixing import dynamic_mix_data_prep\n\n    #     # if the base_folder for dm is not processed, preprocess them\n    #     if \"processed\" not in hparams[\"base_folder_dm\"]:\n    #         # if the processed folder already exists we just use it otherwise we do the preprocessing\n    #         if not os.path.exists(\n    #             os.path.normpath(hparams[\"base_folder_dm\"]) + \"_processed\"\n    #         ):\n    #             from preprocess_dynamic_mixing import resample_folder\n\n    #             print(\"Resampling the base folder\")\n    #             run_on_main(\n    #                 resample_folder,\n    #                 kwargs={\n    #                     \"input_folder\": hparams[\"base_folder_dm\"],\n    #                     \"output_folder\": os.path.normpath(\n    #                         hparams[\"base_folder_dm\"]\n    #                     )\n    #                     + \"_processed\",\n    #                     \"fs\": hparams[\"sample_rate\"],\n    #                     \"regex\": \"**/*.wav\",\n    #                 },\n    #             )\n    #             # adjust the base_folder_dm path\n    #             hparams[\"base_folder_dm\"] = (\n    #                 os.path.normpath(hparams[\"base_folder_dm\"]) + \"_processed\"\n    #             )\n    #         else:\n    #             print(\n    #                 \"Using the existing processed folder on the same directory as base_folder_dm\"\n    #             )\n    #             hparams[\"base_folder_dm\"] = (\n    #                 os.path.normpath(hparams[\"base_folder_dm\"]) + \"_processed\"\n    #             )\n\n    #     # Collecting the hparams for dynamic batching\n    #     dm_hparams = {\n    #         \"train_data\": hparams[\"train_data\"],\n    #         \"data_folder\": hparams[\"data_folder\"],\n    #         \"base_folder_dm\": hparams[\"base_folder_dm\"],\n    #         \"sample_rate\": hparams[\"sample_rate\"],\n    #         \"num_spks\": hparams[\"num_spks\"],\n    #         \"training_signal_len\": hparams[\"training_signal_len\"],\n    #         \"dataloader_opts\": hparams[\"dataloader_opts\"],\n    #     }\n    #     train_data = dynamic_mix_data_prep(dm_hparams)\n    #     _, valid_data, test_data = dataio_prep(hparams)\n    # else:\n    # print(dataio_prep(hparams))\n    train_data, valid_data, test_data = dataio_prep(hparams)\n    \n    # print(train_data)\n        \n    # # Load pretrained model if pretrained_separator is present in the yaml\n    # if \"pretrained_separator\" in hparams:\n    #     run_on_main(hparams[\"pretrained_separator\"].collect_files)\n    #     hparams[\"pretrained_separator\"].load_collected()\n\n     \n    # print(type(train_data))\n\n    # Brain class initialization\n    separator = Separation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n    # time.sleep(1000000) # debugging\n    # # # re-initialize the parameters if we don't use a pretrained model\n    # # if \"pretrained_separator\" not in hparams:\n    # #     for module in separator.modules.values():\n    # #         separator.reset_layer_recursively(module)\n\n    # Training\n        \n    # Usage with SpeechBrain\n    train_data = LazyMusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\")\n    valid_data = LazyMusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\")\n    # test_data = LazyMusDBDataset(hparams[\"db_path\"], subset=\"test\")\n    \n\n# Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=1,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=1,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n    # print(\"STARTING FIT\")\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # # Eval\n    separator.evaluate(test_data, min_key=\"si-snr\")\n    separator.save_results(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:19:25.255501Z","iopub.execute_input":"2025-03-31T19:19:25.255857Z","iopub.status.idle":"2025-03-31T19:19:25.265050Z","shell.execute_reply.started":"2025-03-31T19:19:25.255830Z","shell.execute_reply":"2025-03-31T19:19:25.264214Z"}},"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# To start from scratch, you need to remove the output folder.\n# Otherwise, speechbrain starts from the last valid checkpoint.\n#!rm -rf ./results/AudioMNIST/Autoencoder/\n\n# !python train.py hparams.yaml --data_folder=db_path \n!torchrun --standalone --nproc_per_node=2 train.py hparams.yaml --data_folder=db_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:24:57.199128Z","iopub.execute_input":"2025-03-31T19:24:57.199485Z","iopub.status.idle":"2025-03-31T20:13:06.007154Z","shell.execute_reply.started":"2025-03-31T19:24:57.199459Z","shell.execute_reply":"2025-03-31T20:13:06.005992Z"}},"outputs":[{"name":"stdout","text":"W0331 19:24:58.889000 57125 torch/distributed/run.py:793] \nW0331 19:24:58.889000 57125 torch/distributed/run.py:793] *****************************************\nW0331 19:24:58.889000 57125 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0331 19:24:58.889000 57125 torch/distributed/run.py:793] *****************************************\nspeechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/convtasnet/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `True`\nspeechbrain.core - Using training precision: `--precision=fp16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 6.6M\n* Total Number of Parameters: 6.6M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+18-59-33+00\n/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\nspeechbrain.utils.epoch_loop - Going into epoch 6\n100%|██████████████████████████| 81/81 [08:08<00:00,  6.03s/it, train_loss=4.16]\n100%|███████████████████████████████████████████| 13/13 [01:18<00:00,  6.07s/it]\nspeechbrain.utils.train_logger - epoch: 6, lr: 1.50e-04 - train si-snr: 4.16 - valid si-snr: -7.70e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+19-35-14+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+18-43-33+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+18-59-33+00\nspeechbrain.utils.epoch_loop - Going into epoch 7\n100%|███████████████████████████| 81/81 [07:53<00:00,  5.84s/it, train_loss=2.2]\n100%|███████████████████████████████████████████| 13/13 [01:18<00:00,  6.01s/it]\nspeechbrain.utils.train_logger - epoch: 7, lr: 1.50e-04 - train si-snr: 2.20 - valid si-snr: 8.01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+19-44-25+00\nspeechbrain.utils.epoch_loop - Going into epoch 8\n100%|█████████████████████████| 81/81 [07:58<00:00,  5.91s/it, train_loss=0.547]\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.09s/it]\nspeechbrain.utils.train_logger - epoch: 8, lr: 1.50e-04 - train si-snr: 5.47e-01 - valid si-snr: 1.32\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+19-53-43+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+19-44-25+00\nspeechbrain.utils.epoch_loop - Going into epoch 9\n100%|██████████████████████████| 81/81 [08:00<00:00,  5.94s/it, train_loss=1.43]\n100%|███████████████████████████████████████████| 13/13 [01:18<00:00,  6.07s/it]\nspeechbrain.utils.train_logger - epoch: 9, lr: 1.50e-04 - train si-snr: 1.43 - valid si-snr: 3.16\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-03-03+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+19-53-43+00\nspeechbrain.utils.epoch_loop - Going into epoch 10\n100%|████████████████████████| 81/81 [08:00<00:00,  5.93s/it, train_loss=0.0587]\n100%|███████████████████████████████████████████| 13/13 [01:18<00:00,  6.05s/it]\nspeechbrain.utils.train_logger - epoch: 10, lr: 1.50e-04 - train si-snr: 5.87e-02 - valid si-snr: 5.04\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-12-22+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-03-03+00\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+19-35-14+00\n/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.57s/it]\nspeechbrain.utils.train_logger - Epoch loaded: 6 - test si-snr: -5.72e+00\n  0%|                                                     | 0/1 [00:00<?, ?it/s]/kaggle/working/train.py:466: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/train.py:466: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/train.py:471: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/train.py:471: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n100%|█████████████████████████████████████████████| 1/1 [00:26<00:00, 26.90s/it]\n100%|█████████████████████████████████████████████| 1/1 [00:27<00:00, 27.59s/it]\n__main__ - Mean SISNR is 5.724854469299316\n__main__ - Mean SISNRi is 5.733708381652832\n__main__ - Mean SDR is 6.482112375949265\n__main__ - Mean SDRi is 6.48377990273706\n[rank0]:[W331 20:13:04.222590556 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# def convert_musdb_to_torch(track, target_sr=8000, chunk_size_seconds=1):\n#     \"\"\"\n#     Converts a musdb track to a PyTorch tensor with efficient resampling.\n\n#     Args:\n#         track: A musdb track object (e.g., `mus_train[0]`).\n#         target_sr (int): The target sampling rate for resampling.\n#         chunk_size_seconds (int): Number of seconds per processing chunk.\n\n#     Returns:\n#         torch.Tensor: The resampled waveform tensor of shape (num_channels, num_samples).\n#     \"\"\"\n#     # Convert to tensor and move channels first (PyTorch format)\n#     audio_tensor = torch.from_numpy(track.audio).float().permute(1, 0)  # Shape: (num_channels, num_samples)\n    \n#     orig_sr = track.rate  # Original sample rate\n    \n#     chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n\n#     resampled_chunks = []\n\n#     for i in range(0, audio_tensor.shape[1], chunk_size):\n#         chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n#         resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n#         resampled_chunks.append(resampled_chunk)\n\n#     # Concatenate back the processed chunks\n#     resampled_audio = torch.cat(resampled_chunks, dim=1)\n\n#     return resampled_audio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:45:43.776593Z","iopub.execute_input":"2025-03-31T16:45:43.776896Z","iopub.status.idle":"2025-03-31T16:45:43.780570Z","shell.execute_reply.started":"2025-03-31T16:45:43.776872Z","shell.execute_reply":"2025-03-31T16:45:43.779926Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# import soundfile as sf\n\n# def process_and_save_tracks(db, subset_name, output_dir):\n#     \"\"\"\n#     Process tracks from a musdb subset and save vocals, instrumentals, and mix as WAV files\n    \n#     Args:\n#         db: musdb DB object (train, valid, or test)\n#         subset_name: name of the subset ('train', 'valid', or 'test')\n#         output_dir: root directory where files should be saved\n#     \"\"\"\n#     # Create output directory if it doesn't exist\n#     subset_dir = os.path.join(output_dir, subset_name)\n#     os.makedirs(subset_dir, exist_ok=True)\n    \n#     for idx, track in enumerate(db):\n#         print(f\"Processing {subset_name} track {idx+1}/{len(db)}: {track.name}\")\n        \n#         # Get the audio data\n#         vocals = track.targets['vocals'].audio\n#         mix = track.audio\n#         instrumentals = track.targets['accompaniment'].audio\n        \n#         # # Create track-specific directory\n#         # track_dir = os.path.join(subset_dir, track.name)\n#         # os.makedirs(track_dir, exist_ok=True)\n        \n#         # Save files with appropriate names\n#         sf.write(os.path.join(subset_dir, f\"{track.name}_vocals.wav\"), vocals, track.rate)\n#         sf.write(os.path.join(subset_dir, f\"{track.name}_instrumentals.wav\"), instrumentals, track.rate)\n#         sf.write(os.path.join(subset_dir, f\"{track.name}_mix.wav\"), mix, track.rate)\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:45:43.781487Z","iopub.execute_input":"2025-03-31T16:45:43.781770Z","iopub.status.idle":"2025-03-31T16:45:43.798767Z","shell.execute_reply.started":"2025-03-31T16:45:43.781742Z","shell.execute_reply":"2025-03-31T16:45:43.797938Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# output_dir = '/kaggle/working/musdb'\n# process_and_save_tracks(mus_train, \"train\", output_dir + '/train')\n# process_and_save_tracks(mus_valid, \"valid\", output_dir + '/valid')\n# process_and_save_tracks(mus_test, \"test\", output_dir + '/test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:45:43.799596Z","iopub.execute_input":"2025-03-31T16:45:43.799778Z","iopub.status.idle":"2025-03-31T16:45:43.819891Z","shell.execute_reply.started":"2025-03-31T16:45:43.799761Z","shell.execute_reply":"2025-03-31T16:45:43.819288Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# track = mus_train[0]\n# print(track)\n# print(track.audio.shape)\n# import torchaudio\n# import torch\n# import IPython.display as ipd\n# np.float_ = np.float64\n\n# audio_tensor = torch.from_numpy(track.audio).float()\n\n# # audio_tensor = audio_tensor.permute(1, 0)\n# # audio_tensor = audio_tensor.mean(dim=0, keepdim=True)\n# # print(audio_tensor.shape)\n# # mix_sig = torchaudio.functional.resample(torch.from_numpy(track.audio), track.rate, 8000)\n# mix_sig = convert_musdb_to_torch(track, 16000, chunk_size_seconds=1)\n# voc_sig = convert_musdb_to_torch(track.targets['vocals'], 16000, chunk_size_seconds=1)\n# inst_sig = convert_musdb_to_torch(track.targets['accompaniment'], 16000, chunk_size_seconds=1)\n# print(mix_sig)\n# print(mix_sig.shape)\n# # print(torch.Size(mix_sig))\n# ipd.Audio(mix_sig.numpy(), rate=16000)\n# ipd.Audio(voc_sig.numpy(), rate=16000)\n# ipd.Audio(inst_sig.numpy(), rate=16000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:45:43.820973Z","iopub.execute_input":"2025-03-31T16:45:43.821281Z","iopub.status.idle":"2025-03-31T16:45:43.837133Z","shell.execute_reply.started":"2025-03-31T16:45:43.821253Z","shell.execute_reply":"2025-03-31T16:45:43.836480Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# import json\n# import torchaudio\n# from speechbrain.utils.data_utils import get_all_files\n\n# train_files = []\n# valid_files = []\n# test_files = []\n\n# def create_json(json_file, mus_obj):\n\n#   json_dict = {}\n#   for i, track in enumerate(mus_obj):\n#     if i % 10 == 0:\n#       print(i)\n\n#     file_name = track.name\n#     file_path = track.path\n#     file_rate = track.rate\n#     # file_audio = track.audio\n#     # file_vocal = track.targets['vocals'].audio\n#     # print(file_name)\n#     json_dict[file_name] = {\n#               \"file_path\": file_path,\n#               \"rate\": file_rate\n#       }\n#     # print(json_dict[file_name])\n\n#     with open(json_file, mode=\"w\") as json_f:\n#         json.dump(json_dict, json_f, indent=2)\n\n# # 80% for training\n# create_json(os.path.join(output_path, \"train.json\"), mus_train)\n# create_json(os.path.join(output_path, \"valid.json\"), mus_valid)\n# create_json(os.path.join(output_path, \"test.json\"), mus_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:45:43.838141Z","iopub.execute_input":"2025-03-31T16:45:43.838395Z","iopub.status.idle":"2025-03-31T16:45:43.859145Z","shell.execute_reply.started":"2025-03-31T16:45:43.838365Z","shell.execute_reply":"2025-03-31T16:45:43.858316Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/results/convtasnet/1234/save","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:45:43.859879Z","iopub.execute_input":"2025-03-31T16:45:43.860151Z","iopub.status.idle":"2025-03-31T16:45:43.997721Z","shell.execute_reply.started":"2025-03-31T16:45:43.860120Z","shell.execute_reply":"2025-03-31T16:45:43.996952Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/results/convtasnet/1234/save/ (stored 0%)\n","output_type":"stream"}],"execution_count":15}]}
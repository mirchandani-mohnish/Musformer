{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10005918,"sourceType":"datasetVersion","datasetId":6159348}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:40:43.832191Z","iopub.execute_input":"2025-03-31T19:40:43.832529Z","iopub.status.idle":"2025-03-31T19:40:43.870121Z","shell.execute_reply.started":"2025-03-31T19:40:43.832503Z","shell.execute_reply":"2025-03-31T19:40:43.869091Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/musdb18-music-source-separation-dataset/The Long Wait - Dark Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Raft Monk - Tiring.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Too Much.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Georgia Wonder - Siren.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Sunshine Garcia Band - For I Am The Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Enda Reilly - Cur An Long Ag Seol.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Buitraker - Revo X.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/We Fell From The Sky - Not You.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Mountaineering Club - Mallory.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Skelpolu - Resurrection.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Over The Top.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Bobby Nobody - Stitch Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Arise - Run Run Run.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Carlos Gonzalez - A Place For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Forkupines - Semantics.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises - Falcon 69.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Lyndsey Ollard - Catching Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Triviul feat. The Fiend - Widow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Louis Cressy Band - Good Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Motor Tapes - Shore.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/AM Contra - Heart Peripheral.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Signe Jakobsen - What Have You Done To Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Moosmusic - Big Dummy Shake.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/M.E.R.C. Music - Knockout.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Doppler Shift - Atrophy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Detsky Sad - Walkie Talkie.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Happy Daze.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Timboz - Pony.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Oh No.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Mu - Too Bright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Hollow Ground - Ill Fate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises (Baumi) - SDRNR.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Like Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Sambasevam Shanmugam - Kaathaadi.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Tom McKenzie - Directions.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Borderline.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Side Effects Project - Sing With Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Nerve 9 - Pray For The Rain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Zeno - Signs.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Girls Under Glass - We Feel Alright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Cristina Vane - So Easy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Broken Man.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Bulldozer.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Punkdisco - Oral Hygiene.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Al James - Schoolboy Facination.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dark Ride - Burning Bridges.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Drumtracks - Ghost Bitch.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Aimee Norwich - Child.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - If You Say.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rockabilly.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Steven Clark - Bounty.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Giselle - Moss.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Strand Of Oaks - Spacestation.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - Set Me Free.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Bill Chudziak - Children Of No-one.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Angela Thomas Wade - Milk Cow Blues.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Grants - PunchDrunk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Grunge.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Once More (With Feeling).stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Beatles.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Auctioneer - Our Future Faces.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Air Traffic.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - A Reason To Leave.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Districts - Vermont.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Come Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/North To Alaska - All The Same.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Human Mistakes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dreamers Of The Ghetto - Heavy Love.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Rockshow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Faces On Film - Waiting For Ga.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Snowmine - Curfews.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Swinging Steaks - Lost My Way.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Dorothy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Gospel.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Stella.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Disco.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Reggae.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The So So Glos - Emergency.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Wicked.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/St Vitus - Word Gets Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Celestial Shore - Die For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Facade.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/AvaLuna - Waterduct.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Punk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - One Minute Smile.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Blood To Bone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Tim Taler - Stalker.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Hendrix.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Summerghost.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hop Along - Sister Cities.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - All Souls Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - You Listen.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country2.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Clinic A.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Sirens.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Britpop.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Chris Durban - Celebrate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Angelsaint.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - On The Line.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/A Classic Education - NightOwl.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Together Alone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Titanium - Haunted Age.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Goodbye Bolero.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Secret Mountains - High Horse.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Wall Of Death - Femme.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - The Wind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Velvet Curtain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Johnny Lokke - Whisper To A Scream.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - Take A Step.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Jay Menon - Through My Eyes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Flags - 54.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Waltz For My Victims.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Easy Tiger.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Back From The Start.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hollow Ground - Left Blind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Sweet Lights - You Let Me Down.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Port St Willow - Stay Even.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Helado Negro - Mitad Del Mundo.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Black Bloc - If You Want Success.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Pennies.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Voelund - Comfort Lives In Belief.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Nos Palpitants.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Creepoid - OldTree.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - South Of The Water.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Lushlife - Toynbee Suite.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Matthew Entwistle - Dont You Ever.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Scarlet Brand - Les Fleurs Du Mal.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country1.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - Dont Let Go.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - 80s Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Atlantis Bound - It Was My Fault For Waiting.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Invisible Familiars - Disturbing Wildlife.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Cnoc An Tursa - Bannockburn.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hezekiah Jones - Borrowed Heart.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/BigTroubles - Phantom.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Remember December - C U Next Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Night Panther - Fire.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Long Wait - Back Home To Blue.stem.mp4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install musdb\n!pip install mir_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:40:43.871316Z","iopub.execute_input":"2025-03-31T19:40:43.871761Z","iopub.status.idle":"2025-03-31T19:40:52.605566Z","shell.execute_reply.started":"2025-03-31T19:40:43.871716Z","shell.execute_reply":"2025-03-31T19:40:52.604263Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: musdb in /usr/local/lib/python3.10/dist-packages (0.4.2)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from musdb) (1.26.4)\nRequirement already satisfied: stempeg>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from musdb) (0.2.3)\nRequirement already satisfied: pyaml in /usr/local/lib/python3.10/dist-packages (from musdb) (25.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from musdb) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2.4.1)\nRequirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from stempeg>=0.2.3->musdb) (0.2.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->musdb) (6.0.2)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.7->musdb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.7->musdb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->musdb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.7->musdb) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.7->musdb) (2024.2.0)\nRequirement already satisfied: mir_eval in /usr/local/lib/python3.10/dist-packages (0.8.2)\nRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.26.4)\nRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.13.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mir_eval) (4.4.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.15.4->mir_eval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.15.4->mir_eval) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.15.4->mir_eval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.15.4->mir_eval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.15.4->mir_eval) (2024.2.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n%%capture\n# Installing SpeechBrain via pip\nBRANCH = 'develop'\n!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n\n# Clone SpeechBrain repository\n!git clone https://github.com/speechbrain/speechbrain/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:40:52.607671Z","iopub.execute_input":"2025-03-31T19:40:52.608048Z","iopub.status.idle":"2025-03-31T19:41:07.453573Z","shell.execute_reply.started":"2025-03-31T19:40:52.608017Z","shell.execute_reply":"2025-03-31T19:41:07.452165Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"db_path = '/kaggle/input/musdb18-music-source-separation-dataset'\noutput_path = '/kaggle/working'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:42:06.770126Z","iopub.execute_input":"2025-03-31T19:42:06.770515Z","iopub.status.idle":"2025-03-31T19:42:06.775868Z","shell.execute_reply.started":"2025-03-31T19:42:06.770484Z","shell.execute_reply":"2025-03-31T19:42:06.774393Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"%%file hparams.yaml\n# ################################\n# Model: SepFormer for source separation\n# https://arxiv.org/abs/2010.13154\n# Dataset : WSJ0-2mix and WSJ0-3mix\n# ################################\n# Basic parameters\n# Seed needs to be set at top of yaml, before objects with parameters are made\n#\n\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params\n\n# e.g. '/yourpath/wsj0-mix/2speakers'\n# end with 2speakers for wsj0-2mix or 3speakers for wsj0-3mix\ndata_folder: !PLACEHOLDER\n\n# the path for wsj0/si_tr_s/ folder -- only needed if dynamic mixing is used\n# e.g. /yourpath/wsj0-processed/si_tr_s/\nbase_folder_dm: /yourpath/wsj0-processed/si_tr_s/\n\nexperiment_name: convtasnet\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\n\n# Experiment params\nprecision: fp16 # bf16, fp16 or fp32\nnum_spks: 2 # set to 3 for wsj0-3mix\nnoprogressbar: False\nsave_audio: True # Save estimated sources on disk\nsample_rate: 8000\n\n####################### Training Parameters ####################################\nN_epochs: 90\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999  # this is the upper limit for an acceptable loss\n# if True, the training sequences are cut to a specified length\nlimit_training_signal_len: True\n# this is the length of sequences if we choose to limit\n# the signal length of training sequences\ntraining_signal_len: 240000 # shoudl give 30 seconds of audio\n\n# Set it to True to dynamically create mixtures at training time\ndynamic_mixing: False\n\n# Parameters for data augmentation\nuse_wavedrop: False\nuse_speedperturb: True\nuse_rand_shift: False\nmin_shift: -8000\nmax_shift: 8000\n\n# Speed perturbation\nspeed_changes: [95, 100, 105]  # List of speed changes for time-stretching\n\nspeed_perturb: !new:speechbrain.augment.time_domain.SpeedPerturb\n    orig_freq: !ref <sample_rate>\n    speeds: !ref <speed_changes>\n\n# Frequency drop: randomly drops a number of frequency bands to zero.\ndrop_freq_low: 0  # Min frequency band dropout probability\ndrop_freq_high: 1  # Max frequency band dropout probability\ndrop_freq_count_low: 1  # Min number of frequency bands to drop\ndrop_freq_count_high: 3  # Max number of frequency bands to drop\ndrop_freq_width: 0.05  # Width of frequency bands to drop\n\ndrop_freq: !new:speechbrain.augment.time_domain.DropFreq\n    drop_freq_low: !ref <drop_freq_low>\n    drop_freq_high: !ref <drop_freq_high>\n    drop_freq_count_low: !ref <drop_freq_count_low>\n    drop_freq_count_high: !ref <drop_freq_count_high>\n    drop_freq_width: !ref <drop_freq_width>\n\n# Time drop: randomly drops a number of temporal chunks.\ndrop_chunk_count_low: 1  # Min number of audio chunks to drop\ndrop_chunk_count_high: 5  # Max number of audio chunks to drop\ndrop_chunk_length_low: 1000  # Min length of audio chunks to drop\ndrop_chunk_length_high: 2000  # Max length of audio chunks to drop\n\ndrop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n    drop_length_low: !ref <drop_chunk_length_low>\n    drop_length_high: !ref <drop_chunk_length_high>\n    drop_count_low: !ref <drop_chunk_count_low>\n    drop_count_high: !ref <drop_chunk_count_high>\n\n# loss thresholding -- this thresholds the training loss\nthreshold_byloss: True\nthreshold: -30\n\n# Encoder parameters\nN_encoder_out: 256\n# out_channels: 256\nkernel_size: 16\nkernel_stride: 8\n\n# Dataloader options\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 1\n\n\n# Specifying the network\nEncoder: !new:speechbrain.lobes.models.dual_path.Encoder\n    kernel_size: !ref <kernel_size>\n    out_channels: !ref <N_encoder_out>\n\n# intra: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n#    num_layers: 1\n#    input_size: !ref <out_channels>\n#    hidden_channels: !ref <out_channels>\n#    dropout: 0\n#    bidirectional: True\n\n# inter: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n#    num_layers: 1\n#    input_size: !ref <out_channels>\n#    hidden_channels: !ref <out_channels>\n#    dropout: 0\n#    bidirectional: True\n\nMaskNet: !new:speechbrain.lobes.models.conv_tasnet.MaskNet\n    N: 256\n    B: 256\n    H: 512\n    P: 3\n    X: 6\n    R: 4\n    C: !ref <num_spks>\n    norm_type: 'gLN'\n    causal: True\n    mask_nonlinear: 'relu'\n\nDecoder: !new:speechbrain.lobes.models.dual_path.Decoder\n    in_channels: !ref <N_encoder_out>\n    out_channels: 1\n    kernel_size: !ref <kernel_size>\n    stride: !ref <kernel_stride>\n    bias: False\n\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder: !ref <Encoder>\n    decoder: !ref <Decoder>\n    masknet: !ref <MaskNet>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder: !ref <Encoder>\n        decoder: !ref <Decoder>\n        masknet: !ref <MaskNet>\n        counter: !ref <epoch_counter>\n        lr_scheduler: !ref <lr_scheduler>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:42:09.182295Z","iopub.execute_input":"2025-03-31T19:42:09.182682Z","iopub.status.idle":"2025-03-31T19:42:09.191499Z","shell.execute_reply.started":"2025-03-31T19:42:09.182655Z","shell.execute_reply":"2025-03-31T19:42:09.190191Z"}},"outputs":[{"name":"stdout","text":"Writing hparams.yaml\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%file train.py\n#!/usr/bin/env/python3\n\"\"\"Recipe for training a neural speech separation system on the wsjmix\ndataset. The system employs an encoder, a decoder, and a masking network.\n\nTo run this recipe, do the following:\n> python train.py hparams/sepformer.yaml\n> python train.py hparams/dualpath_rnn.yaml\n> python train.py hparams/convtasnet.yaml\n\nThe experiment file is flexible enough to support different neural\nnetworks. By properly changing the parameter files, you can try\ndifferent architectures. The script supports both wsj2mix and\nwsj3mix.\n\n\nAuthors\n * Cem Subakan 2020\n * Mirco Ravanelli 2020\n * Samuele Cornell 2020\n * Mirko Bronzi 2020\n * Jianyuan Zhong 2020\n\"\"\"\n\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nimport pdb\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.core import AMPConfig\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nimport time\nfrom torch.utils.data import DataLoader\n\nimport musdb\n\n\n\n# Define training procedure\nclass Separation(sb.Brain):\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        \n        mix, mix_lens = mix\n        \n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)\n        \n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n            dim=-1,\n        ).to(self.device)\n\n        # Add speech distortions\n        if stage == sb.Stage.TRAIN:\n            with torch.no_grad():\n      \n                if self.hparams.use_wavedrop:\n                    mix = self.hparams.drop_chunk(mix, mix_lens)\n                    mix = self.hparams.drop_freq(mix)\n\n                if self.hparams.limit_training_signal_len:\n                    mix, targets = self.cut_signals(mix, targets)\n\n        # Separation\n        mix_w = self.hparams.Encoder(mix)\n        est_mask = self.hparams.MaskNet(mix_w)\n        mix_w = torch.stack([mix_w] * self.hparams.num_spks)\n        sep_h = mix_w * est_mask\n\n        # Decoding\n        est_source = torch.cat(\n            [\n                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n                for i in range(self.hparams.num_spks)\n            ],\n            dim=-1,\n        )\n\n        # T changed after conv1d in encoder, fix it here\n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        # pdb.set_trace()\n        \n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n\n        \n        amp = AMPConfig.from_name(self.precision)\n        should_step = (self.step % self.grad_accumulation_factor) == 0\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig] #mix_sig, voc_sig, inst_sig\n        predictions, targets = self.compute_forward(\n            mixture, targets, sb.Stage.TRAIN\n        )\n        loss = self.compute_objectives(predictions, targets)\n\n        if self.hparams.threshold_byloss:\n            th = self.hparams.threshold\n            loss = loss[loss > th]\n            if loss.nelement() > 0:\n                loss = loss.mean()\n        else:\n            loss = loss.mean()\n\n        if (\n            loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        ):  # the fix for computational problems\n            loss.backward()\n            if self.hparams.clip_grad_norm >= 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.optimizer.step()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n     \n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_data):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n        test_loader = sb.dataio.dataloader.make_dataloader(\n            test_data, **self.hparams.dataloader_opts\n        )\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n                   \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_spks, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n\n                    # Compute SDR\n                    sdr, _, _, _ = bss_eval_sources(\n                        targets[0].t().cpu().numpy(),\n                        predictions[0].t().detach().cpu().numpy(),\n                    )\n\n                    sdr_baseline, _, _, _ = bss_eval_sources(\n                        targets[0].t().cpu().numpy(),\n                        mixture_signal[0].t().detach().cpu().numpy(),\n                    )\n\n                    sdr_i = sdr.mean() - sdr_baseline.mean()\n\n                    # Saving on a csv file\n                    row = {\n                        \"snt_id\": snt_id[0],\n                        \"sdr\": sdr.mean(),\n                        \"sdr_i\": sdr_i,\n                        \"si-snr\": -sisnr.item(),\n                        \"si-snr_i\": -sisnr_i.item(),\n                    }\n                    writer.writerow(row)\n\n                    # Metric Accumulation\n                    all_sdrs.append(sdr.mean())\n                    all_sdrs_i.append(sdr_i.mean())\n                    all_sisnrs.append(-sisnr.item())\n                    all_sisnrs_i.append(-sisnr_i.item())\n\n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_spks):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\ndef dataio_prep(hparams):\n    \"\"\"Creates data processing pipeline\"\"\"\n\n\n        \n    MUS_DB_PATH = hparams[\"db_path\"]\n    \n    mus = musdb.DB(root=MUS_DB_PATH)\n    \n    mus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\n    mus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\n    mus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\n\n\n        \n    def create_json(mus_obj):\n      json_dict = {}\n      for i, track in enumerate(mus_obj):\n        \n        file_name = track.name\n        file_path = track.path\n        file_rate = track.rate\n        \n        json_dict[file_name] = {\n                  \"track\": track\n          }\n        \n        return json_dict\n          \n    train_obj = create_json(mus_train)\n    test_obj = create_json(mus_test)\n    valid_obj = create_json(mus_valid)\n    \n   \n    \n    \n  \n    def convert_musdb_to_torch(track, target_sr=8000, chunk_size_seconds=1):\n        \"\"\"\n        Converts a musdb track to a PyTorch tensor with efficient resampling.\n    \n        Args:\n            track: A musdb track object (e.g., `mus_train[0]`).\n            target_sr (int): The target sampling rate for resampling.\n            chunk_size_seconds (int): Number of seconds per processing chunk.\n    \n        Returns:\n            torch.Tensor: The resampled waveform tensor of shape (num_channels, num_samples).\n        \"\"\"\n        # Convert to tensor and move channels first (PyTorch format)\n        \n        audio_tensor = torch.from_numpy(track.audio).float().permute(1, 0)  # Shape: (num_channels, num_samples)\n        orig_sr = track.rate  # Original sample rate\n\n        chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n    \n        resampled_chunks = []\n    \n        for i in range(0, audio_tensor.shape[1], chunk_size):\n            chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n            resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n            resampled_chunks.append(resampled_chunk)\n    \n        # Concatenate back the processed chunks\n        # print(\"PROCESSING CHUNKS\")\n        resampled_audio = torch.cat(resampled_chunks, dim=1)\n        # print(resampled_audio.shape)\n        resampled_audio = resampled_audio.mean(dim=0, keepdim=False)\n        # print(resampled_audio.shape)\n        # print(resampled_audio.shape)\n        return resampled_audio\n    \n    \n    @sb.utils.data_pipeline.takes(\"track\")\n    @sb.utils.data_pipeline.provides(\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\")\n    def audio_pipeline_mix(track):\n\n        #  track.chunk_duration = 5.0\n        # track.chunk_start = random.uniform(0, track.duration - track.chunk_duration)\n        \n        # # while True:\n        # #     track = random.choice(mus.tracks)\n        #     track.chunk_duration = 5.0\n        #     track.chunk_start = random.uniform(0, track.duration - track.chunk_duration)\n        #     x = track.audio.T\n        #     y = track.targets['vocals'].audio.T\n        \n            \n        mix_sig = convert_musdb_to_torch(track, hparams[\"sample_rate\"], chunk_size_seconds=1)\n        voc_sig = convert_musdb_to_torch(track.targets[\"vocals\"], hparams[\"sample_rate\"], chunk_size_seconds=1)\n        inst_sig = convert_musdb_to_torch(track.targets[\"accompaniment\"], hparams[\"sample_rate\"], chunk_size_seconds=1)\n        track_id = track.name\n        \n        return track_id, mix_sig, voc_sig, inst_sig\n\n\n    \n    \n    train_data = sb.dataio.dataset.DynamicItemDataset(train_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    valid_data = sb.dataio.dataset.DynamicItemDataset(valid_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    test_data = sb.dataio.dataset.DynamicItemDataset(test_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    datasets = [train_data, valid_data, test_data]\n    \n    \n    return datasets\n    \n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n    # Check if wsj0_tr is set with dynamic mixing\n    if hparams[\"dynamic_mixing\"] and not os.path.exists(\n        hparams[\"base_folder_dm\"]\n    ):\n        raise ValueError(\n            \"Please, specify a valid base_folder_dm folder when using dynamic mixing\"\n        )\n\n    \n    train_data, valid_data, test_data = dataio_prep(hparams)\n   \n     \n    print(type(train_data))\n\n    # Brain class initialization\n    separator = Separation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n  \n    # Training\n    # print(\"STARTING FIT\")\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_data,\n        valid_data,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # # Eval\n    separator.evaluate(test_data, min_key=\"si-snr\")\n    separator.save_results(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T03:14:21.097674Z","iopub.execute_input":"2025-04-01T03:14:21.098107Z","iopub.status.idle":"2025-04-01T03:14:21.111683Z","shell.execute_reply.started":"2025-04-01T03:14:21.098067Z","shell.execute_reply":"2025-04-01T03:14:21.110482Z"}},"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# To start from scratch, you need to remove the output folder.\n# Otherwise, speechbrain starts from the last valid checkpoint.\n#!rm -rf ./results/AudioMNIST/Autoencoder/\n\n!python train.py hparams.yaml --data_folder=db_path --device \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T20:06:34.024567Z","iopub.execute_input":"2025-03-31T20:06:34.025014Z","iopub.status.idle":"2025-04-01T03:14:21.094274Z","shell.execute_reply.started":"2025-03-31T20:06:34.024981Z","shell.execute_reply":"2025-04-01T03:14:21.092096Z"}},"outputs":[{"name":"stdout","text":"speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/convtasnet/1234\n<class 'speechbrain.dataio.dataset.DynamicItemDataset'>\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `False`\nspeechbrain.core - Using training precision: `--precision=bf16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 6.6M\n* Total Number of Parameters: 6.6M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\nspeechbrain.utils.epoch_loop - Going into epoch 1\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [02:09<00:00, 129.92s/it, train_loss=25.2]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:48<00:00, 168.06s/it]\nspeechbrain.utils.train_logger - epoch: 1, lr: 1.50e-04 - train si-snr: 25.21 - valid si-snr: 16.99\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-12-20+00\nspeechbrain.utils.epoch_loop - Going into epoch 2\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:48<00:00, 108.82s/it, train_loss=13.3]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:37<00:00, 157.50s/it]\nspeechbrain.utils.train_logger - epoch: 2, lr: 1.50e-04 - train si-snr: 13.32 - valid si-snr: 11.62\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-16-47+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-12-20+00\nspeechbrain.utils.epoch_loop - Going into epoch 3\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:50<00:00, 110.60s/it, train_loss=8.29]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:46<00:00, 166.58s/it]\nspeechbrain.utils.train_logger - epoch: 3, lr: 1.50e-04 - train si-snr: 8.29 - valid si-snr: 8.29\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-21-24+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-16-47+00\nspeechbrain.utils.epoch_loop - Going into epoch 4\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:50<00:00, 110.31s/it, train_loss=11.6]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:40<00:00, 160.51s/it]\nspeechbrain.utils.train_logger - epoch: 4, lr: 1.50e-04 - train si-snr: 11.58 - valid si-snr: 6.33\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-25-55+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-21-24+00\nspeechbrain.utils.epoch_loop - Going into epoch 5\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:48<00:00, 108.07s/it, train_loss=4.04]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:55<00:00, 175.66s/it]\nspeechbrain.utils.train_logger - epoch: 5, lr: 1.50e-04 - train si-snr: 4.04 - valid si-snr: 4.85\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-30-39+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-25-55+00\nspeechbrain.utils.epoch_loop - Going into epoch 6\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:51<00:00, 111.34s/it, train_loss=5.26]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.23s/it]\nspeechbrain.utils.train_logger - epoch: 6, lr: 1.50e-04 - train si-snr: 5.26 - valid si-snr: 3.77\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-35-13+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-30-39+00\nspeechbrain.utils.epoch_loop - Going into epoch 7\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|████████████████████████████| 1/1 [01:47<00:00, 107.21s/it, train_loss=2.5]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:57<00:00, 177.50s/it]\nspeechbrain.utils.train_logger - epoch: 7, lr: 1.50e-04 - train si-snr: 2.50 - valid si-snr: 2.99\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-39-58+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-35-13+00\nspeechbrain.utils.epoch_loop - Going into epoch 8\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:51<00:00, 111.91s/it, train_loss=7.83]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:40<00:00, 160.86s/it]\nspeechbrain.utils.train_logger - epoch: 8, lr: 1.50e-04 - train si-snr: 7.83 - valid si-snr: 2.38\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-44-31+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-39-58+00\nspeechbrain.utils.epoch_loop - Going into epoch 9\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:47<00:00, 107.76s/it, train_loss=0.471]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:40<00:00, 160.21s/it]\nspeechbrain.utils.train_logger - epoch: 9, lr: 1.50e-04 - train si-snr: 4.71e-01 - valid si-snr: 1.86\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-48-59+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-44-31+00\nspeechbrain.utils.epoch_loop - Going into epoch 10\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:03<00:00, 123.83s/it, train_loss=0.372]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.83s/it]\nspeechbrain.utils.train_logger - epoch: 10, lr: 1.50e-04 - train si-snr: 3.72e-01 - valid si-snr: 1.39\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-53-47+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-48-59+00\nspeechbrain.utils.epoch_loop - Going into epoch 11\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|█████████████████████████| 1/1 [01:51<00:00, 111.43s/it, train_loss=-0.916]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.59s/it]\nspeechbrain.utils.train_logger - epoch: 11, lr: 1.50e-04 - train si-snr: -9.16e-01 - valid si-snr: 1.01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-58-21+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-53-47+00\nspeechbrain.utils.epoch_loop - Going into epoch 12\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:04<00:00, 124.81s/it, train_loss=0.254]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:45<00:00, 165.25s/it]\nspeechbrain.utils.train_logger - epoch: 12, lr: 1.50e-04 - train si-snr: 2.54e-01 - valid si-snr: 6.81e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-03-12+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+20-58-21+00\nspeechbrain.utils.epoch_loop - Going into epoch 13\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:54<00:00, 114.92s/it, train_loss=-1.93]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.46s/it]\nspeechbrain.utils.train_logger - epoch: 13, lr: 1.50e-04 - train si-snr: -1.93e+00 - valid si-snr: 3.59e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-07-51+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-03-12+00\nspeechbrain.utils.epoch_loop - Going into epoch 14\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|█████████████████████████| 1/1 [01:50<00:00, 110.26s/it, train_loss=-0.508]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [03:02<00:00, 182.64s/it]\nspeechbrain.utils.train_logger - epoch: 14, lr: 1.50e-04 - train si-snr: -5.08e-01 - valid si-snr: 7.86e-02\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-12-44+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-07-51+00\nspeechbrain.utils.epoch_loop - Going into epoch 15\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:52<00:00, 112.93s/it, train_loss=-2.27]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.48s/it]\nspeechbrain.utils.train_logger - epoch: 15, lr: 1.50e-04 - train si-snr: -2.27e+00 - valid si-snr: -1.36e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-17-20+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-12-44+00\nspeechbrain.utils.epoch_loop - Going into epoch 16\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:49<00:00, 109.60s/it, train_loss=1.59]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [03:27<00:00, 207.43s/it]\nspeechbrain.utils.train_logger - epoch: 16, lr: 1.50e-04 - train si-snr: 1.59 - valid si-snr: -3.19e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-22-37+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-17-20+00\nspeechbrain.utils.epoch_loop - Going into epoch 17\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:57<00:00, 117.42s/it, train_loss=-1.32]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:45<00:00, 165.29s/it]\nspeechbrain.utils.train_logger - epoch: 17, lr: 1.50e-04 - train si-snr: -1.32e+00 - valid si-snr: -4.86e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-27-20+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-22-37+00\nspeechbrain.utils.epoch_loop - Going into epoch 18\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.57s/it, train_loss=-1.15]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [03:04<00:00, 184.19s/it]\nspeechbrain.utils.train_logger - epoch: 18, lr: 1.50e-04 - train si-snr: -1.15e+00 - valid si-snr: -6.40e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-32-14+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-27-20+00\nspeechbrain.utils.epoch_loop - Going into epoch 19\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:56<00:00, 116.04s/it, train_loss=0.959]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.85s/it]\nspeechbrain.utils.train_logger - epoch: 19, lr: 1.50e-04 - train si-snr: 9.59e-01 - valid si-snr: -7.66e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-36-56+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-32-14+00\nspeechbrain.utils.epoch_loop - Going into epoch 20\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.32s/it, train_loss=-3.03]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.96s/it]\nspeechbrain.utils.train_logger - epoch: 20, lr: 1.50e-04 - train si-snr: -3.03e+00 - valid si-snr: -8.89e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-41-30+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-36-56+00\nspeechbrain.utils.epoch_loop - Going into epoch 21\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|█████████████████████████████| 1/1 [02:09<00:00, 129.70s/it, train_loss=-3]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:45<00:00, 165.25s/it]\nspeechbrain.utils.train_logger - epoch: 21, lr: 1.50e-04 - train si-snr: -3.00e+00 - valid si-snr: -9.72e-01\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-46-25+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-41-30+00\nspeechbrain.utils.epoch_loop - Going into epoch 22\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:54<00:00, 114.81s/it, train_loss=3.47]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.16s/it]\nspeechbrain.utils.train_logger - epoch: 22, lr: 1.50e-04 - train si-snr: 3.47 - valid si-snr: -1.02e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-51-05+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-46-25+00\nspeechbrain.utils.epoch_loop - Going into epoch 23\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:08<00:00, 128.66s/it, train_loss=0.699]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.25s/it]\nspeechbrain.utils.train_logger - epoch: 23, lr: 1.50e-04 - train si-snr: 6.99e-01 - valid si-snr: -1.09e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-55-58+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-51-05+00\nspeechbrain.utils.epoch_loop - Going into epoch 24\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:53<00:00, 113.73s/it, train_loss=0.506]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.92s/it]\nspeechbrain.utils.train_logger - epoch: 24, lr: 1.50e-04 - train si-snr: 5.06e-01 - valid si-snr: -1.13e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-00-37+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+21-55-58+00\nspeechbrain.utils.epoch_loop - Going into epoch 25\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:04<00:00, 124.33s/it, train_loss=-2.24]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:52<00:00, 172.47s/it]\nspeechbrain.utils.train_logger - epoch: 25, lr: 1.50e-04 - train si-snr: -2.24e+00 - valid si-snr: -1.17e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-05-34+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-00-37+00\nspeechbrain.utils.epoch_loop - Going into epoch 26\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:56<00:00, 116.51s/it, train_loss=-1.11]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:45<00:00, 165.97s/it]\nspeechbrain.utils.train_logger - epoch: 26, lr: 1.50e-04 - train si-snr: -1.11e+00 - valid si-snr: -1.22e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-10-17+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-05-34+00\nspeechbrain.utils.epoch_loop - Going into epoch 27\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:50<00:00, 110.02s/it, train_loss=4.65]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [03:03<00:00, 183.32s/it]\nspeechbrain.utils.train_logger - epoch: 27, lr: 1.50e-04 - train si-snr: 4.65 - valid si-snr: -1.23e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-15-10+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-10-17+00\nspeechbrain.utils.epoch_loop - Going into epoch 28\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:58<00:00, 118.40s/it, train_loss=-2.11]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:45<00:00, 165.33s/it]\nspeechbrain.utils.train_logger - epoch: 28, lr: 1.50e-04 - train si-snr: -2.11e+00 - valid si-snr: -1.28e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-19-54+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-15-10+00\nspeechbrain.utils.epoch_loop - Going into epoch 29\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:51<00:00, 111.37s/it, train_loss=-3.32]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [03:03<00:00, 183.62s/it]\nspeechbrain.utils.train_logger - epoch: 29, lr: 1.50e-04 - train si-snr: -3.32e+00 - valid si-snr: -1.32e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-24-50+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-19-54+00\nspeechbrain.utils.epoch_loop - Going into epoch 30\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:53<00:00, 113.88s/it, train_loss=-3.47]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.09s/it]\nspeechbrain.utils.train_logger - epoch: 30, lr: 1.50e-04 - train si-snr: -3.47e+00 - valid si-snr: -1.37e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-29-28+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-24-50+00\nspeechbrain.utils.epoch_loop - Going into epoch 31\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.19s/it, train_loss=-2.91]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [03:00<00:00, 180.15s/it]\nspeechbrain.utils.train_logger - epoch: 31, lr: 1.50e-04 - train si-snr: -2.91e+00 - valid si-snr: -1.45e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-34-18+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-29-28+00\nspeechbrain.utils.epoch_loop - Going into epoch 32\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:53<00:00, 113.03s/it, train_loss=-3.6]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.24s/it]\nspeechbrain.utils.train_logger - epoch: 32, lr: 1.50e-04 - train si-snr: -3.60e+00 - valid si-snr: -1.52e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-38-53+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-34-18+00\nspeechbrain.utils.epoch_loop - Going into epoch 33\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:55<00:00, 115.28s/it, train_loss=-4.06]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.36s/it]\nspeechbrain.utils.train_logger - epoch: 33, lr: 1.50e-04 - train si-snr: -4.06e+00 - valid si-snr: -1.58e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-43-31+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-38-53+00\nspeechbrain.utils.epoch_loop - Going into epoch 34\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:05<00:00, 125.05s/it, train_loss=-1.25]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.86s/it]\nspeechbrain.utils.train_logger - epoch: 34, lr: 1.50e-04 - train si-snr: -1.25e+00 - valid si-snr: -1.63e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-48-19+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-43-31+00\nspeechbrain.utils.epoch_loop - Going into epoch 35\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.61s/it, train_loss=-3.57]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:41<00:00, 161.76s/it]\nspeechbrain.utils.train_logger - epoch: 35, lr: 1.50e-04 - train si-snr: -3.57e+00 - valid si-snr: -1.68e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-52-52+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-48-19+00\nspeechbrain.utils.epoch_loop - Going into epoch 36\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|█████████████████████████████| 1/1 [01:50<00:00, 110.15s/it, train_loss=-4]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.49s/it]\nspeechbrain.utils.train_logger - epoch: 36, lr: 1.50e-04 - train si-snr: -4.00e+00 - valid si-snr: -1.71e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-57-25+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-52-52+00\nspeechbrain.utils.epoch_loop - Going into epoch 37\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:49<00:00, 109.02s/it, train_loss=16.5]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:44<00:00, 164.75s/it]\nspeechbrain.utils.train_logger - epoch: 37, lr: 1.50e-04 - train si-snr: 16.54 - valid si-snr: -1.70e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-01-59+00\nspeechbrain.utils.epoch_loop - Going into epoch 38\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.34s/it, train_loss=-4.03]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.67s/it]\nspeechbrain.utils.train_logger - epoch: 38, lr: 1.50e-04 - train si-snr: -4.03e+00 - valid si-snr: -1.73e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-06-32+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-01-59+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+22-57-25+00\nspeechbrain.utils.epoch_loop - Going into epoch 39\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:59<00:00, 119.71s/it, train_loss=5.32]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.05s/it]\nspeechbrain.utils.train_logger - epoch: 39, lr: 1.50e-04 - train si-snr: 5.32 - valid si-snr: -1.75e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-11-16+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-06-32+00\nspeechbrain.utils.epoch_loop - Going into epoch 40\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:12<00:00, 132.57s/it, train_loss=-1.74]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.71s/it]\nspeechbrain.utils.train_logger - epoch: 40, lr: 1.50e-04 - train si-snr: -1.74e+00 - valid si-snr: -1.76e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-16-11+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-11-16+00\nspeechbrain.utils.epoch_loop - Going into epoch 41\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.75s/it, train_loss=-1.03]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:41<00:00, 161.44s/it]\nspeechbrain.utils.train_logger - epoch: 41, lr: 1.50e-04 - train si-snr: -1.03e+00 - valid si-snr: -1.79e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-20-43+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-16-11+00\nspeechbrain.utils.epoch_loop - Going into epoch 42\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:48<00:00, 108.56s/it, train_loss=26.4]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:41<00:00, 161.54s/it]\nspeechbrain.utils.train_logger - epoch: 42, lr: 1.50e-04 - train si-snr: 26.43 - valid si-snr: -1.76e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-25-13+00\nspeechbrain.utils.epoch_loop - Going into epoch 43\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:48<00:00, 108.61s/it, train_loss=1.32]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.01s/it]\nspeechbrain.utils.train_logger - epoch: 43, lr: 1.50e-04 - train si-snr: 1.32 - valid si-snr: -1.76e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-29-45+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-25-13+00\nspeechbrain.utils.epoch_loop - Going into epoch 44\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.60s/it, train_loss=-4.22]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:41<00:00, 161.85s/it]\nspeechbrain.utils.train_logger - epoch: 44, lr: 1.50e-04 - train si-snr: -4.22e+00 - valid si-snr: -1.78e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-34-17+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-29-45+00\nspeechbrain.utils.epoch_loop - Going into epoch 45\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.07s/it, train_loss=-4.04]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.32s/it]\nspeechbrain.utils.train_logger - epoch: 45, lr: 1.50e-04 - train si-snr: -4.04e+00 - valid si-snr: -1.78e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-38-49+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-34-17+00\nspeechbrain.utils.epoch_loop - Going into epoch 46\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.99s/it, train_loss=-4.56]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:48<00:00, 168.88s/it]\nspeechbrain.utils.train_logger - epoch: 46, lr: 1.50e-04 - train si-snr: -4.56e+00 - valid si-snr: -1.78e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-43-28+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-38-49+00\nspeechbrain.utils.epoch_loop - Going into epoch 47\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:48<00:00, 108.90s/it, train_loss=-3.93]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.87s/it]\nspeechbrain.utils.train_logger - epoch: 47, lr: 1.50e-04 - train si-snr: -3.93e+00 - valid si-snr: -1.82e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-48-00+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-20-43+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-43-28+00\nspeechbrain.utils.epoch_loop - Going into epoch 48\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.02s/it, train_loss=-4.61]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.10s/it]\nspeechbrain.utils.train_logger - epoch: 48, lr: 1.50e-04 - train si-snr: -4.61e+00 - valid si-snr: -1.87e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-52-33+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-48-00+00\nspeechbrain.utils.epoch_loop - Going into epoch 49\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [02:00<00:00, 120.89s/it, train_loss=-3.2]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.48s/it]\nspeechbrain.utils.train_logger - epoch: 49, lr: 1.50e-04 - train si-snr: -3.20e+00 - valid si-snr: -1.93e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-57-16+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-52-33+00\nspeechbrain.utils.epoch_loop - Going into epoch 50\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.62s/it, train_loss=-4.35]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.83s/it]\nspeechbrain.utils.train_logger - epoch: 50, lr: 1.50e-04 - train si-snr: -4.35e+00 - valid si-snr: -1.97e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-01-50+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-03-31+23-57-16+00\nspeechbrain.utils.epoch_loop - Going into epoch 51\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.08s/it, train_loss=-4.73]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.18s/it]\nspeechbrain.utils.train_logger - epoch: 51, lr: 1.50e-04 - train si-snr: -4.73e+00 - valid si-snr: -2.00e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-06-23+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-01-50+00\nspeechbrain.utils.epoch_loop - Going into epoch 52\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|█████████████████████████████| 1/1 [01:49<00:00, 109.60s/it, train_loss=-3]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.61s/it]\nspeechbrain.utils.train_logger - epoch: 52, lr: 1.50e-04 - train si-snr: -3.00e+00 - valid si-snr: -2.04e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-10-55+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-06-23+00\nspeechbrain.utils.epoch_loop - Going into epoch 53\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:48<00:00, 108.81s/it, train_loss=5.09]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.49s/it]\nspeechbrain.utils.train_logger - epoch: 53, lr: 1.50e-04 - train si-snr: 5.09 - valid si-snr: -2.06e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-15-27+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-10-55+00\nspeechbrain.utils.epoch_loop - Going into epoch 54\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:48<00:00, 108.61s/it, train_loss=-3.39]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.59s/it]\nspeechbrain.utils.train_logger - epoch: 54, lr: 1.50e-04 - train si-snr: -3.39e+00 - valid si-snr: -2.09e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-19-58+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-15-27+00\nspeechbrain.utils.epoch_loop - Going into epoch 55\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:56<00:00, 116.18s/it, train_loss=17.1]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.43s/it]\nspeechbrain.utils.train_logger - epoch: 55, lr: 1.50e-04 - train si-snr: 17.13 - valid si-snr: -2.09e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-24-37+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-19-58+00\nspeechbrain.utils.epoch_loop - Going into epoch 56\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.49s/it, train_loss=-3.91]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.62s/it]\nspeechbrain.utils.train_logger - epoch: 56, lr: 1.50e-04 - train si-snr: -3.91e+00 - valid si-snr: -2.13e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-29-10+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-24-37+00\nspeechbrain.utils.epoch_loop - Going into epoch 57\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:57<00:00, 117.37s/it, train_loss=-2.4]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.12s/it]\nspeechbrain.utils.train_logger - epoch: 57, lr: 1.50e-04 - train si-snr: -2.40e+00 - valid si-snr: -2.16e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-33-51+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-29-10+00\nspeechbrain.utils.epoch_loop - Going into epoch 58\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:48<00:00, 108.43s/it, train_loss=-4.96]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.73s/it]\nspeechbrain.utils.train_logger - epoch: 58, lr: 1.50e-04 - train si-snr: -4.96e+00 - valid si-snr: -2.16e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-38-23+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-33-51+00\nspeechbrain.utils.epoch_loop - Going into epoch 59\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:51<00:00, 111.17s/it, train_loss=-3.49]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.40s/it]\nspeechbrain.utils.train_logger - epoch: 59, lr: 1.50e-04 - train si-snr: -3.49e+00 - valid si-snr: -2.17e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-42-58+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-38-23+00\nspeechbrain.utils.epoch_loop - Going into epoch 60\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.60s/it, train_loss=-2.14]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.65s/it]\nspeechbrain.utils.train_logger - epoch: 60, lr: 1.50e-04 - train si-snr: -2.14e+00 - valid si-snr: -2.19e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-47-31+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-42-58+00\nspeechbrain.utils.epoch_loop - Going into epoch 61\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.59s/it, train_loss=-5.06]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.22s/it]\nspeechbrain.utils.train_logger - epoch: 61, lr: 1.50e-04 - train si-snr: -5.06e+00 - valid si-snr: -2.21e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-52-03+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-47-31+00\nspeechbrain.utils.epoch_loop - Going into epoch 62\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:58<00:00, 118.99s/it, train_loss=-4.68]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:41<00:00, 161.96s/it]\nspeechbrain.utils.train_logger - epoch: 62, lr: 1.50e-04 - train si-snr: -4.68e+00 - valid si-snr: -2.24e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-56-44+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-52-03+00\nspeechbrain.utils.epoch_loop - Going into epoch 63\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:49<00:00, 109.60s/it, train_loss=1.42]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.97s/it]\nspeechbrain.utils.train_logger - epoch: 63, lr: 1.50e-04 - train si-snr: 1.42 - valid si-snr: -2.22e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-01-17+00\nspeechbrain.utils.epoch_loop - Going into epoch 64\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|███████████████████████████| 1/1 [01:49<00:00, 109.21s/it, train_loss=21.4]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:45<00:00, 165.59s/it]\nspeechbrain.utils.train_logger - epoch: 64, lr: 1.50e-04 - train si-snr: 21.42 - valid si-snr: -2.16e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-05-53+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-01-17+00\nspeechbrain.utils.epoch_loop - Going into epoch 65\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:56<00:00, 116.62s/it, train_loss=-2.42]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.50s/it]\nspeechbrain.utils.train_logger - epoch: 65, lr: 1.50e-04 - train si-snr: -2.42e+00 - valid si-snr: -2.15e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-10-33+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-05-53+00\nspeechbrain.utils.epoch_loop - Going into epoch 66\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.87s/it, train_loss=-1.91]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.15s/it]\nspeechbrain.utils.train_logger - epoch: 66, lr: 1.50e-04 - train si-snr: -1.91e+00 - valid si-snr: -2.14e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-15-06+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-10-33+00\nspeechbrain.utils.epoch_loop - Going into epoch 67\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.98s/it, train_loss=-5.06]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.32s/it]\nspeechbrain.utils.train_logger - epoch: 67, lr: 1.50e-04 - train si-snr: -5.06e+00 - valid si-snr: -2.14e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-19-41+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-15-06+00\nspeechbrain.utils.epoch_loop - Going into epoch 68\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|█████████████████████████| 1/1 [01:48<00:00, 108.62s/it, train_loss=-0.949]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.04s/it]\nspeechbrain.utils.train_logger - epoch: 68, lr: 1.50e-04 - train si-snr: -9.49e-01 - valid si-snr: -2.12e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-24-12+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-19-41+00\nspeechbrain.utils.epoch_loop - Going into epoch 69\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.83s/it, train_loss=-3.14]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.51s/it]\nspeechbrain.utils.train_logger - epoch: 69, lr: 1.50e-04 - train si-snr: -3.14e+00 - valid si-snr: -2.15e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-28-46+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-24-12+00\nspeechbrain.utils.epoch_loop - Going into epoch 70\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.82s/it, train_loss=-5.12]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.30s/it]\nspeechbrain.utils.train_logger - epoch: 70, lr: 1.50e-04 - train si-snr: -5.12e+00 - valid si-snr: -2.24e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-33-19+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+00-56-44+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-28-46+00\nspeechbrain.utils.epoch_loop - Going into epoch 71\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:15<00:00, 135.81s/it, train_loss=-4.71]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:46<00:00, 166.60s/it]\nspeechbrain.utils.train_logger - epoch: 71, lr: 1.50e-04 - train si-snr: -4.71e+00 - valid si-snr: -2.32e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-38-22+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-33-19+00\nspeechbrain.utils.epoch_loop - Going into epoch 72\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.29s/it, train_loss=-3.45]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.33s/it]\nspeechbrain.utils.train_logger - epoch: 72, lr: 1.50e-04 - train si-snr: -3.45e+00 - valid si-snr: -2.39e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-42-55+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-38-22+00\nspeechbrain.utils.epoch_loop - Going into epoch 73\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.17s/it, train_loss=-2.95]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.65s/it]\nspeechbrain.utils.train_logger - epoch: 73, lr: 1.50e-04 - train si-snr: -2.95e+00 - valid si-snr: -2.44e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-47-27+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-42-55+00\nspeechbrain.utils.epoch_loop - Going into epoch 74\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:48<00:00, 108.88s/it, train_loss=-4.35]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 163.00s/it]\nspeechbrain.utils.train_logger - epoch: 74, lr: 1.50e-04 - train si-snr: -4.35e+00 - valid si-snr: -2.49e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-51-59+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-47-27+00\nspeechbrain.utils.epoch_loop - Going into epoch 75\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.55s/it, train_loss=-5.51]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.08s/it]\nspeechbrain.utils.train_logger - epoch: 75, lr: 1.50e-04 - train si-snr: -5.51e+00 - valid si-snr: -2.54e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-56-32+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-51-59+00\nspeechbrain.utils.epoch_loop - Going into epoch 76\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.78s/it, train_loss=-1.62]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.76s/it]\nspeechbrain.utils.train_logger - epoch: 76, lr: 1.50e-04 - train si-snr: -1.62e+00 - valid si-snr: -2.55e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-01-05+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+01-56-32+00\nspeechbrain.utils.epoch_loop - Going into epoch 77\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|████████████████████████████| 1/1 [01:54<00:00, 114.57s/it, train_loss=2.1]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.69s/it]\nspeechbrain.utils.train_logger - epoch: 77, lr: 1.50e-04 - train si-snr: 2.10 - valid si-snr: -2.49e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-05-43+00\nspeechbrain.utils.epoch_loop - Going into epoch 78\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:13<00:00, 133.50s/it, train_loss=-1.69]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.82s/it]\nspeechbrain.utils.train_logger - epoch: 78, lr: 1.50e-04 - train si-snr: -1.69e+00 - valid si-snr: -2.40e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-10-40+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-05-43+00\nspeechbrain.utils.epoch_loop - Going into epoch 79\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.52s/it, train_loss=-4.99]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.65s/it]\nspeechbrain.utils.train_logger - epoch: 79, lr: 1.50e-04 - train si-snr: -4.99e+00 - valid si-snr: -2.36e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-15-14+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-10-40+00\nspeechbrain.utils.epoch_loop - Going into epoch 80\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.70s/it, train_loss=-4.72]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.62s/it]\nspeechbrain.utils.train_logger - epoch: 80, lr: 1.50e-04 - train si-snr: -4.72e+00 - valid si-snr: -2.39e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-19-47+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-15-14+00\nspeechbrain.utils.epoch_loop - Going into epoch 81\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.25s/it, train_loss=-2.43]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.22s/it]\nspeechbrain.utils.train_logger - epoch: 81, lr: 1.50e-04 - train si-snr: -2.43e+00 - valid si-snr: -2.43e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-24-19+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-19-47+00\nspeechbrain.utils.epoch_loop - Going into epoch 82\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:48<00:00, 108.96s/it, train_loss=-4.96]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.10s/it]\nspeechbrain.utils.train_logger - epoch: 82, lr: 1.50e-04 - train si-snr: -4.96e+00 - valid si-snr: -2.51e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-28-52+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-24-19+00\nspeechbrain.utils.epoch_loop - Going into epoch 83\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.39s/it, train_loss=0.811]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.55s/it]\nspeechbrain.utils.train_logger - epoch: 83, lr: 1.50e-04 - train si-snr: 8.11e-01 - valid si-snr: -2.35e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-33-25+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-28-52+00\nspeechbrain.utils.epoch_loop - Going into epoch 84\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.19s/it, train_loss=-3.51]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.81s/it]\nspeechbrain.utils.train_logger - epoch: 84, lr: 1.50e-04 - train si-snr: -3.51e+00 - valid si-snr: -2.36e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-37-58+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-33-25+00\nspeechbrain.utils.epoch_loop - Going into epoch 85\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [02:01<00:00, 121.53s/it, train_loss=-5.19]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.73s/it]\nspeechbrain.utils.train_logger - epoch: 85, lr: 1.50e-04 - train si-snr: -5.19e+00 - valid si-snr: -2.55e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-42-44+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-37-58+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-01-05+00\nspeechbrain.utils.epoch_loop - Going into epoch 86\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:50<00:00, 110.37s/it, train_loss=-3.76]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.01s/it]\nspeechbrain.utils.train_logger - epoch: 86, lr: 1.50e-04 - train si-snr: -3.76e+00 - valid si-snr: -2.64e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-47-17+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-42-44+00\nspeechbrain.utils.epoch_loop - Going into epoch 87\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:48<00:00, 108.81s/it, train_loss=-2.94]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.72s/it]\nspeechbrain.utils.train_logger - epoch: 87, lr: 1.50e-04 - train si-snr: -2.94e+00 - valid si-snr: -2.66e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-51-48+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-47-17+00\nspeechbrain.utils.epoch_loop - Going into epoch 88\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.75s/it, train_loss=-4.69]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.34s/it]\nspeechbrain.utils.train_logger - epoch: 88, lr: 1.50e-04 - train si-snr: -4.69e+00 - valid si-snr: -2.74e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-56-21+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-51-48+00\nspeechbrain.utils.epoch_loop - Going into epoch 89\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:49<00:00, 109.57s/it, train_loss=-5.02]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:42<00:00, 162.76s/it]\nspeechbrain.utils.train_logger - epoch: 89, lr: 1.50e-04 - train si-snr: -5.02e+00 - valid si-snr: -2.79e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+03-00-54+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+02-56-21+00\nspeechbrain.utils.epoch_loop - Going into epoch 90\n  0%|                                                     | 0/1 [00:00<?, ?it/s]INSIDE FIT BATCH\ncompute_forward ended\n100%|██████████████████████████| 1/1 [01:48<00:00, 108.84s/it, train_loss=0.209]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [02:43<00:00, 163.37s/it]\nspeechbrain.utils.train_logger - epoch: 90, lr: 1.50e-04 - train si-snr: 2.09e-01 - valid si-snr: -2.73e+00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+03-05-26+00\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-01+03-00-54+00\n/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n100%|████████████████████████████████████████████| 1/1 [04:27<00:00, 267.74s/it]\nspeechbrain.utils.train_logger - Epoch loaded: 89 - test si-snr: -4.90e+00\n  0%|                                                     | 0/1 [00:00<?, ?it/s]compute_forward ended\n/kaggle/working/train.py:287: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/train.py:292: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n100%|████████████████████████████████████████████| 1/1 [04:23<00:00, 263.15s/it]\n__main__ - Mean SISNR is 4.9031829833984375\n__main__ - Mean SISNRi is 4.9120378494262695\n__main__ - Mean SDR is 5.7536416727372695\n__main__ - Mean SDRi is 5.755309199525072\n","output_type":"stream"}],"execution_count":19}]}
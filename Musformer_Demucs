{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10005918,"sourceType":"datasetVersion","datasetId":6159348}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T02:32:28.222234Z","iopub.execute_input":"2025-04-05T02:32:28.222423Z","iopub.status.idle":"2025-04-05T02:32:29.079420Z","shell.execute_reply.started":"2025-04-05T02:32:28.222402Z","shell.execute_reply":"2025-04-05T02:32:29.078695Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/musdb18-music-source-separation-dataset/The Long Wait - Dark Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Raft Monk - Tiring.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Too Much.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Georgia Wonder - Siren.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Sunshine Garcia Band - For I Am The Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Enda Reilly - Cur An Long Ag Seol.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Buitraker - Revo X.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/We Fell From The Sky - Not You.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Mountaineering Club - Mallory.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Skelpolu - Resurrection.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Over The Top.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Bobby Nobody - Stitch Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Arise - Run Run Run.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Carlos Gonzalez - A Place For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Forkupines - Semantics.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises - Falcon 69.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Lyndsey Ollard - Catching Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Triviul feat. The Fiend - Widow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Louis Cressy Band - Good Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Motor Tapes - Shore.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/AM Contra - Heart Peripheral.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Signe Jakobsen - What Have You Done To Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Moosmusic - Big Dummy Shake.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/M.E.R.C. Music - Knockout.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Doppler Shift - Atrophy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Detsky Sad - Walkie Talkie.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Happy Daze.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Timboz - Pony.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Oh No.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Mu - Too Bright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Hollow Ground - Ill Fate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises (Baumi) - SDRNR.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Like Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Sambasevam Shanmugam - Kaathaadi.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Tom McKenzie - Directions.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Borderline.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Side Effects Project - Sing With Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Nerve 9 - Pray For The Rain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Zeno - Signs.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Girls Under Glass - We Feel Alright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Cristina Vane - So Easy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Broken Man.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Bulldozer.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Punkdisco - Oral Hygiene.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Al James - Schoolboy Facination.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dark Ride - Burning Bridges.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Drumtracks - Ghost Bitch.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Aimee Norwich - Child.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - If You Say.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rockabilly.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Steven Clark - Bounty.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Giselle - Moss.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Strand Of Oaks - Spacestation.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - Set Me Free.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Bill Chudziak - Children Of No-one.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Angela Thomas Wade - Milk Cow Blues.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Grants - PunchDrunk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Grunge.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Once More (With Feeling).stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Beatles.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Auctioneer - Our Future Faces.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Air Traffic.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - A Reason To Leave.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Districts - Vermont.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Come Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/North To Alaska - All The Same.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Human Mistakes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dreamers Of The Ghetto - Heavy Love.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Rockshow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Faces On Film - Waiting For Ga.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Snowmine - Curfews.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Swinging Steaks - Lost My Way.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Dorothy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Gospel.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Stella.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Disco.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Reggae.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The So So Glos - Emergency.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Wicked.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/St Vitus - Word Gets Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Celestial Shore - Die For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Facade.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/AvaLuna - Waterduct.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Punk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - One Minute Smile.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Blood To Bone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Tim Taler - Stalker.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Hendrix.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Summerghost.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hop Along - Sister Cities.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - All Souls Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - You Listen.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country2.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Clinic A.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Sirens.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Britpop.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Chris Durban - Celebrate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Angelsaint.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - On The Line.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/A Classic Education - NightOwl.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Together Alone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Titanium - Haunted Age.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Goodbye Bolero.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Secret Mountains - High Horse.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Wall Of Death - Femme.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - The Wind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Velvet Curtain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Johnny Lokke - Whisper To A Scream.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - Take A Step.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Jay Menon - Through My Eyes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Flags - 54.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Waltz For My Victims.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Easy Tiger.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Back From The Start.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hollow Ground - Left Blind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Sweet Lights - You Let Me Down.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Port St Willow - Stay Even.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Helado Negro - Mitad Del Mundo.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Black Bloc - If You Want Success.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Pennies.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Voelund - Comfort Lives In Belief.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Nos Palpitants.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Creepoid - OldTree.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - South Of The Water.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Lushlife - Toynbee Suite.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Matthew Entwistle - Dont You Ever.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Scarlet Brand - Les Fleurs Du Mal.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country1.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - Dont Let Go.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - 80s Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Atlantis Bound - It Was My Fault For Waiting.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Invisible Familiars - Disturbing Wildlife.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Cnoc An Tursa - Bannockburn.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hezekiah Jones - Borrowed Heart.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/BigTroubles - Phantom.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Remember December - C U Next Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Night Panther - Fire.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Long Wait - Back Home To Blue.stem.mp4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install musdb\n!pip install mir_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T02:32:29.080831Z","iopub.execute_input":"2025-04-05T02:32:29.081200Z","iopub.status.idle":"2025-04-05T02:32:36.919116Z","shell.execute_reply.started":"2025-04-05T02:32:29.081174Z","shell.execute_reply":"2025-04-05T02:32:36.918403Z"}},"outputs":[{"name":"stdout","text":"Collecting musdb\n  Downloading musdb-0.4.2-py2.py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from musdb) (1.26.4)\nCollecting stempeg>=0.2.3 (from musdb)\n  Downloading stempeg-0.2.3-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pyaml in /usr/local/lib/python3.10/dist-packages (from musdb) (25.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from musdb) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.7->musdb) (2.4.1)\nCollecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb)\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->musdb) (6.0.2)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.7->musdb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.7->musdb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->musdb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.7->musdb) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.7->musdb) (2024.2.0)\nDownloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\nDownloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nInstalling collected packages: ffmpeg-python, stempeg, musdb\nSuccessfully installed ffmpeg-python-0.2.0 musdb-0.4.2 stempeg-0.2.3\nCollecting mir_eval\n  Downloading mir_eval-0.8.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.26.4)\nRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.13.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mir_eval) (4.4.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.15.4->mir_eval) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.15.4->mir_eval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.15.4->mir_eval) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.15.4->mir_eval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.15.4->mir_eval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.15.4->mir_eval) (2024.2.0)\nDownloading mir_eval-0.8.2-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mir_eval\nSuccessfully installed mir_eval-0.8.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n%%capture\n# Installing SpeechBrain via pip\nBRANCH = 'develop'\n!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n\n# Clone SpeechBrain repository\n!git clone https://github.com/speechbrain/speechbrain/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T02:32:36.920431Z","iopub.execute_input":"2025-04-05T02:32:36.920671Z","iopub.status.idle":"2025-04-05T02:32:54.743821Z","shell.execute_reply.started":"2025-04-05T02:32:36.920651Z","shell.execute_reply":"2025-04-05T02:32:54.743018Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"db_path = '/kaggle/input/musdb18-music-source-separation-dataset'\noutput_path = '/kaggle/working'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T02:32:54.744545Z","iopub.execute_input":"2025-04-05T02:32:54.744719Z","iopub.status.idle":"2025-04-05T02:32:54.747890Z","shell.execute_reply.started":"2025-04-05T02:32:54.744702Z","shell.execute_reply":"2025-04-05T02:32:54.747279Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nnp.float_ = np.float64\nimport musdb\n\nMUS_DB_PATH = db_path\n\nmus = musdb.DB(root=MUS_DB_PATH)\nmus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\nmus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\nmus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\nprint(mus_train[0])\nprint(mus_test[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T02:32:54.748603Z","iopub.execute_input":"2025-04-05T02:32:54.748833Z","iopub.status.idle":"2025-04-05T02:33:22.947911Z","shell.execute_reply.started":"2025-04-05T02:32:54.748813Z","shell.execute_reply":"2025-04-05T02:33:22.947131Z"}},"outputs":[{"name":"stdout","text":"A Classic Education - NightOwl\nAM Contra - Heart Peripheral\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%file hparams.yaml\n\n# ################################\n# Model: Demucs for source separation\n# https://hal.science/hal-02379796/document\n# Dataset : Musdb\n# ################################\n# Basic parameters\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params (unchanged from DPRNN)\ndata_folder: !PLACEHOLDER\n\nexperiment_name: demucs\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\n\n# Experiment params \nprecision: fp16\nnum_spks: 2\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True \nsample_rate: 16000\nn_audio_to_save: 10\n\n####################### Training Parameters ####################################\n\nN_epochs: 200\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999\nlimit_training_signal_len: False\ntraining_signal_len: 32000000\n\n\n# Data augmentation (unchanged)\nuse_wavedrop: False\nuse_rand_shift: False\nmin_shift: -8000\nmax_shift: 8000\n\n\n# Frequency/time drop (unchanged)\ndrop_freq: !new:speechbrain.augment.time_domain.DropFreq\n    drop_freq_low: 0\n    drop_freq_high: 1\n    drop_freq_count_low: 1\n    drop_freq_count_high: 3\n    drop_freq_width: 0.05\n\ndrop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n    drop_length_low: 1000\n    drop_length_high: 2000\n    drop_count_low: 1\n    drop_count_high: 5\n\nthreshold_byloss: True\nthreshold: -30\n\n################ Demucs Specific Parameters #############################\n## for Demucs V3/4\n# # Fourier Transform Parameters\n# n_fft: 2048\n# hop_length: 512\n\n\nkernel_size: 16\n# kernel_stride: 8\n\n# Dataloader options (unchanged)\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 3\n\n######################## Network Definition ####################################\n\n\nEncoder1: !new:models.EncoderBlock\n    in_channels: 2\n    # kernel_size: !ref <kernel_size>\n    out_channels: 64\n\n\nEncoder2: !new:models.EncoderBlock\n    in_channels: 64\n    # kernel_size: !ref <kernel_size>\n    out_channels: 128\n\n\nEncoder3: !new:models.EncoderBlock\n    in_channels: 128\n    # kernel_size: !ref <kernel_size>\n    out_channels: 256\n\n\nEncoder4: !new:models.EncoderBlock\n    in_channels: 256\n    # kernel_size: !ref <kernel_size>\n    out_channels: 512\n\n\nEncoder5: !new:models.EncoderBlock\n    in_channels: 512\n    # kernel_size: !ref <kernel_size>\n    out_channels: 1024\n\n\nEncoder6: !new:models.EncoderBlock\n    in_channels: 1024\n    # kernel_size: !ref <kernel_size>\n    out_channels: 2048\n\n\n\n\nDecoder6: !new:models.DecoderBlock\n    in_channels: 2048\n    out_channels: 1024\n    # # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n    \n\nDecoder5: !new:models.DecoderBlock\n    in_channels: 1024\n    out_channels: 512\n    # # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n    \n\nDecoder4: !new:models.DecoderBlock\n    in_channels: 512\n    out_channels: 256\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n    \n\nDecoder3: !new:models.DecoderBlock\n    in_channels: 256\n    out_channels: 128\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n    \n\nDecoder2: !new:models.DecoderBlock\n    in_channels: 128\n    out_channels: 64\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n    \n\nDecoder1: !new:models.DecoderBlock\n    in_channels: 64\n    out_channels: 8\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n    \n\nLinear: !new:speechbrain.nnet.linear.Linear\n    input_size: 4096\n    bias: False\n    n_neurons: 2048\n\nBiLSTM: !new:speechbrain.nnet.RNN.LSTM\n    hidden_size: 2048\n    input_size: 2048\n    num_layers: 2\n    bidirectional: True\n\n\n######################## Remaining Config ######################################\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder1: !ref <Encoder1>\n    encoder2: !ref <Encoder2>\n    encoder3: !ref <Encoder3>\n    encoder4: !ref <Encoder4>\n    encoder5: !ref <Encoder5>\n    encoder6: !ref <Encoder6>\n    lstm: !ref <BiLSTM>\n    linear: !ref <Linear>\n    decoder6: !ref <Decoder6>\n    decoder5: !ref <Decoder5>\n    decoder4: !ref <Decoder4>\n    decoder3: !ref <Decoder3>\n    decoder2: !ref <Decoder2>\n    decoder1: !ref <Decoder1>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:42:02.042530Z","iopub.execute_input":"2025-04-05T03:42:02.042936Z","iopub.status.idle":"2025-04-05T03:42:02.049280Z","shell.execute_reply.started":"2025-04-05T03:42:02.042907Z","shell.execute_reply":"2025-04-05T03:42:02.048254Z"}},"outputs":[{"name":"stdout","text":"Overwriting hparams.yaml\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"%%file models.py\n\nimport torch\nfrom torch import nn\nfrom speechbrain.nnet.CNN import Conv1d, ConvTranspose1d\n# from speechbrain.nnet.activations import GLU\nfrom speechbrain.lobes.models.beats import GLU_Linear\nfrom torch.nn import GLU\nfrom speechbrain.nnet.RNN import LSTM\nfrom speechbrain.nnet.linear import Linear\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = Conv1d(\n            out_channels=out_channels,\n            in_channels=in_channels,\n            kernel_size=8,\n            stride=4,\n            default_padding=2,\n            # skip_transpose=True,  \n        )\n        self.glu_conv = Conv1d(\n            out_channels=2*out_channels,  \n            in_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            # skip_transpose=True,\n        )\n        self.relu = torch.nn.ReLU()\n        self.glu = GLU()\n\n    def forward(self, x):\n        x = self.relu(self.conv(x))\n        x = self.glu(self.glu_conv(x))\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.glu_conv = Conv1d(\n            out_channels=2*in_channels,\n            in_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            skip_transpose=True,\n        )\n        self.conv_tr = ConvTranspose1d(\n            out_channels=out_channels,\n            in_channels=in_channels,  # After GLU split\n            kernel_size=8,\n            stride=4,\n            padding=2,\n            skip_transpose=True,\n        )\n        self.glu = GLU()\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.glu(self.glu_conv(x))\n        x = self.relu(self.conv_tr(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T04:08:59.597690Z","iopub.execute_input":"2025-04-05T04:08:59.598390Z","iopub.status.idle":"2025-04-05T04:08:59.605493Z","shell.execute_reply.started":"2025-04-05T04:08:59.598334Z","shell.execute_reply":"2025-04-05T04:08:59.604233Z"}},"outputs":[{"name":"stdout","text":"Overwriting models.py\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"%%file train.py\n#!/usr/bin/env/python3\n\"\"\"Recipe for training a neural speech separation system on the wsjmix\ndataset. The system employs an encoder, a decoder, and a masking network.\n\nTo run this recipe, do the following:\n> python train.py hparams/sepformer.yaml\n> python train.py hparams/dualpath_rnn.yaml\n> python train.py hparams/convtasnet.yaml\n\nThe experiment file is flexible enough to support different neural\nnetworks. By properly changing the parameter files, you can try\ndifferent architectures. The script supports both wsj2mix and\nwsj3mix.\n\n\nAuthors\n * Cem Subakan 2020\n * Mirco Ravanelli 2020\n * Samuele Cornell 2020\n * Mirko Bronzi 2020\n * Jianyuan Zhong 2020\n\"\"\"\n## CHECKPOINT\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nfrom speechbrain.nnet.CNN import Conv1d, ConvTranspose1d\n# from speechbrain.nnet.activations import GLU\nfrom speechbrain.lobes.models.beats import GLU_Linear\nfrom torch.nn import GLU\nfrom speechbrain.nnet.RNN import LSTM\nfrom speechbrain.nnet.linear import Linear\nfrom models import EncoderBlock, DecoderBlock\nimport musdb\n\n# Define training procedure\nclass DemucsSeparation(sb.Brain):\n    # def on_fit_start(self):\n        \n    #     self.encoder1 = EncoderBlock(C_in=2, C_out=64)\n    #     self.encoder2 = EncoderBlock(C_in=64, C_out=128)\n    #     self.encoder3 = EncoderBlock(C_in=128, C_out=256)\n    #     self.encoder4 = EncoderBlock(C_in=256, C_out=512)\n    #     self.encoder5 = EncoderBlock(C_in=512, C_out=1024)\n    #     self.encoder6 = EncoderBlock(C_in=1024, C_out=2048)\n\n    #     # Bidirectional LSTM (hidden_size=2048, 2 layers)\n    #     self.lstm = LSTM(\n    #         input_size=2048,\n    #         hidden_size=2048,\n    #         num_layers=2,\n    #         bidirectional=True\n    #     )\n        \n    #     # Linear layer (4096 -> 2048)\n    #     self.linear = Linear(input_size=4096, n_neurons=2048)\n\n    #     # Decoder path (matches image specs)\n    #     self.decoder6 = DecoderBlock(C_in=2048, C_out=1024)\n    #     self.decoder5 = DecoderBlock(C_in=1024, C_out=512)\n    #     self.decoder4 = DecoderBlock(C_in=512, C_out=256)\n    #     self.decoder3 = DecoderBlock(C_in=256, C_out=128)\n    #     self.decoder2 = DecoderBlock(C_in=128, C_out=64)\n    #     self.decoder1 = DecoderBlock(C_in=64, C_out=8)  # 4*2 channels\n        \n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix\n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)\n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n            dim=-1,\n        ).to(self.device)\n\n        # Add speech distortions\n        if stage == sb.Stage.TRAIN:\n            with torch.no_grad():\n\n                if self.hparams.use_wavedrop:\n                    mix = self.hparams.drop_chunk(mix, mix_lens)\n                    mix = self.hparams.drop_freq(mix)\n\n                if self.hparams.limit_training_signal_len:\n                    mix, targets = self.cut_signals(mix, targets)\n        \n\n        # Separation\n        mix_enc_1 = self.modules.encoder1(mix)\n        mix_enc_2 = self.modules.encoder2(mix_enc_1)\n        mix_enc_3 = self.modules.encoder3(mix_enc_2)\n        mix_enc_4 = self.modules.encoder4(mix_enc_3)\n        mix_enc_5 = self.modules.encoder5(mix_enc_4)\n        mix_enc_6 = self.modules.encoder6(mix_enc_5)\n\n\n        lstm_out = self.modules.lstm(mix_enc_6)\n\n        lin_out = self.modules.linear(lstm_out)\n        \n        mix_dec_6 = self.modules.decoder6(lin_out + mix_enc_6)\n        mix_dec_5 = self.modules.decoder5(mix_dec_6 + mix_enc_5)\n        mix_dec_4 = self.modules.decoder4(mix_dec_5 + mix_enc_4)\n        mix_dec_3 = self.modules.decoder3(mix_dec_4 + mix_enc_3)\n        mix_dec_2 = self.modules.decoder2(mix_dec_3 + mix_enc_2)\n        mix_dec_1 = self.modules.decoder1(mix_dec_2 + mix_enc_1)\n        \n        \n\n\n\n        \n        # mix_w = self.hparams.Encoder(mix)\n        # est_mask = self.hparams.MaskNet(mix_w)\n        # mix_w = torch.stack([mix_w] * self.hparams.num_spks)\n        # sep_h = mix_w * est_mask\n\n        # Decoding\n        est_source = torch.cat(\n            [\n                mix_dec_1[i].unsqueeze(-1)\n                for i in range(self.hparams.num_spks)\n            ],\n            dim=-1,\n        )\n\n        # T changed after conv1d in encoder, fix it here\n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n## CHECKPOINT\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with self.training_ctx:\n            predictions, targets = self.compute_forward(\n                mixture, targets, sb.Stage.TRAIN\n            )\n            loss = self.compute_objectives(predictions, targets)\n\n            # hard threshold the easy dataitems\n            if self.hparams.threshold_byloss:\n                th = self.hparams.threshold\n                loss = loss[loss > th]\n                if loss.nelement() > 0:\n                    loss = loss.mean()\n            else:\n                loss = loss.mean()\n\n        if loss.nelement() > 0 and loss < self.hparams.loss_upper_lim:\n            self.scaler.scale(loss).backward()\n            if self.hparams.clip_grad_norm >= 0:\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n        \n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_data):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n        test_loader = sb.dataio.dataloader.make_dataloader(\n            test_data, **self.hparams.dataloader_opts\n        )\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.id\n                    targets = [batch.s1_sig, batch.s2_sig]\n                    \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_spks, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n\n                    # Compute SDR\n                    sdr, _, _, _ = bss_eval_sources(\n                        targets[0].t().cpu().numpy(),\n                        predictions[0].t().detach().cpu().numpy(),\n                    )\n\n                    sdr_baseline, _, _, _ = bss_eval_sources(\n                        targets[0].t().cpu().numpy(),\n                        mixture_signal[0].t().detach().cpu().numpy(),\n                    )\n\n                    sdr_i = sdr.mean() - sdr_baseline.mean()\n\n                    # Saving on a csv file\n                    row = {\n                        \"snt_id\": snt_id[0],\n                        \"sdr\": sdr.mean(),\n                        \"sdr_i\": sdr_i,\n                        \"si-snr\": -sisnr.item(),\n                        \"si-snr_i\": -sisnr_i.item(),\n                    }\n                    writer.writerow(row)\n\n                    # Metric Accumulation\n                    all_sdrs.append(sdr.mean())\n                    all_sdrs_i.append(sdr_i.mean())\n                    all_sisnrs.append(-sisnr.item())\n                    all_sisnrs_i.append(-sisnr_i.item())\n\n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_spks):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n# def dataio_prep(hparams):\n#     \"\"\"Creates data processing pipeline\"\"\"\n\n#     # 1. Define datasets\n#     train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n#         csv_path=hparams[\"train_data\"],\n#         replacements={\"data_root\": hparams[\"data_folder\"]},\n#     )\n\n#     valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n#         csv_path=hparams[\"valid_data\"],\n#         replacements={\"data_root\": hparams[\"data_folder\"]},\n#     )\n\n#     test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n#         csv_path=hparams[\"test_data\"],\n#         replacements={\"data_root\": hparams[\"data_folder\"]},\n#     )\n\n#     datasets = [train_data, valid_data, test_data]\n\n#     # 2. Provide audio pipelines\n\n#     @sb.utils.data_pipeline.takes(\"mix_wav\")\n#     @sb.utils.data_pipeline.provides(\"mix_sig\")\n#     def audio_pipeline_mix(mix_wav):\n#         mix_sig = sb.dataio.dataio.read_audio(mix_wav)\n#         return mix_sig\n\n#     @sb.utils.data_pipeline.takes(\"s1_wav\")\n#     @sb.utils.data_pipeline.provides(\"s1_sig\")\n#     def audio_pipeline_s1(s1_wav):\n#         s1_sig = sb.dataio.dataio.read_audio(s1_wav)\n#         return s1_sig\n\n#     @sb.utils.data_pipeline.takes(\"s2_wav\")\n#     @sb.utils.data_pipeline.provides(\"s2_sig\")\n#     def audio_pipeline_s2(s2_wav):\n#         s2_sig = sb.dataio.dataio.read_audio(s2_wav)\n#         return s2_sig\n\n#     if hparams[\"num_spks\"] == 3:\n\n#         @sb.utils.data_pipeline.takes(\"s3_wav\")\n#         @sb.utils.data_pipeline.provides(\"s3_sig\")\n#         def audio_pipeline_s3(s3_wav):\n#             s3_sig = sb.dataio.dataio.read_audio(s3_wav)\n#             return s3_sig\n\n#     sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_mix)\n#     sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_s1)\n#     sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_s2)\n#     if hparams[\"num_spks\"] == 3:\n#         sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline_s3)\n#         sb.dataio.dataset.set_output_keys(\n#             datasets, [\"id\", \"mix_sig\", \"s1_sig\", \"s2_sig\", \"s3_sig\"]\n#         )\n#     else:\n#         sb.dataio.dataset.set_output_keys(\n#             datasets, [\"id\", \"mix_sig\", \"s1_sig\", \"s2_sig\"]\n#         )\n\n#     return train_data, valid_data, test_data\n\n## CHECKPOINT\ndef dataio_prep(hparams):\n    \"\"\"Creates data processing pipeline\"\"\"\n\n    # 1. Define datasets\n\n    # datasets = {}\n    # data_info = {\n    #     \"train\": hparams[\"train_annotation\"],\n    #     \"valid\": hparams[\"valid_annotation\"],\n    #     \"test\": hparams[\"test_annotation\"],\n    # }\n\n        \n    MUS_DB_PATH = hparams[\"db_path\"]\n    \n    mus = musdb.DB(root=MUS_DB_PATH)\n    \n    mus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\n    mus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\n    mus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\n\n\n        \n    def create_json(mus_obj):\n      json_dict = {}\n      for i, track in enumerate(mus_obj):\n        \n        file_name = track.name\n        file_path = track.path\n        file_rate = track.rate\n        \n        json_dict[file_name] = {\n                  \"track\": track\n          }\n        \n        return json_dict\n          \n    train_obj = create_json(mus_train)\n    test_obj = create_json(mus_test)\n    valid_obj = create_json(mus_valid)\n    \n   \n    \n    \n  \n    def convert_musdb_to_torch(track, target_sr=8000, chunk_size_seconds=1):\n        \"\"\"\n        Converts a musdb track to a PyTorch tensor with efficient resampling.\n    \n        Args:\n            track: A musdb track object (e.g., `mus_train[0]`).\n            target_sr (int): The target sampling rate for resampling.\n            chunk_size_seconds (int): Number of seconds per processing chunk.\n    \n        Returns:\n            torch.Tensor: The resampled waveform tensor of shape (num_channels, num_samples).\n        \"\"\"\n        # Convert to tensor and move channels first (PyTorch format)\n        audio_tensor = torch.from_numpy(track.audio).float().permute(1, 0)  # Shape: (num_channels, num_samples)\n        \n        orig_sr = track.rate  # Original sample rate\n        \n        chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n    \n        resampled_chunks = []\n    \n        for i in range(0, audio_tensor.shape[1], chunk_size):\n            chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n            resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n            resampled_chunks.append(resampled_chunk)\n    \n        # Concatenate back the processed chunks\n        # print(\"PROCESSING CHUNKS\")\n        resampled_audio = torch.cat(resampled_chunks, dim=1)\n        # print(resampled_audio.shape)\n        resampled_audio = resampled_audio #.mean(dim=0, keepdim=False) # need 2 channels for demucs\n        # print(resampled_audio.shape)\n        # print(resampled_audio.shape)\n        return resampled_audio\n    \n    \n    @sb.utils.data_pipeline.takes(\"track\")\n    @sb.utils.data_pipeline.provides(\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\")\n    def audio_pipeline_mix(track):\n        # mix_sig = torchaudio.functional.resample(torch.from_numpy(track.audio), track.rate, hparams[\"sample_rate\"])\n\n        # voc_sig = torchaudio.functional.resample(torch.from_numpy(track.targets['vocals'].audio), track.rate, hparams[\"sample_rate\"])\n\n        # inst_sig = torchaudio.functional.resample(torch.from_numpy(track.targets['accompaniment'].audio), track.rate, hparams[\"sample_rate\"])\n         #.squeeze(dim=0) \n        mix_sig = convert_musdb_to_torch(track, hparams[\"sample_rate\"], chunk_size_seconds=1)\n        voc_sig = convert_musdb_to_torch(track.targets[\"vocals\"], hparams[\"sample_rate\"], chunk_size_seconds=1)\n        inst_sig = convert_musdb_to_torch(track.targets[\"accompaniment\"], hparams[\"sample_rate\"], chunk_size_seconds=1)\n        track_id = track.name\n        \n        return track_id, mix_sig, voc_sig, inst_sig\n\n\n    \n    \n    train_data = sb.dataio.dataset.DynamicItemDataset(train_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    valid_data = sb.dataio.dataset.DynamicItemDataset(valid_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    test_data = sb.dataio.dataset.DynamicItemDataset(test_obj, dynamic_items=[audio_pipeline_mix], output_keys=[\"track_id\",\"mix_sig\", \"voc_sig\", \"inst_sig\"])\n    datasets = [train_data, valid_data, test_data]\n    \n    \n    return datasets\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n\n \n    train_data, valid_data, test_data = dataio_prep(hparams)\n\n   \n    # Brain class initialization\n    separator = DemucsSeparation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n\n  \n    # Training\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_data,\n        valid_data,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # Eval\n    separator.evaluate(test_data, min_key=\"si-snr\")\n    separator.save_results(test_data)\n    ## CHECKPOINT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T04:00:37.206724Z","iopub.execute_input":"2025-04-05T04:00:37.207068Z","iopub.status.idle":"2025-04-05T04:00:37.214883Z","shell.execute_reply.started":"2025-04-05T04:00:37.207048Z","shell.execute_reply":"2025-04-05T04:00:37.214184Z"}},"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"# To start from scratch, you need to remove the output folder.\n# Otherwise, speechbrain starts from the last valid checkpoint.\n#!rm -rf ./results/AudioMNIST/Autoencoder/\n## CHECKPOINT\n!python train.py hparams.yaml --data_folder=db_path --device=\"cpu\"\n# !torchrun --standalone --nproc_per_node=2 train.py hparams.yaml --data_folder=db_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T04:09:08.230306Z","iopub.execute_input":"2025-04-05T04:09:08.230566Z","iopub.status.idle":"2025-04-05T04:09:50.783007Z","shell.execute_reply.started":"2025-04-05T04:09:08.230549Z","shell.execute_reply":"2025-04-05T04:09:50.781875Z"}},"outputs":[{"name":"stdout","text":"speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/demucs/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `False`\nspeechbrain.core - Using training precision: `--precision=bf16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - DemucsSeparation Model Statistics:\n* Total Number of Trainable Parameters: 237.7M\n* Total Number of Parameters: 237.7M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\nspeechbrain.utils.epoch_loop - Going into epoch 1\n  0%|                                                     | 0/1 [00:03<?, ?it/s]\nspeechbrain.core - Exception:\nTraceback (most recent call last):\n  File \"/kaggle/working/train.py\", line 626, in <module>\n    separator.fit(\n  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1575, in fit\n    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1400, in _fit_train\n    loss = self.fit_batch(batch)\n  File \"/kaggle/working/train.py\", line 164, in fit_batch\n    predictions, targets = self.compute_forward(\n  File \"/kaggle/working/train.py\", line 104, in compute_forward\n    mix_enc_1 = self.modules.encoder1(mix)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/models.py\", line 33, in forward\n    x = self.relu(self.conv(x))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/CNN.py\", line 438, in forward\n    x = self._manage_padding(\n  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/CNN.py\", line 493, in _manage_padding\n    x = F.pad(x, padding, mode=self.padding_mode)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 5096, in pad\n    return torch._C._nn.pad(input, pad, mode, value)\nRuntimeError: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (4, 4) at dimension 2 of input [1, 2739955, 2]\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"import musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\n\nclass LazyMusDBDataset(Dataset):\n    def __init__(self, root, subset=\"train\", split=None, target_sr=8000, chunk_size=15):\n        \"\"\"\n        True lazy-loading for MUSDB\n        :param chunk_size: in seconds\n        \"\"\"\n        self.db = musdb.DB(root=root, subsets=subset, split=split, is_wav=False)\n        self.target_sr = target_sr\n        self.chunk_size = chunk_size\n        self.tracks = [{\n            \"path\": track.path,\n            \"duration\": track.duration,\n            \"rate\": track.rate,\n            \"stem_id\": track.stem_id  # Needed for STEM access\n        } for track in self.db.tracks]\n\n    def __len__(self):\n        return len(self.tracks)\n\n    def __getitem__(self, idx):\n        track_info = self.tracks[idx]\n        \n        # Load chunk directly from disk without full track loading\n        def load_stem_chunk(stem_name, random_chunk=True):\n            # MUSDB's internal lazy loading\n            track = self.db.tracks[idx]\n            if stem_name == \"mix\":\n                source = track\n            else:\n                source = track.targets[stem_name]\n            \n            # Calculate chunk bounds\n            if random_chunk:\n                max_start = int(track_info[\"duration\"] * track_info[\"rate\"]) - self.chunk_size * track_info[\"rate\"]\n                start = np.random.randint(0, max(max_start, 1))\n            else:\n                start = 0\n            stop = start + self.chunk_size * track_info[\"rate\"]\n            \n            # Load only the needed segment\n            audio = source.audio[start:stop]\n            \n            # Convert and resample\n            audio_tensor = torch.from_numpy(audio).float().permute(1, 0)\n            return torchaudio.functional.resample(\n                audio_tensor,\n                orig_freq=track_info[\"rate\"],\n                new_freq=self.target_sr\n            ).mean(dim=0, keepdim=False)\n\n        \n        # orig_sr = track.rate  # Original sample rate\n        \n        # chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n    \n        # resampled_chunks = []\n    \n        # for i in range(0, audio_tensor.shape[1], chunk_size):\n        #     chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n        #     resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n        #     resampled_chunks.append(resampled_chunk)\n    \n        # # Concatenate back the processed chunks\n        # # print(\"PROCESSING CHUNKS\")\n        # resampled_audio = torch.cat(resampled_chunks, dim=1)\n        # # print(resampled_audio.shape)\n        # resampled_audio = resampled_audio.mean(dim=0, keepdim=False)\n        # # print(resampled_audio.shape)\n        # # print(resampled_audio.shape)\n        # return resampled_audio\n\n        return {\n            \"mix_sig\": load_stem_chunk(\"mix\"),\n            \"voc_sig\": load_stem_chunk(\"vocals\"),\n            \"inst_sig\": load_stem_chunk(\"accompaniment\"),\n            \"track_id\": track_info[\"stem_id\"]\n        }\n\n# Usage with SpeechBrain\ntrain_data = LazyMusDBDataset(db_path, subset=\"train\", split=\"train\")\nvalid_data = LazyMusDBDataset(db_path, subset=\"train\", split=\"valid\")\ntest_data = LazyMusDBDataset(db_path, subset=\"test\")\n\n# Create DataLoader\ntrain_loader = sb.dataio.dataloader.make_dataloader(\n    train_data,\n    batch_size=1,\n    collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:41:25.848415Z","iopub.execute_input":"2025-04-05T03:41:25.848635Z","iopub.status.idle":"2025-04-05T03:41:38.963032Z","shell.execute_reply.started":"2025-04-05T03:41:25.848617Z","shell.execute_reply":"2025-04-05T03:41:38.962405Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def convert_musdb_to_torch(track, target_sr=8000, chunk_size_seconds=1):\n#     \"\"\"\n#     Converts a musdb track to a PyTorch tensor with efficient resampling.\n\n#     Args:\n#         track: A musdb track object (e.g., `mus_train[0]`).\n#         target_sr (int): The target sampling rate for resampling.\n#         chunk_size_seconds (int): Number of seconds per processing chunk.\n\n#     Returns:\n#         torch.Tensor: The resampled waveform tensor of shape (num_channels, num_samples).\n#     \"\"\"\n#     # Convert to tensor and move channels first (PyTorch format)\n#     audio_tensor = torch.from_numpy(track.audio).float().permute(1, 0)  # Shape: (num_channels, num_samples)\n    \n#     orig_sr = track.rate  # Original sample rate\n    \n#     chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n\n#     resampled_chunks = []\n\n#     for i in range(0, audio_tensor.shape[1], chunk_size):\n#         chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n#         resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n#         resampled_chunks.append(resampled_chunk)\n\n#     # Concatenate back the processed chunks\n#     resampled_audio = torch.cat(resampled_chunks, dim=1)\n\n#     return resampled_audio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:41:38.963940Z","iopub.execute_input":"2025-04-05T03:41:38.964140Z","iopub.status.idle":"2025-04-05T03:41:38.966983Z","shell.execute_reply.started":"2025-04-05T03:41:38.964120Z","shell.execute_reply":"2025-04-05T03:41:38.966401Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# import soundfile as sf\n\n# def process_and_save_tracks(db, subset_name, output_dir):\n#     \"\"\"\n#     Process tracks from a musdb subset and save vocals, instrumentals, and mix as WAV files\n    \n#     Args:\n#         db: musdb DB object (train, valid, or test)\n#         subset_name: name of the subset ('train', 'valid', or 'test')\n#         output_dir: root directory where files should be saved\n#     \"\"\"\n#     # Create output directory if it doesn't exist\n#     subset_dir = os.path.join(output_dir, subset_name)\n#     os.makedirs(subset_dir, exist_ok=True)\n    \n#     for idx, track in enumerate(db):\n#         print(f\"Processing {subset_name} track {idx+1}/{len(db)}: {track.name}\")\n        \n#         # Get the audio data\n#         vocals = track.targets['vocals'].audio\n#         mix = track.audio\n#         instrumentals = track.targets['accompaniment'].audio\n        \n#         # # Create track-specific directory\n#         # track_dir = os.path.join(subset_dir, track.name)\n#         # os.makedirs(track_dir, exist_ok=True)\n        \n#         # Save files with appropriate names\n#         sf.write(os.path.join(subset_dir, f\"{track.name}_vocals.wav\"), vocals, track.rate)\n#         sf.write(os.path.join(subset_dir, f\"{track.name}_instrumentals.wav\"), instrumentals, track.rate)\n#         sf.write(os.path.join(subset_dir, f\"{track.name}_mix.wav\"), mix, track.rate)\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:41:38.967716Z","iopub.execute_input":"2025-04-05T03:41:38.968061Z","iopub.status.idle":"2025-04-05T03:41:38.983253Z","shell.execute_reply.started":"2025-04-05T03:41:38.968038Z","shell.execute_reply":"2025-04-05T03:41:38.982546Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# output_dir = '/kaggle/working/musdb'\n# process_and_save_tracks(mus_train, \"train\", output_dir + '/train')\n# process_and_save_tracks(mus_valid, \"valid\", output_dir + '/valid')\n# process_and_save_tracks(mus_test, \"test\", output_dir + '/test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:41:38.983894Z","iopub.execute_input":"2025-04-05T03:41:38.984080Z","iopub.status.idle":"2025-04-05T03:41:38.997220Z","shell.execute_reply.started":"2025-04-05T03:41:38.984061Z","shell.execute_reply":"2025-04-05T03:41:38.996523Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# track = mus_train[0]\n# print(track)\n# print(track.audio.shape)\n# import torchaudio\n# import torch\n# import IPython.display as ipd\n# np.float_ = np.float64\n\n# audio_tensor = torch.from_numpy(track.audio).float()\n\n# # audio_tensor = audio_tensor.permute(1, 0)\n# # audio_tensor = audio_tensor.mean(dim=0, keepdim=True)\n# # print(audio_tensor.shape)\n# # mix_sig = torchaudio.functional.resample(torch.from_numpy(track.audio), track.rate, 8000)\n# mix_sig = convert_musdb_to_torch(track, 16000, chunk_size_seconds=1)\n# voc_sig = convert_musdb_to_torch(track.targets['vocals'], 16000, chunk_size_seconds=1)\n# inst_sig = convert_musdb_to_torch(track.targets['accompaniment'], 16000, chunk_size_seconds=1)\n# print(mix_sig)\n# print(mix_sig.shape)\n# # print(torch.Size(mix_sig))\n# ipd.Audio(mix_sig.numpy(), rate=16000)\n# ipd.Audio(voc_sig.numpy(), rate=16000)\n# ipd.Audio(inst_sig.numpy(), rate=16000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:41:38.997973Z","iopub.execute_input":"2025-04-05T03:41:38.998191Z","iopub.status.idle":"2025-04-05T03:41:39.009259Z","shell.execute_reply.started":"2025-04-05T03:41:38.998169Z","shell.execute_reply":"2025-04-05T03:41:39.008506Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"# import json\n# import torchaudio\n# from speechbrain.utils.data_utils import get_all_files\n\n# train_files = []\n# valid_files = []\n# test_files = []\n\n# def create_json(json_file, mus_obj):\n\n#   json_dict = {}\n#   for i, track in enumerate(mus_obj):\n#     if i % 10 == 0:\n#       print(i)\n\n#     file_name = track.name\n#     file_path = track.path\n#     file_rate = track.rate\n#     # file_audio = track.audio\n#     # file_vocal = track.targets['vocals'].audio\n#     # print(file_name)\n#     json_dict[file_name] = {\n#               \"file_path\": file_path,\n#               \"rate\": file_rate\n#       }\n#     # print(json_dict[file_name])\n\n#     with open(json_file, mode=\"w\") as json_f:\n#         json.dump(json_dict, json_f, indent=2)\n\n# # 80% for training\n# create_json(os.path.join(output_path, \"train.json\"), mus_train)\n# create_json(os.path.join(output_path, \"valid.json\"), mus_valid)\n# create_json(os.path.join(output_path, \"test.json\"), mus_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:41:39.009972Z","iopub.execute_input":"2025-04-05T03:41:39.010189Z","iopub.status.idle":"2025-04-05T03:41:39.024734Z","shell.execute_reply.started":"2025-04-05T03:41:39.010172Z","shell.execute_reply":"2025-04-05T03:41:39.023869Z"}},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/results/convtasnet/1234/save","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T03:41:39.026458Z","iopub.execute_input":"2025-04-05T03:41:39.026723Z","iopub.status.idle":"2025-04-05T03:41:39.171425Z","shell.execute_reply.started":"2025-04-05T03:41:39.026696Z","shell.execute_reply":"2025-04-05T03:41:39.170351Z"}},"outputs":[{"name":"stdout","text":"\tzip warning: name not matched: /kaggle/working/results/convtasnet/1234/save\n\nzip error: Nothing to do! (try: zip -r file.zip . -i /kaggle/working/results/convtasnet/1234/save)\n","output_type":"stream"}],"execution_count":85}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10005918,"sourceType":"datasetVersion","datasetId":6159348}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:47:55.325918Z","iopub.execute_input":"2025-04-16T18:47:55.326180Z","iopub.status.idle":"2025-04-16T18:47:55.646943Z","shell.execute_reply.started":"2025-04-16T18:47:55.326161Z","shell.execute_reply":"2025-04-16T18:47:55.646057Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/musdb18-music-source-separation-dataset/The Long Wait - Dark Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Raft Monk - Tiring.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Too Much.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Georgia Wonder - Siren.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Sunshine Garcia Band - For I Am The Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Enda Reilly - Cur An Long Ag Seol.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Buitraker - Revo X.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/We Fell From The Sky - Not You.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Mountaineering Club - Mallory.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Skelpolu - Resurrection.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Over The Top.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Bobby Nobody - Stitch Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Arise - Run Run Run.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Carlos Gonzalez - A Place For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Forkupines - Semantics.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises - Falcon 69.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Lyndsey Ollard - Catching Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Triviul feat. The Fiend - Widow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Louis Cressy Band - Good Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Motor Tapes - Shore.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/AM Contra - Heart Peripheral.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Signe Jakobsen - What Have You Done To Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Moosmusic - Big Dummy Shake.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/M.E.R.C. Music - Knockout.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Doppler Shift - Atrophy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Detsky Sad - Walkie Talkie.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Happy Daze.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Timboz - Pony.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Oh No.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Mu - Too Bright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Hollow Ground - Ill Fate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises (Baumi) - SDRNR.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Like Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Sambasevam Shanmugam - Kaathaadi.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Tom McKenzie - Directions.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Borderline.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Side Effects Project - Sing With Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Nerve 9 - Pray For The Rain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Zeno - Signs.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Girls Under Glass - We Feel Alright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Cristina Vane - So Easy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Broken Man.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Bulldozer.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Punkdisco - Oral Hygiene.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Al James - Schoolboy Facination.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dark Ride - Burning Bridges.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Drumtracks - Ghost Bitch.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Aimee Norwich - Child.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - If You Say.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rockabilly.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Steven Clark - Bounty.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Giselle - Moss.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Strand Of Oaks - Spacestation.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - Set Me Free.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Bill Chudziak - Children Of No-one.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Angela Thomas Wade - Milk Cow Blues.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Grants - PunchDrunk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Grunge.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Once More (With Feeling).stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Beatles.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Auctioneer - Our Future Faces.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Air Traffic.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - A Reason To Leave.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Districts - Vermont.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Come Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/North To Alaska - All The Same.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Human Mistakes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dreamers Of The Ghetto - Heavy Love.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Rockshow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Faces On Film - Waiting For Ga.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Snowmine - Curfews.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Swinging Steaks - Lost My Way.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Dorothy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Gospel.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Stella.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Disco.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Reggae.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The So So Glos - Emergency.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Wicked.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/St Vitus - Word Gets Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Celestial Shore - Die For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Facade.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/AvaLuna - Waterduct.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Punk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - One Minute Smile.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Blood To Bone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Tim Taler - Stalker.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Hendrix.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Summerghost.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hop Along - Sister Cities.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - All Souls Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - You Listen.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country2.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Clinic A.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Sirens.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Britpop.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Chris Durban - Celebrate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Angelsaint.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - On The Line.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/A Classic Education - NightOwl.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Together Alone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Titanium - Haunted Age.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Goodbye Bolero.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Secret Mountains - High Horse.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Wall Of Death - Femme.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - The Wind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Velvet Curtain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Johnny Lokke - Whisper To A Scream.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - Take A Step.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Jay Menon - Through My Eyes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Flags - 54.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Waltz For My Victims.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Easy Tiger.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Back From The Start.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hollow Ground - Left Blind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Sweet Lights - You Let Me Down.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Port St Willow - Stay Even.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Helado Negro - Mitad Del Mundo.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Black Bloc - If You Want Success.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Pennies.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Voelund - Comfort Lives In Belief.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Nos Palpitants.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Creepoid - OldTree.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - South Of The Water.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Lushlife - Toynbee Suite.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Matthew Entwistle - Dont You Ever.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Scarlet Brand - Les Fleurs Du Mal.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country1.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - Dont Let Go.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - 80s Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Atlantis Bound - It Was My Fault For Waiting.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Invisible Familiars - Disturbing Wildlife.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Cnoc An Tursa - Bannockburn.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hezekiah Jones - Borrowed Heart.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/BigTroubles - Phantom.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Remember December - C U Next Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Night Panther - Fire.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Long Wait - Back Home To Blue.stem.mp4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Musformer\nAlthough misleading, but the name seemed nice. The goal of this project is to try out multiple source separation models in speechbrain. The idea is to replicate building a model in the waveform domain directly. We see that multiple models try to solve the music sourse separation as a problem and most state-of-the-art models reach an SiSDR ratio of 9 to tackle the same. \n\nMost models working on this problem came into existence as a solution to the SDX (Sound Demixing) challenge. A few of the existing solutions are as follows: \n- **MMDenseLSTM**: Combines dense blocks with LSTMs for lightweight waveform separation.\n- **Demucs (v1/v2)**: U-Net with bidirectional LSTMs; later versions add transformers.\n- **ConvTasNet**: Efficient temporal convolutional network with learned encoder/decoder.\n- **DPRNNet**: Dual-path RNN with attention, offering SOTA results at higher compute costs.\n- **BandSplitRNN**: Separates audio into frequency bands processed by independent RNNs before recombination.\n- **Wave-U-Net**: Adapts medical imaging's U-Net architecture for waveform-based source separation with learned down/upsampling.\n- **Open-Unmix**: Spectrogram-based separation model using three-layer BiLSTMs with industry-standard implementation.\n- **ResUNetDecouple**: U-Net variant with residual connections that decouples magnitude and phase processing.\n- **TDCN++**: Improved temporal convolutional network with global skip connections and stacked dilation patterns.\n- **Spleeter**: Facebook's lightweight CNN-based separator using spectrogram masking with pretrained models.\n\nOf these, two models which catch one's eye are convtasnet, which seemed to provide good results with the most optimally efficient architecture and demucs, which uses an advanced convolutional architecure built on top of Wave-U-Net and stretches its performance to state-of-the-art levels.","metadata":{}},{"cell_type":"markdown","source":"## Disclaimers\nThis notebook refers to the tip of the iceberg of multiple approaches tried along the way in order to reach here. It provides for a summary of the underlying work done around trials and errors in speech separation. More about the same is appended in the Archives section at the end of this report notebook ","metadata":{}},{"cell_type":"markdown","source":"## Step1 : Gaining Context (Literature Review)\n\n### A bit about the Dataset\nFirst things first, lets talk about the dataset and the models a bit. MUSDB18 is the benchmark dataset for music source separation, containing 150 full-track recordings (100 for training, 50 for test) with isolated stems for vocals, drums, bass, and other instruments. It provides professionally mixed 44.1kHz stereo audio, enabling evaluation of waveform-domain separation models. The dataset covers diverse genres and production styles, making it ideal for testing real-world generalization. It has become the standard benchmark for models like Open-Unmix, Demucs, and D3Net, with SI-SDR and SDR as primary metrics. The included Python toolbox (musdb) provides data loading, evaluation, and stem mixing utilities. Musdb is however dependent on something known as STEM files i.e. music tensor files combined and compressed into one. We use the kaggle uploaded version of the dataset which saves us the trouble of downloading, unzipping and uploading the same. You can find the dataset here: https://www.kaggle.com/datasets/jakerr5280/musdb18-music-source-separation-dataset\n\n\n### The MusDB Package\nOne of the most helpful things available in the community to enhance the friendliness of this challenge is the [package provided by sigsep](https://github.com/sigsep/sigsep-mus-db/tree/master/musdb). Given that all the files are STEM format files i.e. a format extension which clubs multiple vectors into a cohesive mp4 format, the musdb package helps us read those using [FFMPEG](https://ffmpeg.org/). This allows us to load the vectors directly into the file without the need of \n\n### A bit about the models\n\n\n#### Demucs v2\n- Key Idea: A hybrid U-Net + Bi-LSTM architecture that processes raw waveforms.\n- U-Net Structure: Encoder-decoder with skip connections for multiscale feature extraction.\n- Bidirectional LSTMs: Added between encoder and decoder to capture long-term temporal relationships.\n\nImprovements in v2:\n- Deeper architecture with more layers.\n- Better training strategies (e.g., dynamic mixing).\n- Optional transformer layers in later variants.\n\nAdvantages:\n- Strong separation quality, especially for music.\n- Handles variable-length inputs well.\n\nLimitations:\n- Computationally heavier than ConvTasNet due to LSTMs.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Installing Stuff and Basic Setup\n","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install musdb\n!pip install mir_eval\n!pip install museval\n\n# Installing SpeechBrain via pip\nBRANCH = 'develop'\n!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n\n# Clone SpeechBrain repository\n!git clone https://github.com/speechbrain/speechbrain/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:47:55.648010Z","iopub.execute_input":"2025-04-16T18:47:55.648330Z","iopub.status.idle":"2025-04-16T18:49:59.745786Z","shell.execute_reply.started":"2025-04-16T18:47:55.648308Z","shell.execute_reply":"2025-04-16T18:49:59.744040Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"db_path = '/kaggle/input/musdb18-music-source-separation-dataset'\noutput_path = '/kaggle/working'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:49:59.747128Z","iopub.execute_input":"2025-04-16T18:49:59.747457Z","iopub.status.idle":"2025-04-16T18:49:59.753792Z","shell.execute_reply.started":"2025-04-16T18:49:59.747429Z","shell.execute_reply":"2025-04-16T18:49:59.752913Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nnp.float_ = np.float64\nimport musdb\n\nMUS_DB_PATH = db_path\n\nmus = musdb.DB(root=MUS_DB_PATH)\nmus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\nmus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\nmus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\nprint(mus_train[0])\nprint(mus_test[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:49:59.756252Z","iopub.execute_input":"2025-04-16T18:49:59.756515Z","iopub.status.idle":"2025-04-16T18:50:39.489948Z","shell.execute_reply.started":"2025-04-16T18:49:59.756496Z","shell.execute_reply":"2025-04-16T18:50:39.488753Z"}},"outputs":[{"name":"stdout","text":"A Classic Education - NightOwl\nAM Contra - Heart Peripheral\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%file dataset.py\n\n\nimport csv\nimport os\nimport sys\n\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport musdb\nimport numpy as np\nimport speechbrain as sb\nimport psutil\n\n\nclass MusDBDataset(Dataset):\n    def __init__(self, root, subset=\"train\", split=None, target_sr=8000, chunk_size=15):\n        \"\"\"\n        True lazy-loading for MUSDB\n        :param chunk_size: in seconds\n        \"\"\"\n        if split is not None:\n            self.db = musdb.DB(root=root, subsets=subset, split=split, is_wav=False)\n        else:\n            self.db = musdb.DB(root=root, subsets=subset, is_wav=False)\n        self.target_sr = target_sr\n        self.chunk_size = chunk_size\n        self.tracks = [{\n            \"path\": track.path,\n            \"duration\": track.duration,\n            \"rate\": track.rate,\n            \"stem_id\": track.stem_id  # Needed for STEM access\n        } for track in self.db.tracks]\n\n    def __len__(self):\n        return len(self.tracks)\n\n    def __getitem__(self, idx):\n        track_info = self.tracks[idx]\n        \n        # Load chunk directly from disk without full track loading\n        def load_stem_chunk(stem_name, random_chunk_val=0):\n            # MUSDB's internal lazy loading\n            track = self.db.tracks[idx]\n            if stem_name == \"mix\":\n                source = track\n            else:\n                source = track.targets[stem_name]\n            \n            # Calculate chunk bounds\n            start = random_chunk_val\n            stop = start + self.chunk_size * track_info[\"rate\"]\n            \n            # Load only the needed segment\n            audio = source.audio[start:stop]\n            \n            # Convert and resample\n            audio_tensor = torch.from_numpy(audio).float().permute(1, 0)\n            return torchaudio.functional.resample(\n                audio_tensor,\n                orig_freq=track_info[\"rate\"],\n                new_freq=self.target_sr\n            ).mean(dim=0, keepdim=False)\n            \n        \n            # chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n        \n            # resampled_chunks = []\n        \n            # for i in range(0, audio_tensor.shape[1], chunk_size):\n            #     chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n            #     resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n            #     resampled_chunks.append(resampled_chunk)\n        \n            # # Concatenate back the processed chunks\n            # # print(\"PROCESSING CHUNKS\")\n            # resampled_audio = torch.cat(resampled_chunks, dim=1)\n            # # print(resampled_audio.shape)\n            # resampled_audio = resampled_audio.mean(dim=0, keepdim=False)\n            # # print(resampled_audio.shape)\n            # # print(resampled_audio.shape)\n            # return resampled_audio\n        # if random_chunk_start_val:\n        \n        random_chunk_start_val = 0\n        \n        return {\n            \"mix_sig\": load_stem_chunk(\"mix\", random_chunk_start_val),\n            \"voc_sig\": load_stem_chunk(\"vocals\",random_chunk_start_val),\n            \"inst_sig\": load_stem_chunk(\"accompaniment\",random_chunk_start_val),\n            \"track_id\": track_info[\"stem_id\"]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:50:39.504681Z","iopub.execute_input":"2025-04-16T18:50:39.504924Z","iopub.status.idle":"2025-04-16T18:50:39.559846Z","shell.execute_reply.started":"2025-04-16T18:50:39.504902Z","shell.execute_reply":"2025-04-16T18:50:39.558161Z"}},"outputs":[{"name":"stdout","text":"Overwriting dataset.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 3: Building ConvTasNet\n\n#### ConvTasNet\n- Key Idea: A fully convolutional, end-to-end waveform model that avoids spectrograms entirely.\n- Encoder-Decoder: Uses 1D convolutions to learn a latent representation of the waveform.\n- Temporal Convolutional Network (TCN): Processes the latent features with stacked dilated convolutions for long-range dependencies.\n- Mask Estimation: Applies a soft mask in the latent space to separate sources.\n\nAdvantages:\n- Lightweight and parallelizable (no RNNs).\n- Strong performance on speech and music separation.\n\nLimitations:\n- May struggle with very long-term dependencies due to fixed receptive fields.\n\nPersonal Interests:\nConvtasnet proves to be one of the smallest models which generally speaking outperforms most other state-of-the-art models (in the waveform domain). This is intruiging because it uses a simple encoder decoder architecture with a masknet type bottleneck. The version we use in this has 3.3 Million parameters and is successfully able to detach vocals from its accompaniements\n\nReference and Sources:\n","metadata":{}},{"cell_type":"code","source":"%%file convtasnet-hparams.yaml\n# ################################\n# Model: ConvTasNet for Music Vocal Separation\n# https://arxiv.org/abs/2010.13154\n# Dataset : Musdb\n# ################################\n# Basic parameters\n# Seed needs to be set at top of yaml, before objects with parameters are made\n\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params\ndata_folder: !PLACEHOLDER\n\n\nexperiment_name: convtasnet\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\n\n# Experiment params\nprecision: fp16 # bf16, fp16 or fp32\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True # Save estimated sources on disk\nsample_rate: 16000\nn_audio_to_save: 10\nchunk_size: 20\n\n####################### Training Parameters ####################################\nN_epochs: 50\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999  # this is the upper limit for an acceptable loss\nnum_sources: 2\n\n\n# loss thresholding -- this thresholds the training loss\nthreshold_byloss: True\nthreshold: -30\n\n# Encoder parameters\nN_encoder_out: 256\n# out_channels: 256\nkernel_size: 16\nkernel_stride: 8\n\n# Dataloader options\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 1\n\n\n# Specifying the network\nEncoder: !new:speechbrain.lobes.models.dual_path.Encoder\n    kernel_size: !ref <kernel_size>\n    out_channels: !ref <N_encoder_out>\n\n\nMaskNet: !new:speechbrain.lobes.models.conv_tasnet.MaskNet\n    N: 256\n    B: 256\n    H: 512\n    P: 3\n    X: 6\n    R: 4\n    C: !ref <num_sources>\n    norm_type: 'gLN'\n    causal: True\n    mask_nonlinear: 'relu'\n\n\nDecoder: !new:speechbrain.lobes.models.dual_path.Decoder\n    in_channels: !ref <N_encoder_out>\n    out_channels: 1\n    kernel_size: !ref <kernel_size>\n    stride: !ref <kernel_stride>\n    bias: False\n\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder: !ref <Encoder>\n    decoder: !ref <Decoder>\n    masknet: !ref <MaskNet>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder: !ref <Encoder>\n        decoder: !ref <Decoder>\n        masknet: !ref <MaskNet>\n        counter: !ref <epoch_counter>\n        lr_scheduler: !ref <lr_scheduler>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:02:03.642278Z","iopub.execute_input":"2025-04-16T15:02:03.642687Z","iopub.status.idle":"2025-04-16T15:02:03.649851Z","shell.execute_reply.started":"2025-04-16T15:02:03.642659Z","shell.execute_reply":"2025-04-16T15:02:03.649020Z"}},"outputs":[{"name":"stdout","text":"Overwriting convtasnet-hparams.yaml\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"%%file convtasnet-train.py\n#!/usr/bin/env/python3\n\"\"\"\nRecipe for vocal separation using convtasnet\n\"\"\"\n\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nimport pdb\n\nimport musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\nfrom dataset import MusDBDataset\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.core import AMPConfig\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nimport time\nfrom torch.utils.data import DataLoader\n\nimport musdb\n\n\n# Define training procedure\nclass Separation(sb.Brain):\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix       \n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)      \n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_sources)],\n            dim=-1,\n        ).to(self.device)\n        \n        # Separation\n        mix_w = self.hparams.Encoder(mix)\n        est_mask = self.hparams.MaskNet(mix_w)\n        mix_w = torch.stack([mix_w] * self.hparams.num_sources)\n        sep_h = mix_w * est_mask\n        \n        # Decoding\n        est_source = torch.cat(\n            [\n                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n                for i in range(self.hparams.num_sources)\n            ],\n            dim=-1,\n        )\n\n        # pad estimates as per requirement \n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n        # print(\"INSIDE FIT BATCH\")\n        \n        amp = AMPConfig.from_name(self.precision)\n        should_step = (self.step % self.grad_accumulation_factor) == 0\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig] #mix_sig, voc_sig, inst_sig\n       \n        \n        predictions, targets = self.compute_forward(\n            mixture, targets, sb.Stage.TRAIN\n        )\n        loss = self.compute_objectives(predictions, targets)\n\n        if self.hparams.threshold_byloss:\n            th = self.hparams.threshold\n            loss = loss[loss > th]\n            if loss.nelement() > 0:\n                loss = loss.mean()\n        else:\n            loss = loss.mean()\n\n        if (\n            loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        ):  # the fix for computational problems\n            loss.backward()\n            if self.hparams.clip_grad_norm >= 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.optimizer.step()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_loader):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n\n        def is_silent(source, threshold=1e-6):\n            return np.max(np.abs(source[0])) < threshold or np.max(np.abs(source[1])) < threshold\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n            skip_cnt = 0\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n                   \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_sources, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n                    \n     \n                    if not is_silent(targets[0].t().cpu().numpy()) and not is_silent(predictions[0].t().detach().cpu().numpy()) and not is_silent(mixture_signal[0].t().detach().cpu().numpy()):\n                        \n                    \n                        sdr, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            predictions[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_baseline, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            mixture_signal[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_i = sdr.mean() - sdr_baseline.mean()\n    \n                        # Saving on a csv file\n                        row = {\n                            \"snt_id\": snt_id[0],\n                            \"sdr\": sdr.mean(),\n                            \"sdr_i\": sdr_i,\n                            \"si-snr\": -sisnr.item(),\n                            \"si-snr_i\": -sisnr_i.item(),\n                        }\n                        writer.writerow(row)\n    \n                        # Metric Accumulation\n                        all_sdrs.append(sdr.mean())\n                        all_sdrs_i.append(sdr_i.mean())\n                        all_sisnrs.append(-sisnr.item())\n                        all_sisnrs_i.append(-sisnr_i.item())\n    \n                    row = {\n                        \"snt_id\": \"avg\",\n                        \"sdr\": np.array(all_sdrs).mean(),\n                        \"sdr_i\": np.array(all_sdrs_i).mean(),\n                        \"si-snr\": np.array(all_sisnrs).mean(),\n                        \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                    }\n                    writer.writerow(row)\n                else:\n                    skip_cnt += 1\n                    print(f\"Warning: skipping silent target, this has happened {skip_cnt} times\")\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_sources):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n   \n\n    # Brain class initialization\n    separator = Separation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n \n    # Training\n        \n    # Usage with SpeechBrain\n    train_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    valid_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    test_data = MusDBDataset(hparams[\"db_path\"], subset=\"test\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    \n\n    # Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    test_loader = sb.dataio.dataloader.make_dataloader(\n        test_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n    # print(\"STARTING FIT\")\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # # Eval\n    separator.evaluate(test_loader, min_key=\"si-snr\")\n    separator.save_results(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:02:10.751916Z","iopub.execute_input":"2025-04-16T15:02:10.752560Z","iopub.status.idle":"2025-04-16T15:02:10.766428Z","shell.execute_reply.started":"2025-04-16T15:02:10.752533Z","shell.execute_reply":"2025-04-16T15:02:10.765607Z"}},"outputs":[{"name":"stdout","text":"Overwriting convtasnet-train.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!python convtasnet-train.py convtasnet-hparams.yaml --data_folder=db_path --device \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:02:19.284918Z","iopub.execute_input":"2025-04-16T15:02:19.285704Z","iopub.status.idle":"2025-04-16T15:36:54.737175Z","shell.execute_reply.started":"2025-04-16T15:02:19.285672Z","shell.execute_reply":"2025-04-16T15:36:54.736263Z"}},"outputs":[{"name":"stdout","text":"speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/convtasnet/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `False`\nspeechbrain.core - Using training precision: `--precision=bf16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 6.6M\n* Total Number of Parameters: 6.6M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\nspeechbrain.utils.epoch_loop - Going into epoch 1\n 17%|███▉                   | 14/81 [32:02<2:30:16, 134.58s/it, train_loss=14.1]^C\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## DPRNN (Dual-Path Recurrent Neural Network)\nKey Idea\nDPRNN is designed to model extremely long sequences efficiently by splitting input signals into smaller chunks and processing them through intra-chunk (local) and inter-chunk (global) RNNs. This dual-path approach addresses the limitations of traditional RNNs in handling long-term dependencies while maintaining low computational complexity.\n\nArchitecture\n- Encoder: Converts input waveforms (or time-frequency representations) into latent features using 1D convolutions (time-domain) or STFT (TF-domain).\n- Decoder: Reconstructs separated signals using transposed convolutions or overlap-add operations.\n\nDual-Path Processing:\n- Intra-chunk RNN: Models local patterns within short segments (e.g., 5.75-ms windows) 11.\n- Inter-chunk RNN: Captures global dependencies across segments, enabling utterance-level modeling 612.\n- Mask Estimation: Learns soft masks in the latent space to separate sources, similar to ConvTasNet but with RNN-based temporal modeling.\n\nAdvantages\n- Efficiency: Outperforms ConvTasNet in parameter efficiency (e.g., 20x smaller model than prior SOTA) 6.\n- Long-Sequence Modeling: Superior to 1D CNNs and vanilla RNNs for sequences with >100k time steps 611.\n- Versatility: Adaptable to time-domain (e.g., TasNet) and TF-domain tasks (e.g., speech enhancement) 52.\n\nLimitations\n- Computational Overhead: RNNs may introduce higher latency compared to fully convolutional models like ConvTasNet 11.\n- Fixed Chunking: Performance depends on optimal chunk size selection 6.\n\nPersonal Interests\nDPRNN’s innovative dual-path mechanism achieves state-of-the-art separation quality (e.g., WSJ0-2mix dataset) with minimal parameters, making it ideal for real-time applications like hearing aids and telecommunication. Its extension to variable speaker counts (e.g., Multi-Decoder DPRNN) further showcases its flexibility.","metadata":{}},{"cell_type":"code","source":"%%file dprnn-hparams.yaml\n# ################################\n# Model: DPRNN for Music vocal separation\n# https://arxiv.org/abs/2010.13154\n# Dataset : MusDB\n# ################################\n# Basic parameters\n# Seed needs to be set at top of yaml, before objects with parameters are made\n\n\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params\n\ndata_folder: !PLACEHOLDER\n\n\nexperiment_name: dprnn\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\n\n# Experiment params\nprecision: fp16 # bf16, fp16 or fp32\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True # Save estimated sources on disk\nsample_rate: 16000\nn_audio_to_save: 10\nchunk_size: 30\n\n####################### Training Parameters ####################################\nN_epochs: 5\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999  # this is the upper limit for an acceptable loss\nnum_sources: 2\n\n\n\n# loss thresholding -- this thresholds the training loss\nthreshold_byloss: True\nthreshold: -30\n\n# Encoder parameters\nN_encoder_out: 256\nout_channels: 256\nkernel_size: 16\nkernel_stride: 8\n\n# Dataloader options\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 3\n\n\n# Specifying the network\nEncoder: !new:speechbrain.lobes.models.dual_path.Encoder\n    kernel_size: !ref <kernel_size>\n    out_channels: !ref <N_encoder_out>\n\nintra: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n    num_layers: 1\n    input_size: !ref <out_channels>\n    hidden_channels: !ref <out_channels>\n    dropout: 0\n    bidirectional: True\n\ninter: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n    num_layers: 1\n    input_size: !ref <out_channels>\n    hidden_channels: !ref <out_channels>\n    dropout: 0\n    bidirectional: True\n\nMaskNet: !new:speechbrain.lobes.models.dual_path.Dual_Path_Model\n    num_spks: !ref <num_sources>\n    in_channels: !ref <N_encoder_out>\n    out_channels: !ref <out_channels>\n    num_layers: 6\n    K: 250\n    intra_model: !ref <intra>\n    inter_model: !ref <inter>\n    norm: ln\n    linear_layer_after_inter_intra: True\n    skip_around_intra: True\n\nDecoder: !new:speechbrain.lobes.models.dual_path.Decoder\n    in_channels: !ref <N_encoder_out>\n    out_channels: 1\n    kernel_size: !ref <kernel_size>\n    stride: !ref <kernel_stride>\n    bias: False\n\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder: !ref <Encoder>\n    decoder: !ref <Decoder>\n    masknet: !ref <MaskNet>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder: !ref <Encoder>\n        decoder: !ref <Decoder>\n        masknet: !ref <MaskNet>\n        counter: !ref <epoch_counter>\n        lr_scheduler: !ref <lr_scheduler>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:55:28.543601Z","iopub.execute_input":"2025-04-16T18:55:28.543933Z","iopub.status.idle":"2025-04-16T18:55:28.553931Z","shell.execute_reply.started":"2025-04-16T18:55:28.543910Z","shell.execute_reply":"2025-04-16T18:55:28.552704Z"}},"outputs":[{"name":"stdout","text":"Overwriting dprnn-hparams.yaml\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%file dprnn-train.py\n#!/usr/bin/env/python3\n\"\"\"\nRecipe for vocal separation using convtasnet\n\"\"\"\n\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nimport pdb\n\nimport musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\nfrom dataset import MusDBDataset\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.core import AMPConfig\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nimport time\nfrom torch.utils.data import DataLoader\n\nimport musdb\n\n\n# Define training procedure\nclass Separation(sb.Brain):\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix       \n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)      \n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_sources)],\n            dim=-1,\n        ).to(self.device)\n        \n        # Separation\n        mix_w = self.hparams.Encoder(mix)\n        est_mask = self.hparams.MaskNet(mix_w)\n        mix_w = torch.stack([mix_w] * self.hparams.num_sources)\n        sep_h = mix_w * est_mask\n        \n        # Decoding\n        est_source = torch.cat(\n            [\n                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n                for i in range(self.hparams.num_sources)\n            ],\n            dim=-1,\n        )\n\n        # pad estimates as per requirement \n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n        # print(\"INSIDE FIT BATCH\")\n        \n        amp = AMPConfig.from_name(self.precision)\n        should_step = (self.step % self.grad_accumulation_factor) == 0\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig] #mix_sig, voc_sig, inst_sig\n       \n        \n        predictions, targets = self.compute_forward(\n            mixture, targets, sb.Stage.TRAIN\n        )\n        loss = self.compute_objectives(predictions, targets)\n\n        if self.hparams.threshold_byloss:\n            th = self.hparams.threshold\n            loss = loss[loss > th]\n            if loss.nelement() > 0:\n                loss = loss.mean()\n        else:\n            loss = loss.mean()\n\n        if (\n            loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        ):  # the fix for computational problems\n            loss.backward()\n            if self.hparams.clip_grad_norm >= 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.optimizer.step()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_loader):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n\n        def is_silent(source, threshold=1e-6):\n            return np.max(np.abs(source[0])) < threshold or np.max(np.abs(source[1])) < threshold\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n            skip_cnt = 0\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n                   \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_sources, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n                    \n     \n                    if not is_silent(targets[0].t().cpu().numpy()) and not is_silent(predictions[0].t().detach().cpu().numpy()) and not is_silent(mixture_signal[0].t().detach().cpu().numpy()):\n                        \n                    \n                        sdr, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            predictions[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_baseline, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            mixture_signal[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_i = sdr.mean() - sdr_baseline.mean()\n    \n                        # Saving on a csv file\n                        row = {\n                            \"snt_id\": snt_id[0],\n                            \"sdr\": sdr.mean(),\n                            \"sdr_i\": sdr_i,\n                            \"si-snr\": -sisnr.item(),\n                            \"si-snr_i\": -sisnr_i.item(),\n                        }\n                        writer.writerow(row)\n    \n                        # Metric Accumulation\n                        all_sdrs.append(sdr.mean())\n                        all_sdrs_i.append(sdr_i.mean())\n                        all_sisnrs.append(-sisnr.item())\n                        all_sisnrs_i.append(-sisnr_i.item())\n    \n                    row = {\n                        \"snt_id\": \"avg\",\n                        \"sdr\": np.array(all_sdrs).mean(),\n                        \"sdr_i\": np.array(all_sdrs_i).mean(),\n                        \"si-snr\": np.array(all_sisnrs).mean(),\n                        \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                    }\n                    writer.writerow(row)\n                else:\n                    skip_cnt += 1\n                    print(f\"Warning: skipping silent target, this has happened {skip_cnt} times\")\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_sources):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n   \n\n    # Brain class initialization\n    separator = Separation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n \n    # Training\n        \n    # Usage with SpeechBrain\n    train_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    valid_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    test_data = MusDBDataset(hparams[\"db_path\"], subset=\"test\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    \n\n    # Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    test_loader = sb.dataio.dataloader.make_dataloader(\n        test_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n    # print(\"STARTING FIT\")\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # # Eval\n    separator.evaluate(test_loader, min_key=\"si-snr\")\n    separator.save_results(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:55:31.291058Z","iopub.execute_input":"2025-04-16T18:55:31.291397Z","iopub.status.idle":"2025-04-16T18:55:31.304908Z","shell.execute_reply.started":"2025-04-16T18:55:31.291374Z","shell.execute_reply":"2025-04-16T18:55:31.303236Z"}},"outputs":[{"name":"stdout","text":"Overwriting dprnn-train.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!python dprnn-train.py dprnn-hparams.yaml --data_folder=db_path --device \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:55:34.574722Z","iopub.execute_input":"2025-04-16T18:55:34.575053Z","iopub.status.idle":"2025-04-16T19:24:40.680684Z","shell.execute_reply.started":"2025-04-16T18:55:34.575030Z","shell.execute_reply":"2025-04-16T19:24:40.679370Z"}},"outputs":[{"name":"stdout","text":"speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/dprnn/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `False`\nspeechbrain.core - Using training precision: `--precision=bf16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 14.6M\n* Total Number of Parameters: 14.6M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\nspeechbrain.utils.epoch_loop - Going into epoch 1\n 16%|███▋                   | 13/81 [28:33<2:24:15, 127.29s/it, train_loss=10.2]^C\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Demucs\n","metadata":{}},{"cell_type":"code","source":"%%file demucsModels.py\n\nimport torch\nfrom torch import nn\nfrom speechbrain.nnet.CNN import Conv1d, ConvTranspose1d\n# from speechbrain.nnet.activations import GLU\nfrom speechbrain.lobes.models.beats import GLU_Linear\nfrom torch.nn import GLU\nfrom speechbrain.nnet.RNN import LSTM\nfrom speechbrain.nnet.linear import Linear\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = Conv1d(\n            out_channels=out_channels,\n            in_channels=in_channels,\n            kernel_size=8,\n            stride=4,\n            # default_padding=2,\n            skip_transpose=True,\n        )\n        self.glu_conv = Conv1d(\n            out_channels=2*out_channels,\n            in_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            skip_transpose=True,\n        )\n        self.relu = torch.nn.ReLU()\n        self.glu = GLU(dim=1)\n\n    def forward(self, x):\n        # print(x.size())\n        x = self.relu(self.conv(x))\n        # print(\"#########checking encoder\")\n        # print(x.size())\n        # print(self.glu_conv(x).size())\n        # print(\"#########encoder ended\")\n        x = self.glu(self.glu_conv(x))\n        # print(x.size())\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.glu_conv = Conv1d(\n            out_channels=2*in_channels,\n            in_channels=in_channels,\n            kernel_size=1,\n            stride=1,\n            skip_transpose=True,\n        )\n        self.conv_tr = ConvTranspose1d(\n            out_channels=out_channels,\n            in_channels=in_channels,  # After GLU split\n            kernel_size=8,\n            stride=4,\n            # padding=2,\n            # output_padding=2,\n            skip_transpose=True,\n        )\n        self.glu = GLU(dim=1)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x, skip=None):\n\n        if skip is not None:\n\n            # T changed after conv1d in encoder, fix it here\n            T_x = x.size(-1)\n            T_skip = skip.size(-1)\n\n            # # Case 1: Decoder output is longer\n            # if T_skip > T_x:\n            #     # Center-trim decoder output\n            #     start = (T_skip - T_x) // 2\n            #     skip = skip[..., start : start + T_x]\n\n            # # Case 2: Skip is longer\n            # elif T_skip < T_x:\n            #     # Center-pad decoder output\n            #     pad = T_x - T_skip\n            #     skip = nn.functional.pad(skip, (pad // 2, pad - pad // 2))\n\n            if T_skip > T_x:\n                # Trim from the end of 'skip' to match 'x'\n                skip = skip[..., :T_x]  # Keeps the first T_x samples\n\n            # Case 2: Skip is shorter than target (x)\n            elif T_skip < T_x:\n                # Pad zeros at the end of 'skip' to match 'x'\n                pad = T_x - T_skip\n                skip = nn.functional.pad(skip, (0, pad))\n\n\n            x = x + skip\n\n\n\n\n        x = self.glu(self.glu_conv(x))\n\n        x = self.relu(self.conv_tr(x))\n\n        return x\n\n\n\n\nclass SourceSeparator(nn.Module):\n    def __init__(self, in_channels, out_channels=2, num_sources=4):\n        \"\"\"\n        Args:\n            C_in: Input channels from last decoder (typically 8)\n            C_out: Output channels per source (2 for stereo)\n            num_sources: Number of sources to separate (e.g. 4 for vocals, drums, bass, other)\n        \"\"\"\n        super().__init__()\n        # Final linear layer (no activation)\n        self.output_proj = Linear(\n            input_size=in_channels,\n            n_neurons=num_sources * out_channels,  # 4 sources * 2 channels = 8\n            bias=True\n        )\n        self.num_sources = num_sources\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        \"\"\"\n        Input: [batch, C_in, time]\n        Output: [batch, num_sources, out_channels, time]\n        \"\"\"\n        # Permute to [batch, time, features]\n        # print(x.size())\n        x = x.permute(0, 2, 1)\n\n        # Project to source waveforms\n        x = self.output_proj(x)  # [batch, time, num_sources*out_channels]\n        # print(x.size())\n\n        # Reshape to separated sources\n        # x = x.view(x.size(0), -1, self.num_sources, self.out_channels)\n        x = x.permute(0, 2, 1)\n        # print(x.size())\n        # Return as [batch, sources, time]\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:58:44.137116Z","iopub.execute_input":"2025-04-16T19:58:44.137996Z","iopub.status.idle":"2025-04-16T19:58:44.146059Z","shell.execute_reply.started":"2025-04-16T19:58:44.137963Z","shell.execute_reply":"2025-04-16T19:58:44.144848Z"}},"outputs":[{"name":"stdout","text":"Overwriting demucsModels.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"%%file demucs-hparams.yaml\n\n# ################################\n# Model: Demucs for source separation\n# https://hal.science/hal-02379796/document\n# Dataset : Musdb\n# ################################\n# Basic parameters\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params (unchanged from DPRNN)\ndata_folder: !PLACEHOLDER\n\nexperiment_name: demucs\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\n\n# Experiment params\nprecision: fp32\nnum_sources: 2\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True\nsample_rate: 16000\nn_audio_to_save: 10\nchunk_size: 30\n\n####################### Training Parameters ####################################\n\nN_epochs: 5\nbatch_size: 1\nlr: 0.0005\nclip_grad_norm: 5\nloss_upper_lim: 999999\nlimit_training_signal_len: False\ntraining_signal_len: 32000000\n\n\n# Data augmentation (unchanged)\nuse_wavedrop: False\nuse_rand_shift: False\nmin_shift: -8000\nmax_shift: 8000\n\n\n# Frequency/time drop (unchanged)\ndrop_freq: !new:speechbrain.augment.time_domain.DropFreq\n    drop_freq_low: 0\n    drop_freq_high: 1\n    drop_freq_count_low: 1\n    drop_freq_count_high: 3\n    drop_freq_width: 0.05\n\ndrop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n    drop_length_low: 1000\n    drop_length_high: 2000\n    drop_count_low: 1\n    drop_count_high: 5\n\nthreshold_byloss: True\nthreshold: -30\n\n################ Demucs Specific Parameters #############################\n## for Demucs V3/4\n# # Fourier Transform Parameters\n# n_fft: 2048\n# hop_length: 512\n\n\nkernel_size: 16\n# kernel_stride: 8\n\n# Dataloader options (unchanged)\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 3\n\n######################## Network Definition ####################################\n\n\nEncoder1: !new:demucsModels.EncoderBlock\n    in_channels: 1\n    # kernel_size: !ref <kernel_size>\n    out_channels: 64\n\n\nEncoder2: !new:demucsModels.EncoderBlock\n    in_channels: 64\n    # kernel_size: !ref <kernel_size>\n    out_channels: 128\n\n\nEncoder3: !new:demucsModels.EncoderBlock\n    in_channels: 128\n    # kernel_size: !ref <kernel_size>\n    out_channels: 256\n\n\nEncoder4: !new:demucsModels.EncoderBlock\n    in_channels: 256\n    # kernel_size: !ref <kernel_size>\n    out_channels: 512\n\n\nEncoder5: !new:demucsModels.EncoderBlock\n    in_channels: 512\n    # kernel_size: !ref <kernel_size>\n    out_channels: 1024\n\n\nEncoder6: !new:demucsModels.EncoderBlock\n    in_channels: 1024\n    # kernel_size: !ref <kernel_size>\n    out_channels: 2048\n\n\n\n\nDecoder6: !new:demucsModels.DecoderBlock\n    in_channels: 2048\n    out_channels: 1024\n    # # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n\n\nDecoder5: !new:demucsModels.DecoderBlock\n    in_channels: 1024\n    out_channels: 512\n    # # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n\n\nDecoder4: !new:demucsModels.DecoderBlock\n    in_channels: 512\n    out_channels: 256\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n\n\nDecoder3: !new:demucsModels.DecoderBlock\n    in_channels: 256\n    out_channels: 128\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n\n\nDecoder2: !new:demucsModels.DecoderBlock\n    in_channels: 128\n    out_channels: 64\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n\n\nDecoder1: !new:demucsModels.DecoderBlock\n    in_channels: 64\n    out_channels: 4\n    # kernel_size: !ref <kernel_size>\n    # stride: !ref <kernel_stride>\n\n\nLinear: !new:speechbrain.nnet.linear.Linear\n    input_size: 4096\n    bias: False\n    n_neurons: 2048\n\nBiLSTM: !new:speechbrain.nnet.RNN.LSTM\n    hidden_size: 2048\n    input_size: 2048\n    num_layers: 2\n    bidirectional: True\n    # batch_first: True\n\nLinearSeparator: !new:demucsModels.SourceSeparator\n    in_channels: 4\n    out_channels: 1\n    num_sources: !ref <num_sources>\n\n\n######################## Remaining Config ######################################\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\n# loss: !name:speechbrain.nnet.losses.mse_loss\n# loss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\nloss: !name:speechbrain.nnet.losses.l1_loss\n\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 50\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder1: !ref <Encoder1>\n    encoder2: !ref <Encoder2>\n    encoder3: !ref <Encoder3>\n    encoder4: !ref <Encoder4>\n    encoder5: !ref <Encoder5>\n    encoder6: !ref <Encoder6>\n    lstm: !ref <BiLSTM>\n    linear: !ref <Linear>\n    decoder6: !ref <Decoder6>\n    decoder5: !ref <Decoder5>\n    decoder4: !ref <Decoder4>\n    decoder3: !ref <Decoder3>\n    decoder2: !ref <Decoder2>\n    decoder1: !ref <Decoder1>\n    linearSeparator: !ref <LinearSeparator>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder1: !ref <Encoder1>\n        encoder2: !ref <Encoder2>\n        encoder3: !ref <Encoder3>\n        encoder4: !ref <Encoder4>\n        encoder5: !ref <Encoder5>\n        encoder6: !ref <Encoder6>\n        lstm: !ref <BiLSTM>\n        linear: !ref <Linear>\n        decoder6: !ref <Decoder6>\n        decoder5: !ref <Decoder5>\n        decoder4: !ref <Decoder4>\n        decoder3: !ref <Decoder3>\n        decoder2: !ref <Decoder2>\n        decoder1: !ref <Decoder1>\n        linearSeparator: !ref <LinearSeparator>\n        counter: !ref <epoch_counter>\n\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:58:46.464986Z","iopub.execute_input":"2025-04-16T19:58:46.465264Z","iopub.status.idle":"2025-04-16T19:58:46.472726Z","shell.execute_reply.started":"2025-04-16T19:58:46.465243Z","shell.execute_reply":"2025-04-16T19:58:46.471923Z"}},"outputs":[{"name":"stdout","text":"Overwriting demucs-hparams.yaml\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"%%file demucs-train.py\n#!/usr/bin/env/python3\n\"\"\"Recipe for training a neural speech separation system on the wsjmix\ndataset. The system employs an encoder, a decoder, and a masking network.\n\nTo run this recipe, do the following:\n> python train.py hparams/sepformer.yaml\n> python train.py hparams/dualpath_rnn.yaml\n> python train.py hparams/convtasnet.yaml\n\nThe experiment file is flexible enough to support different neural\nnetworks. By properly changing the parameter files, you can try\ndifferent architectures. The script supports both wsj2mix and\nwsj3mix.\n\n\nAuthors\n * Cem Subakan 2020\n * Mirco Ravanelli 2020\n * Samuele Cornell 2020\n * Mirko Bronzi 2020\n * Jianyuan Zhong 2020\n\"\"\"\n## CHECKPOINT\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\n\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nfrom speechbrain.nnet.CNN import Conv1d, ConvTranspose1d\n# from speechbrain.nnet.activations import GLU\nfrom speechbrain.lobes.models.beats import GLU_Linear\nfrom torch.nn import GLU\nfrom speechbrain.nnet.RNN import LSTM\nfrom speechbrain.nnet.linear import Linear\nfrom demucsModels import EncoderBlock, DecoderBlock\nfrom speechbrain.nnet.losses import get_si_snr_with_pitwrapper\nfrom dataset import MusDBDataset\n\nfrom torch.utils.data import Dataset\nimport musdb\nnp.float_ = np.float64\n\n\n\n\n\n# Define training procedure\nclass DemucsSeparation(sb.Brain):\n    # def on_fit_start(self):\n\n\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix\n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)\n\n        # Convert targets to tensor\n        # print([targets[i][0].size() for i in range(self.hparams.num_sources)])\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_sources)],\n            dim=-1,\n        ).to(self.device)\n        # print(\"comp for 1\")\n        print(mix.size())\n        print(targets.size())\n        mix=mix.unsqueeze(1)\n        targets=targets.permute(0,2,1)\n        # targets=targets.mean(dim=1,keepdim=False).permute(0,2,1)\n        print(mix.size())\n        print(targets.size())\n\n\n        mix_enc_1 = self.modules.encoder1(mix)\n\n        mix_enc_2 = self.modules.encoder2(mix_enc_1)\n\n        mix_enc_3 = self.modules.encoder3(mix_enc_2)\n        mix_enc_4 = self.modules.encoder4(mix_enc_3)\n        mix_enc_5 = self.modules.encoder5(mix_enc_4)\n        mix_enc_6 = self.modules.encoder6(mix_enc_5)\n\n        lstm_in = mix_enc_6.permute(0,2,1)\n        lstm_out, _ = self.modules.lstm(lstm_in) # outputs both -- outputs as well as hidden states -- we dont need hidden states\n        # print(lstm_out.size())\n        lin_out = self.modules.linear(lstm_out)\n        # print(lin_out.size())\n        lin_out = lin_out.permute(0,2,1)\n\n        mix_dec_6 = self.modules.decoder6(lin_out, skip=mix_enc_6)\n        mix_dec_5 = self.modules.decoder5(mix_dec_6, skip=mix_enc_5)\n        mix_dec_4 = self.modules.decoder4(mix_dec_5, skip=mix_enc_4)\n        mix_dec_3 = self.modules.decoder3(mix_dec_4, skip=mix_enc_3)\n        mix_dec_2 = self.modules.decoder2(mix_dec_3, skip=mix_enc_2)\n        mix_dec_1 = self.modules.decoder1(mix_dec_2, skip=mix_enc_1)\n\n        mix_out = self.modules.linearSeparator(mix_dec_1)\n        # print(\"comp for 2\")\n        # print(mix_out.size())\n\n\n        est_source = mix_out\n\n\n\n        # T changed after conv1d in encoder, fix it here\n        T_origin = targets.size(2)\n        T_est = est_source.size(2)\n\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, : , :T_origin]\n        print(\"comp for 3\")\n        print(est_source.size())\n        print(targets.size())\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        # print(\"comp obj\")\n        targets = targets.permute(0,2,1)\n        predictions = predictions.permute(0,2,1)\n        # print(targets.size())\n        # print(predictions.size())\n\n        # print(targets.mean(dim=1,keepdim=False).permute(0,2,1).size())\n        # print(predictions.mean(dim=1,keepdim=False).permute(0,2,1).size())\n        return self.hparams.loss(targets, predictions) # for pitwrapper\n        # return self.hparams.loss(targets=targets, predictions=predictions)\n## CHECKPOINT\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with self.training_ctx:\n            predictions, targets = self.compute_forward(\n                mixture, targets, sb.Stage.TRAIN\n            )\n\n            loss = self.compute_objectives(predictions, targets)\n\n            # hard threshold the easy dataitems\n            if self.hparams.threshold_byloss:\n                th = self.hparams.threshold\n                loss = loss[loss > th]\n                if loss.nelement() > 0:\n                    loss = loss.mean()\n            else:\n                loss = loss.mean()\n\n        if loss.nelement() > 0 and loss < self.hparams.loss_upper_lim:\n            self.scaler.scale(loss).backward()\n            if self.hparams.clip_grad_norm >= 0:\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id, mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id, mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def save_results(self, test_loader):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n        # test_loader = sb.dataio.dataloader.make_dataloader(\n        #     test_data, **self.hparams.dataloader_opts\n        # )\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n\n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    # predictions = predictions.permute(0,2,1)\n                    # targets = targets.permute(0,2,1)\n                    print(predictions.size())\n                    print(targets.size())\n                    # sisnr = self.compute_objectives(predictions, targets)\n                    sisnr = get_si_snr_with_pitwrapper(predictions,targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_sources, dim=-1\n                    )\n                    #.permute(0,2,1)\n                    print(\"---------------------\")\n                    # print(mixture.size())\n                    print(mixture_signal.size())\n\n                    mixture_signal = mixture_signal.to(targets.device)\n                    # sisnr_baseline = self.compute_objectives(\n                    #     mixture_signal, targets\n                    # )\n                    sisnr_baseline = get_si_snr_with_pitwrapper(mixture_signal, targets)\n                    sisnr_i = sisnr - sisnr_baseline\n\n                    # Compute SDR\n                    sdr, _, _, _ = bss_eval_sources(\n                        targets[0].mean(dim=1).t().cpu().numpy(),\n                        predictions[0].mean(dim=1).t().detach().cpu().numpy(),\n                    )\n\n                    sdr_baseline, _, _, _ = bss_eval_sources(\n                        targets[0].mean(dim=1).t().cpu().numpy(),\n                        mixture_signal[0].mean(dim=1).t().detach().cpu().numpy(),\n                    )\n\n                    sdr_i = sdr.mean() - sdr_baseline.mean()\n\n                    # Saving on a csv file\n                    row = {\n                        \"snt_id\": snt_id[0],\n                        \"sdr\": sdr.mean(),\n                        \"sdr_i\": sdr_i,\n                        \"si-snr\": -sisnr.item(),\n                        \"si-snr_i\": -sisnr_i.item(),\n                    }\n                    writer.writerow(row)\n\n                    # Metric Accumulation\n                    all_sdrs.append(sdr.mean())\n                    all_sdrs_i.append(sdr_i.mean())\n                    all_sisnrs.append(-sisnr.item())\n                    all_sisnrs_i.append(-sisnr_i.item())\n\n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n        targets = targets.permute(0,2,1)\n        predictions = predictions.permute(0,2,1)\n        for ns in range(self.hparams.num_sources):\n            # Estimated source\\\n            \n            print(\"------- in here --------------\")\n            print(predictions.size())\n            signal = predictions[0, :, ns]\n            print(signal.size())\n            signal = signal / signal.abs().max()\n            # signal = signal / signal.abs().max(dim=1, keepdim=True)[0]\n            print(signal.size())\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        print(signal.size())\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.cpu(), self.hparams.sample_rate\n        )\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n\n\n        # Usage with SpeechBrain\n    train_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    valid_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    test_data = MusDBDataset(hparams[\"db_path\"], subset=\"test\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n\n\n    # Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    test_loader = sb.dataio.dataloader.make_dataloader(\n        test_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n\n    # Brain class initialization\n    separator = DemucsSeparation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n\n\n    # Training\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # Eval\n    separator.evaluate(test_loader, min_key=\"si-snr\")\n    separator.save_results(test_loader)\n    ## CHECKPOINT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:55:00.052541Z","iopub.execute_input":"2025-04-16T20:55:00.053412Z","iopub.status.idle":"2025-04-16T20:55:00.064142Z","shell.execute_reply.started":"2025-04-16T20:55:00.053383Z","shell.execute_reply":"2025-04-16T20:55:00.063229Z"}},"outputs":[{"name":"stdout","text":"Overwriting demucs-train.py\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"!python3 demucs-train.py demucs-hparams.yaml --data_folder=db_path --device \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:55:12.483447Z","iopub.execute_input":"2025-04-16T20:55:12.484224Z","iopub.status.idle":"2025-04-16T20:56:43.951729Z","shell.execute_reply.started":"2025-04-16T20:55:12.484198Z","shell.execute_reply":"2025-04-16T20:56:43.950562Z"}},"outputs":[{"name":"stdout","text":"speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/demucs/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `False`\nspeechbrain.core - Using training precision: `--precision=fp32`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - DemucsSeparation Model Statistics:\n* Total Number of Trainable Parameters: 243.3M\n* Total Number of Parameters: 243.3M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\nspeechbrain.utils.epoch_loop - Going into epoch 1\n  0%|                                                    | 0/81 [00:00<?, ?it/s]torch.Size([1, 480000])\ntorch.Size([1, 480000, 2])\ntorch.Size([1, 1, 480000])\ntorch.Size([1, 2, 480000])\ncomp for 3\ntorch.Size([1, 2, 480000])\ntorch.Size([1, 2, 480000])\n  1%|▎                         | 1/81 [00:23<31:24, 23.55s/it, train_loss=0.364]torch.Size([1, 480000])\ntorch.Size([1, 480000, 2])\ntorch.Size([1, 1, 480000])\ntorch.Size([1, 2, 480000])\ncomp for 3\ntorch.Size([1, 2, 480000])\ntorch.Size([1, 2, 480000])\n  2%|▋                         | 2/81 [00:36<22:55, 17.41s/it, train_loss=0.364]torch.Size([1, 480000])\ntorch.Size([1, 480000, 2])\ntorch.Size([1, 1, 480000])\ntorch.Size([1, 2, 480000])\ncomp for 3\ntorch.Size([1, 2, 480000])\ntorch.Size([1, 2, 480000])\n  4%|▉                         | 3/81 [00:49<19:41, 15.14s/it, train_loss=0.363]torch.Size([1, 480000])\ntorch.Size([1, 480000, 2])\ntorch.Size([1, 1, 480000])\ntorch.Size([1, 2, 480000])\ncomp for 3\ntorch.Size([1, 2, 480000])\ntorch.Size([1, 2, 480000])\n  5%|█▎                        | 4/81 [01:00<17:46, 13.85s/it, train_loss=0.363]^C\n  5%|█▎                        | 4/81 [01:03<20:21, 15.86s/it, train_loss=0.363]\nspeechbrain.core - Exception:\nTraceback (most recent call last):\n  File \"/kaggle/working/demucs-train.py\", line 465, in <module>\n    separator.fit(\n  File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1585, in fit\n    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n  File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1404, in _fit_train\n    for batch in t:\n  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 1181, in __iter__\n    for obj in iterable:\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/kaggle/working/dataset.py\", line 93, in __getitem__\n    \"inst_sig\": load_stem_chunk(\"accompaniment\",random_chunk_start_val),\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/dataset.py\", line 58, in load_stem_chunk\n    audio = source.audio[start:stop]\n            ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/musdb/audio_classes.py\", line None, in audio\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## Step 3: Baseline Models\n\nWe can use the baseline models available in torch as well as in the original repository of demucs in order to have a fair comparison of our implementations.\n\nWe use the repository created by facebook research in order to implement the demucs baseline model. We use their pretrained model on our dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Installing SpeechBrain via pip\nBRANCH = 'v2'\n!python -m pip install git+https://github.com/facebookresearch/demucs.git@$BRANCH\n\n# Clone Demucs repository\n!git clone https://github.com/facebookresearch/demucs.git --branch 'v2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:05:42.628646Z","iopub.execute_input":"2025-04-14T00:05:42.629606Z","iopub.status.idle":"2025-04-14T00:07:31.067872Z","shell.execute_reply.started":"2025-04-14T00:05:42.629556Z","shell.execute_reply":"2025-04-14T00:07:31.066756Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/facebookresearch/demucs.git@v2\n  Cloning https://github.com/facebookresearch/demucs.git (to revision v2) to /tmp/pip-req-build-gq0lj0fk\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/demucs.git /tmp/pip-req-build-gq0lj0fk\n  Running command git checkout -b v2 --track origin/v2\n  Switched to a new branch 'v2'\n  Branch 'v2' set up to track remote branch 'v2' from 'origin'.\n  Resolved https://github.com/facebookresearch/demucs.git to commit 64ed2cb029301743b2714b3c8fe930c00945842c\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting diffq<0.2.0 (from demucs==2.0.3)\n  Downloading diffq-0.1.1.tar.gz (34 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting lameenc>=1.2 (from demucs==2.0.3)\n  Downloading lameenc-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (9.9 kB)\nCollecting julius>=0.2.3 (from demucs==2.0.3)\n  Downloading julius-0.2.7.tar.gz (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (1.26.4)\nRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (2.5.1+cu124)\nRequirement already satisfied: torchaudio>=0.8 in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (2.5.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.1->demucs==2.0.3)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.1->demucs==2.0.3)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.1->demucs==2.0.3)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.1->demucs==2.0.3)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.1->demucs==2.0.3)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.1->demucs==2.0.3)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.1->demucs==2.0.3)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.1->demucs==2.0.3) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.1->demucs==2.0.3) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->demucs==2.0.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->demucs==2.0.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->demucs==2.0.3) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->demucs==2.0.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->demucs==2.0.3) (2024.2.0)\nDownloading lameenc-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (249 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: demucs, diffq, julius\n  Building wheel for demucs (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for demucs: filename=demucs-2.0.3-py3-none-any.whl size=44652 sha256=c0dc9e204449963f77b54e8a5be633f3eb2c323ef86bff138a95d604900f3ac0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0y94p01m/wheels/13/63/ef/f135563cc28c964ceb0365ecf358eb915d64241f3319c4b0a5\n  Building wheel for diffq (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for diffq: filename=diffq-0.1.1-py3-none-any.whl size=18962 sha256=9bca7d9244676b88702dd9fb7ce92befd9ee36e3f2136e55943623322592be1c\n  Stored in directory: /root/.cache/pip/wheels/e2/9c/6e/98f7ff95859ab18d48294f909735bb4bda7d898272ff045a49\n  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21869 sha256=7bc669c3ecdda33cbc8891dae77b233a827704735a4abea0dd31f8ad38f1c717\n  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\nSuccessfully built demucs diffq julius\nInstalling collected packages: lameenc, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, julius, diffq, demucs\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed demucs-2.0.3 diffq-0.1.1 julius-0.2.7 lameenc-1.8.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCloning into 'demucs'...\nremote: Enumerating objects: 5813, done.\u001b[K\nremote: Total 5813 (delta 0), reused 0 (delta 0), pack-reused 5813 (from 1)\u001b[K\nReceiving objects: 100% (5813/5813), 76.72 MiB | 39.14 MiB/s, done.\nResolving deltas: 100% (1130/1130), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/demucs.git --branch 'v2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:14:52.827437Z","iopub.execute_input":"2025-04-14T01:14:52.827748Z","iopub.status.idle":"2025-04-14T01:14:56.144885Z","shell.execute_reply.started":"2025-04-14T01:14:52.827718Z","shell.execute_reply":"2025-04-14T01:14:56.143695Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'demucs'...\nremote: Enumerating objects: 5813, done.\u001b[K\nremote: Total 5813 (delta 0), reused 0 (delta 0), pack-reused 5813 (from 1)\u001b[K\nReceiving objects: 100% (5813/5813), 76.73 MiB | 41.75 MiB/s, done.\nResolving deltas: 100% (1125/1125), done.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!ls demucs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:17:05.784321Z","iopub.execute_input":"2025-04-14T01:17:05.784625Z","iopub.status.idle":"2025-04-14T01:17:05.906019Z","shell.execute_reply.started":"2025-04-14T01:17:05.784599Z","shell.execute_reply":"2025-04-14T01:17:05.904698Z"}},"outputs":[{"name":"stdout","text":"baselines\t    docs\t\t  Makefile\t    run.py\nCODE_OF_CONDUCT.md  dora.py\t\t  MANIFEST.in\t    run_slurm.py\nCONTRIBUTING.md     environment-cpu.yml   README.md\t    setup.cfg\ndemucs\t\t    environment-cuda.yml  requirements.txt  setup.py\nDemucs.ipynb\t    hubconf.py\t\t  results\t    test.mp3\ndemucs.png\t    LICENSE\t\t  result_table.py   valid_table.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%cd /kaggle/working/demucs\n!pip install -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:25:43.149368Z","iopub.execute_input":"2025-04-14T01:25:43.149718Z","iopub.status.idle":"2025-04-14T01:25:50.902142Z","shell.execute_reply.started":"2025-04-14T01:25:43.149689Z","shell.execute_reply":"2025-04-14T01:25:50.900985Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/demucs\nObtaining file:///kaggle/working/demucs\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: diffq<0.2.0 in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (0.1.1)\nRequirement already satisfied: lameenc>=1.2 in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (1.8.1)\nRequirement already satisfied: julius>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (0.2.7)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (1.26.4)\nRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (2.5.1+cu124)\nRequirement already satisfied: torchaudio>=0.8 in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (2.5.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from demucs==2.0.3) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->demucs==2.0.3) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.1->demucs==2.0.3) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->demucs==2.0.3) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.1->demucs==2.0.3) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->demucs==2.0.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->demucs==2.0.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->demucs==2.0.3) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->demucs==2.0.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->demucs==2.0.3) (2024.2.0)\nInstalling collected packages: demucs\n  Running setup.py develop for demucs\nSuccessfully installed demucs-2.0.3\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!python3 -m demucs -h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T02:49:14.238521Z","iopub.execute_input":"2025-04-14T02:49:14.238864Z","iopub.status.idle":"2025-04-14T02:49:19.096648Z","shell.execute_reply.started":"2025-04-14T02:49:14.238835Z","shell.execute_reply":"2025-04-14T02:49:19.095273Z"}},"outputs":[{"name":"stdout","text":"usage: demucs [-h] [--raw RAW] [--no_raw] [-m MUSDB] [--is_wav] [--metadata METADATA] [--wav WAV]\n              [--concat] [--samplerate SAMPLERATE] [--audio_channels AUDIO_CHANNELS]\n              [--samples SAMPLES] [--data_stride DATA_STRIDE] [-w WORKERS]\n              [--eval_workers EVAL_WORKERS] [-d DEVICE] [--eval_cpu] [--dummy DUMMY] [--test TEST]\n              [--test_pretrained TEST_PRETRAINED] [--rank RANK] [--world_size WORLD_SIZE]\n              [--master MASTER] [--checkpoints CHECKPOINTS] [--evals EVALS] [--save] [--logs LOGS]\n              [--models MODELS] [-R] [--seed SEED] [-e EPOCHS] [-r REPEAT] [-b BATCH_SIZE]\n              [--lr LR] [--mse] [--init INIT] [--no_augment] [--repitch REPITCH]\n              [--max_tempo MAX_TEMPO] [--remix_group_size REMIX_GROUP_SIZE] [--shifts SHIFTS]\n              [--overlap OVERLAP] [--growth GROWTH] [--depth DEPTH] [--lstm_layers LSTM_LAYERS]\n              [--channels CHANNELS] [--kernel_size KERNEL_SIZE] [--conv_stride CONV_STRIDE]\n              [--context CONTEXT] [--rescale RESCALE] [--no_resample] [--no_glu] [--no_rewrite]\n              [--normalize] [--no_norm_wav] [--tasnet] [--split_valid] [--X X] [--show]\n              [--save_model] [--save_state SAVE_STATE] [--half] [--q-min-size Q_MIN_SIZE]\n              [--qat QAT] [--diffq DIFFQ] [--ms-target MS_TARGET]\n\nTrain and evaluate Demucs.\n\noptions:\n  -h, --help            show this help message and exit\n  --raw RAW             Path to raw audio, can be faster, see python3 -m demucs.raw to extract.\n  --no_raw\n  -m MUSDB, --musdb MUSDB\n                        Path to musdb root\n  --is_wav              Indicate that the MusDB dataset is in wav format (i.e. MusDB-HQ).\n  --metadata METADATA   Folder where metadata information is stored.\n  --wav WAV             Path to a wav dataset. This should contain a 'train' and a 'valid'\n                        subfolder.\n  --concat              Concat MusDB and wav dataset when provided.\n  --samplerate SAMPLERATE\n  --audio_channels AUDIO_CHANNELS\n  --samples SAMPLES     number of samples to feed in\n  --data_stride DATA_STRIDE\n                        Stride for chunks, shorter = longer epochs\n  -w WORKERS, --workers WORKERS\n                        Loader workers\n  --eval_workers EVAL_WORKERS\n                        Final evaluation workers\n  -d DEVICE, --device DEVICE\n                        Device to train on, default is cuda if available else cpu\n  --eval_cpu            Eval on test will be run on cpu.\n  --dummy DUMMY         Dummy parameter, useful to create a new checkpoint file\n  --test TEST           Just run the test pipeline + one validation. This should be a filename\n                        relative to the models/ folder.\n  --test_pretrained TEST_PRETRAINED\n                        Just run the test pipeline + one validation, on a pretrained model.\n  --rank RANK\n  --world_size WORLD_SIZE\n  --master MASTER\n  --checkpoints CHECKPOINTS\n                        Folder where to store checkpoints etc\n  --evals EVALS         Folder where to store evals and waveforms\n  --save                Save estimated for the test set waveforms\n  --logs LOGS           Folder where to store logs\n  --models MODELS       Folder where to store trained models\n  -R, --restart         Restart training, ignoring previous run\n  --seed SEED\n  -e EPOCHS, --epochs EPOCHS\n                        Number of epochs\n  -r REPEAT, --repeat REPEAT\n                        Repeat the train set, longer epochs\n  -b BATCH_SIZE, --batch_size BATCH_SIZE\n  --lr LR\n  --mse                 Use MSE instead of L1\n  --init INIT           Initialize from a pre-trained model.\n  --no_augment          No basic data augmentation.\n  --repitch REPITCH     Probability to do tempo/pitch change\n  --max_tempo MAX_TEMPO\n                        Maximum relative tempo change in % when using repitch.\n  --remix_group_size REMIX_GROUP_SIZE\n                        Shuffle sources using group of this size. Useful to somewhat replicate\n                        multi-gpu training on less GPUs.\n  --shifts SHIFTS       Number of random shifts used for the shift trick.\n  --overlap OVERLAP     Overlap when --split_valid is passed.\n  --growth GROWTH       Number of channels between two layers will increase by this factor\n  --depth DEPTH         Number of layers for the encoder and decoder\n  --lstm_layers LSTM_LAYERS\n                        Number of layers for the LSTM\n  --channels CHANNELS   Number of channels for the first encoder layer\n  --kernel_size KERNEL_SIZE\n                        Kernel size for the (transposed) convolutions\n  --conv_stride CONV_STRIDE\n                        Stride for the (transposed) convolutions\n  --context CONTEXT     Context size for the decoder convolutions before the transposed\n                        convolutions\n  --rescale RESCALE     Initial weight rescale reference\n  --no_resample         No Resampling of the input/output x2\n  --no_glu              Replace all GLUs by ReLUs\n  --no_rewrite          No 1x1 rewrite convolutions\n  --normalize\n  --no_norm_wav\n  --tasnet\n  --split_valid         Predict chunks by chunks for valid and test. Required for tasnet\n  --X X\n  --show                Show model architecture, size and exit\n  --save_model          Skip traning, just save final model for the current checkpoint value.\n  --save_state SAVE_STATE\n                        Skip training, just save state for the current checkpoint value. You\n                        should provide a model name as argument.\n  --half                When saving the model, uses half precision.\n  --q-min-size Q_MIN_SIZE\n                        Only quantize layers over this size (in MB)\n  --qat QAT             If provided, use QAT training with that many bits.\n  --diffq DIFFQ\n  --ms-target MS_TARGET\n                        Model size target in MB, when using DiffQ. Best model will be kept only if\n                        it is smaller than this target.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# !python3 -m demucs -b 1  --samplerate 8000 --split_valid --device \"cpu\" --samples 240000 -e 5 --musdb \"{db_path}\"\n!python3 -m demucs -b 1 --workers=1 --test_pretrained demucs_quantized --musdb \"{db_path}\" --samples=240000 --samplerate=8000 --device \"cpu\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:28:06.397711Z","iopub.execute_input":"2025-04-14T03:28:06.398061Z","iopub.status.idle":"2025-04-14T03:43:08.912281Z","shell.execute_reply.started":"2025-04-14T03:28:06.398036Z","shell.execute_reply":"2025-04-14T03:43:08.910699Z"}},"outputs":[{"name":"stdout","text":"Experiment musdb=musdb18-music-source-separation-dataset test_pretrained=demucs_quantized batch_size=1\nDownloading: \"https://dl.fbaipublicfiles.com/demucs/v3.0/demucs_quantized-07afea75.th\" to /root/.cache/torch/hub/checkpoints/demucs_quantized-07afea75.th\n100%|████████████████████████████████████████| 148M/148M [00:06<00:00, 23.4MB/s]\n/kaggle/working/demucs/demucs/utils.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = th.load(buf, \"cpu\")\n/kaggle/working/demucs/demucs/__main__.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  saved = th.load(checkpoint, map_location='cpu')\nAgumentation pipeline: Sequential(\n  (0): Shift()\n  (1): FlipSign()\n  (2): FlipChannels()\n  (3): Scale()\n  (4): Remix()\n)\nNumber of training samples adjusted to 447146\n^C\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/kaggle/working/demucs/demucs/__main__.py\", line 328, in <module>\n    main()\n  File \"/kaggle/working/demucs/demucs/__main__.py\", line 214, in main\n    train_set, valid_set = get_compressed_datasets(args, samples)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/demucs/demucs/compressed.py\", line 98, in get_compressed_datasets\n    _build_musdb_metadata(metadata_file, args.musdb, args.workers)\n  File \"/kaggle/working/demucs/demucs/compressed.py\", line 90, in _build_musdb_metadata\n    metadata = _build_metadata(tracks, workers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/demucs/demucs/compressed.py\", line 82, in _build_metadata\n    with futures.ProcessPoolExecutor(workers) as pool:\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 647, in __exit__\n    self.shutdown(wait=True)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 851, in shutdown\n    self._executor_manager_thread.join()\n  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n    self._wait_for_tstate_lock()\n  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n    if lock.acquire(block, timeout):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"!ls metadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:43.810956Z","iopub.execute_input":"2025-04-14T03:14:43.811393Z","iopub.status.idle":"2025-04-14T03:14:43.933941Z","shell.execute_reply.started":"2025-04-14T03:14:43.811361Z","shell.execute_reply":"2025-04-14T03:14:43.932502Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# !python3 -m demucs -b 1  --samplerate 8000 --split_valid --device \"cpu\" --samples 240000 -e 5 --musdb \"{db_path}\"\n!python3 -m demucs --workers=1 -b 1 --test_pretrained tasnet --samples=240000 --samplerate=8000 --device \"cpu\" --musdb \"{db_path}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:03:14.002767Z","iopub.execute_input":"2025-04-14T03:03:14.003260Z","iopub.status.idle":"2025-04-14T03:04:08.755285Z","shell.execute_reply.started":"2025-04-14T03:03:14.003209Z","shell.execute_reply":"2025-04-14T03:04:08.754038Z"}},"outputs":[{"name":"stdout","text":"Experiment musdb=musdb18-music-source-separation-dataset samplerate=8000 samples=240000 device=cpu test_pretrained=tasnet batch_size=1\n/kaggle/working/demucs/demucs/__main__.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  saved = th.load(checkpoint, map_location='cpu')\nAgumentation pipeline: Sequential(\n  (0): Shift()\n  (1): FlipSign()\n  (2): FlipChannels()\n  (3): Scale()\n  (4): Remix()\n)\nNumber of training samples adjusted to 240000\n^C\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/kaggle/working/demucs/demucs/__main__.py\", line 328, in <module>\n    main()\n  File \"/kaggle/working/demucs/demucs/__main__.py\", line 214, in main\n    train_set, valid_set = get_compressed_datasets(args, samples)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/demucs/demucs/compressed.py\", line 98, in get_compressed_datasets\n    _build_musdb_metadata(metadata_file, args.musdb, args.workers)\n  File \"/kaggle/working/demucs/demucs/compressed.py\", line 90, in _build_musdb_metadata\n    metadata = _build_metadata(tracks, workers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/demucs/demucs/compressed.py\", line 82, in _build_metadata\n    with futures.ProcessPoolExecutor(workers) as pool:\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 647, in __exit__\n    self.shutdown(wait=True)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 851, in shutdown\n    self._executor_manager_thread.join()\n  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n    self._wait_for_tstate_lock()\n  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n    if lock.acquire(block, timeout):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"python3 -m demucs -b 1  --musdb MUSDB_PATH --tasnet --samplerate=8000 --samples=240000 --split_valid --init tasnet # Conv-Tasnet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}
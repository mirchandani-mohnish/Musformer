{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10005918,"sourceType":"datasetVersion","datasetId":6159348}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:29:22.629457Z","iopub.execute_input":"2025-04-18T15:29:22.629727Z","iopub.status.idle":"2025-04-18T15:29:22.690870Z","shell.execute_reply.started":"2025-04-18T15:29:22.629709Z","shell.execute_reply":"2025-04-18T15:29:22.690238Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/musdb18-music-source-separation-dataset/The Long Wait - Dark Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Raft Monk - Tiring.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Too Much.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Georgia Wonder - Siren.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Sunshine Garcia Band - For I Am The Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Enda Reilly - Cur An Long Ag Seol.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Buitraker - Revo X.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/We Fell From The Sky - Not You.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Mountaineering Club - Mallory.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Skelpolu - Resurrection.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Over The Top.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Bobby Nobody - Stitch Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Arise - Run Run Run.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Carlos Gonzalez - A Place For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Forkupines - Semantics.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises - Falcon 69.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Lyndsey Ollard - Catching Up.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Triviul feat. The Fiend - Widow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Louis Cressy Band - Good Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Motor Tapes - Shore.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/AM Contra - Heart Peripheral.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Signe Jakobsen - What Have You Done To Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Moosmusic - Big Dummy Shake.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/M.E.R.C. Music - Knockout.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Doppler Shift - Atrophy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Detsky Sad - Walkie Talkie.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Happy Daze.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Timboz - Pony.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/PR - Oh No.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Mu - Too Bright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Hollow Ground - Ill Fate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/The Easton Ellises (Baumi) - SDRNR.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Like Horses.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Sambasevam Shanmugam - Kaathaadi.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Tom McKenzie - Directions.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Secretariat - Borderline.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Side Effects Project - Sing With Me.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Nerve 9 - Pray For The Rain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Zeno - Signs.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Girls Under Glass - We Feel Alright.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Cristina Vane - So Easy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Speak Softly - Broken Man.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/BKS - Bulldozer.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Punkdisco - Oral Hygiene.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/test/Al James - Schoolboy Facination.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dark Ride - Burning Bridges.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Drumtracks - Ghost Bitch.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Aimee Norwich - Child.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - If You Say.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rockabilly.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Steven Clark - Bounty.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Giselle - Moss.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Strand Of Oaks - Spacestation.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - Set Me Free.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Bill Chudziak - Children Of No-one.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Angela Thomas Wade - Milk Cow Blues.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Grants - PunchDrunk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Grunge.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Once More (With Feeling).stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Beatles.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Auctioneer - Our Future Faces.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Air Traffic.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Patrick Talbot - A Reason To Leave.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Districts - Vermont.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Come Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/North To Alaska - All The Same.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Human Mistakes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Dreamers Of The Ghetto - Heavy Love.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Rockshow.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Faces On Film - Waiting For Ga.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Snowmine - Curfews.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Swinging Steaks - Lost My Way.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Dorothy.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Gospel.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Stella.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Disco.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Reggae.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The So So Glos - Emergency.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Wicked.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/St Vitus - Word Gets Around.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Celestial Shore - Die For Us.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Facade.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/AvaLuna - Waterduct.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Punk.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - One Minute Smile.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Blood To Bone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Tim Taler - Stalker.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Hendrix.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Leaf - Summerghost.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hop Along - Sister Cities.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - All Souls Moon.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - You Listen.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country2.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Clinic A.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Traffic Experiment - Sirens.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Britpop.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Chris Durban - Celebrate.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Triviul - Angelsaint.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - On The Line.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/A Classic Education - NightOwl.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Skelpolu - Together Alone.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Titanium - Haunted Age.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Goodbye Bolero.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Secret Mountains - High Horse.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Wall Of Death - Femme.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - The Wind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Alexander Ross - Velvet Curtain.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Johnny Lokke - Whisper To A Scream.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Meaxic - Take A Step.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Jay Menon - Through My Eyes.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Flags - 54.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Clara Berry And Wooldog - Waltz For My Victims.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/ANiMAL - Easy Tiger.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Back From The Start.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hollow Ground - Left Blind.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Sweet Lights - You Let Me Down.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Port St Willow - Stay Even.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Helado Negro - Mitad Del Mundo.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Black Bloc - If You Want Success.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Young Griffo - Pennies.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Voelund - Comfort Lives In Belief.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Fergessen - Nos Palpitants.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Creepoid - OldTree.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Actions - South Of The Water.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Lushlife - Toynbee Suite.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Matthew Entwistle - Dont You Ever.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Scarlet Brand - Les Fleurs Du Mal.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - Country1.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/James May - Dont Let Go.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Music Delta - 80s Rock.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Atlantis Bound - It Was My Fault For Waiting.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Invisible Familiars - Disturbing Wildlife.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Cnoc An Tursa - Bannockburn.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Hezekiah Jones - Borrowed Heart.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/BigTroubles - Phantom.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Remember December - C U Next Time.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/Night Panther - Fire.stem.mp4\n/kaggle/input/musdb18-music-source-separation-dataset/train/The Long Wait - Back Home To Blue.stem.mp4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Musformer\nAlthough misleading, but the name seemed nice. The goal of this project is to try out multiple source separation models in speechbrain. The idea is to replicate building a model in the waveform domain directly. We see that multiple models try to solve the music sourse separation as a problem and most state-of-the-art models reach an SiSDR ratio of 9 to tackle the same. \n\nMost models working on this problem came into existence as a solution to the SDX (Sound Demixing) challenge. A few of the existing solutions are as follows: \n- **MMDenseLSTM**: Combines dense blocks with LSTMs for lightweight waveform separation.\n- **Demucs (v1/v2)**: U-Net with bidirectional LSTMs; later versions add transformers.\n- **ConvTasNet**: Efficient temporal convolutional network with learned encoder/decoder.\n- **DPRNNet**: Dual-path RNN with attention, offering SOTA results at higher compute costs.\n- **BandSplitRNN**: Separates audio into frequency bands processed by independent RNNs before recombination.\n- **Wave-U-Net**: Adapts medical imaging's U-Net architecture for waveform-based source separation with learned down/upsampling.\n- **Open-Unmix**: Spectrogram-based separation model using three-layer BiLSTMs with industry-standard implementation.\n- **ResUNetDecouple**: U-Net variant with residual connections that decouples magnitude and phase processing.\n- **TDCN++**: Improved temporal convolutional network with global skip connections and stacked dilation patterns.\n- **Spleeter**: Facebook's lightweight CNN-based separator using spectrogram masking with pretrained models.\n\nOf these, two models which catch one's eye are convtasnet, which seemed to provide good results with the most optimally efficient architecture and demucs, which uses an advanced convolutional architecure built on top of Wave-U-Net and stretches its performance to state-of-the-art levels.","metadata":{}},{"cell_type":"markdown","source":"## Disclaimers\nThis notebook refers to the tip of the iceberg of multiple approaches tried along the way in order to reach here. It provides for a summary of the underlying work done around trials and errors in speech separation. More about the same is appended in the Archives section at the end of this report notebook. Additionally, the informal nature of the text in this report is to keep the content somewhat light-hearted in order to ensure engagement with the reader...\n\n\nThis report is intended to be self explanatory and so all the data you need to learn about these models should be referenced in the report itself. Additionally, all the models presented here have their intended recipes which you can use to cook your own models... \n\nBonne Apetit!","metadata":{}},{"cell_type":"markdown","source":"## Step1 : Gaining Context (Literature Review)\n\n### Gimme the Data!!\nFirst things first, lets talk about the dataset and the models a bit. MUSDB18 is the benchmark dataset for music source separation, containing 150 full-track recordings (100 for training, 50 for test) with isolated stems for vocals, drums, bass, and other instruments. It provides professionally mixed 44.1kHz stereo audio, enabling evaluation of waveform-domain separation models. The dataset covers diverse genres and production styles, making it ideal for testing real-world generalization. It has become the standard benchmark for models like Open-Unmix, Demucs, and D3Net, with SI-SDR and SDR as primary metrics. The included Python toolbox (musdb) provides data loading, evaluation, and stem mixing utilities. Musdb is however dependent on something known as STEM files i.e. music tensor files combined and compressed into one. We use the kaggle uploaded version of the dataset which saves us the trouble of downloading, unzipping and uploading the same. You can find the dataset here: https://www.kaggle.com/datasets/jakerr5280/musdb18-music-source-separation-dataset\n\n\n### The MusDB Package\nOne of the most helpful things available in the community to enhance the friendliness of this challenge is the [package provided by sigsep](https://github.com/sigsep/sigsep-mus-db/tree/master/musdb). Given that all the files are STEM format files i.e. a format extension which clubs multiple vectors into a cohesive mp4 format, the musdb package helps us read those using [FFMPEG](https://ffmpeg.org/). This allows us to load the vectors directly into the file without the need of creating a manifest file to read the particular values.\n\n","metadata":{}},{"cell_type":"markdown","source":"## How You Doin...? (Methodology)\n\nIn order to learn and explore the different model approaches, we draw up each of the available models in speechbrain and try to retrofit them to a simple vocal separation task. The expectation being that the data more often than not, the model is able to reach state-of-the-art...ish levels. The steps we follow are as below:\n1. Learn and draw up the model architecture using speechbrain modules wherever possible\n2. Run them on a particular set of data values with particular hyperparameters\n3. Compare the results\n","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Installing Stuff and Basic Setup\n\nUgghh... this is the boring part, but thats what makes the fun part funn! So we start by installing the necessary requirements for the entire project, as well as some global variables\n\nThe following are the use cases\n- Musdb: Our python package wrapper which helps us iterate through the dataset\n- mir_eval and museval: progressively used to check the sdri scores across models. These are the standard packages used in the MDX challenge as well.\n\nAdditionally, we set certain variables namely:\n- db_path: The path to the dataset\n- output_path: The working directory\n- result_file_path: The file path for the metrics to be provided (Not really needed but thats where you see the magic)\n\nWe then create the result_file if needed and additionally setup the musdb object for global requirement","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install musdb\n!pip install mir_eval\n!pip install museval\n\n# Installing SpeechBrain via pip\nBRANCH = 'develop'\n!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n\n# Clone SpeechBrain repository\n!git clone https://github.com/speechbrain/speechbrain/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:29:32.791235Z","iopub.execute_input":"2025-04-18T15:29:32.791839Z","iopub.status.idle":"2025-04-18T15:31:26.806389Z","shell.execute_reply.started":"2025-04-18T15:29:32.791814Z","shell.execute_reply":"2025-04-18T15:31:26.805456Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"db_path = '/kaggle/input/musdb18-music-source-separation-dataset'\noutput_path = '/kaggle/working'\nresult_file_path = '/kaggle/working/metrics.csv'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:31:26.807957Z","iopub.execute_input":"2025-04-18T15:31:26.808283Z","iopub.status.idle":"2025-04-18T15:31:26.812737Z","shell.execute_reply.started":"2025-04-18T15:31:26.808232Z","shell.execute_reply":"2025-04-18T15:31:26.811964Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import csv\nfrom pathlib import Path\n\ndef create_metrics_csv(file_path):\n    headers = [\n        \"model_name\", \"n_epochs\", \"learning_rate\", \"chunk_size\", \"sample_rate\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"\n    ]\n    \n    Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(headers)\n    \n    print(f\"CSV file created successfully at: {file_path}\")\n\nif result_file_path is not None:\n    create_metrics_csv(result_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:31:26.813515Z","iopub.execute_input":"2025-04-18T15:31:26.813688Z","iopub.status.idle":"2025-04-18T15:31:26.832254Z","shell.execute_reply.started":"2025-04-18T15:31:26.813673Z","shell.execute_reply":"2025-04-18T15:31:26.831732Z"}},"outputs":[{"name":"stdout","text":"CSV file created successfully at: /kaggle/working/metrics.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nnp.float_ = np.float64\nimport musdb\n\nMUS_DB_PATH = db_path\n\nmus = musdb.DB(root=MUS_DB_PATH)\nmus_train = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"train\")\nmus_valid = musdb.DB(root=MUS_DB_PATH,subsets=\"train\", split=\"valid\")\nmus_test = musdb.DB(root=MUS_DB_PATH,subsets=\"test\")\nprint(mus_train[0])\nprint(mus_test[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:39:19.896356Z","iopub.execute_input":"2025-04-18T02:39:19.896525Z","iopub.status.idle":"2025-04-18T02:39:56.862428Z","shell.execute_reply.started":"2025-04-18T02:39:19.896512Z","shell.execute_reply":"2025-04-18T02:39:56.861839Z"}},"outputs":[{"name":"stdout","text":"A Classic Education - NightOwl\nAM Contra - Heart Peripheral\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 3: Make the Dataset Class\n\nSince music tensors are difficult to load, we use a self made dataset class for our custom dataloader instead of using the DynamicDatasetItems provided by speechbrain. This allows us more flexibility with regard to selecting the tensors at runtime. Due to our prowess with STEM files, we can happily pick up stem chunks rather than taking the entire vocal array to pick up a chunk. \n\nThis is what we do in the `load_stem_chunk` function. We also pick up some data about the music file in order to use later during the project. (We need a taste of what our recipe makes ;)","metadata":{}},{"cell_type":"code","source":"%%file dataset.py\n\n\nimport csv\nimport os\nimport sys\n\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport musdb\nimport numpy as np\nimport speechbrain as sb\nimport psutil\n\n\nclass MusDBDataset(Dataset):\n    def __init__(self, root, subset=\"train\", split=None, target_sr=8000, chunk_size=15):\n        \"\"\"\n        True lazy-loading for MUSDB\n        :param chunk_size: in seconds\n        \"\"\"\n        if split is not None:\n            self.db = musdb.DB(root=root, subsets=subset, split=split, is_wav=False)\n        else:\n            self.db = musdb.DB(root=root, subsets=subset, is_wav=False)\n        self.target_sr = target_sr\n        self.chunk_size = chunk_size\n        self.tracks = [{\n            \"path\": track.path,\n            \"duration\": track.duration,\n            \"rate\": track.rate,\n            \"stem_id\": track.stem_id  # Needed for STEM access\n        } for track in self.db.tracks]\n\n    def __len__(self):\n        return len(self.tracks)\n\n    def __getitem__(self, idx):\n        track_info = self.tracks[idx]\n        \n        # Load chunk directly from disk without full track loading\n        def load_stem_chunk(stem_name, random_chunk_val=0):\n            # MUSDB's internal lazy loading\n            track = self.db.tracks[idx]\n            if stem_name == \"mix\":\n                source = track\n            else:\n                source = track.targets[stem_name]\n            \n            # Calculate chunk bounds\n            start = random_chunk_val\n            stop = start + self.chunk_size * track_info[\"rate\"]\n            \n            # Load only the needed segment\n            audio = source.audio[start:stop]\n            \n            # Convert and resample\n            audio_tensor = torch.from_numpy(audio).float().permute(1, 0)\n            return torchaudio.functional.resample(\n                audio_tensor,\n                orig_freq=track_info[\"rate\"],\n                new_freq=self.target_sr\n            ).mean(dim=0, keepdim=False)\n            \n        \n            # chunk_size = orig_sr * chunk_size_seconds  # Convert chunk size to samples\n        \n            # resampled_chunks = []\n        \n            # for i in range(0, audio_tensor.shape[1], chunk_size):\n            #     chunk = audio_tensor[:, i:i + chunk_size]  # Extract chunk\n            #     resampled_chunk = torchaudio.functional.resample(chunk, orig_freq=orig_sr, new_freq=target_sr)\n            #     resampled_chunks.append(resampled_chunk)\n        \n            # # Concatenate back the processed chunks\n            # # print(\"PROCESSING CHUNKS\")\n            # resampled_audio = torch.cat(resampled_chunks, dim=1)\n            # # print(resampled_audio.shape)\n            # resampled_audio = resampled_audio.mean(dim=0, keepdim=False)\n            # # print(resampled_audio.shape)\n            # # print(resampled_audio.shape)\n            # return resampled_audio\n        # if random_chunk_start_val:\n        \n        random_chunk_start_val = 0\n        \n        return {\n            \"mix_sig\": load_stem_chunk(\"mix\", random_chunk_start_val),\n            \"voc_sig\": load_stem_chunk(\"vocals\",random_chunk_start_val),\n            \"inst_sig\": load_stem_chunk(\"accompaniment\",random_chunk_start_val),\n            \"track_id\": track_info[\"stem_id\"]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:37:19.456307Z","iopub.execute_input":"2025-04-18T03:37:19.456806Z","iopub.status.idle":"2025-04-18T03:37:19.463655Z","shell.execute_reply.started":"2025-04-18T03:37:19.456764Z","shell.execute_reply":"2025-04-18T03:37:19.462863Z"}},"outputs":[{"name":"stdout","text":"Overwriting dataset.py\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## Step 3: Building Models\n\n### ConvTasNet\n- Key Idea: A fully convolutional, end-to-end waveform model that avoids spectrograms entirely.\n- Encoder-Decoder: Uses 1D convolutions to learn a latent representation of the waveform.\n- Temporal Convolutional Network (TCN): Processes the latent features with stacked dilated convolutions for long-range dependencies.\n- Mask Estimation: Applies a soft mask in the latent space to separate sources.\n\nAdvantages:\n- Lightweight and parallelizable (no RNNs).\n- Strong performance on speech and music separation.\n\nLimitations:\n- May struggle with very long-term dependencies due to fixed receptive fields.\n\nPersonal Interests:\nConvtasnet proves to be one of the smallest models which generally speaking outperforms most other state-of-the-art models (in the waveform domain). This is intruiging because it uses a simple encoder decoder architecture with a masknet type bottleneck. The version we use in this has 6.6 Million parameters and is successfully able to detach vocals from its accompaniements. \n\nReference and Sources:\n","metadata":{}},{"cell_type":"code","source":"%%file convtasnet-hparams.yaml\n# ################################\n# Model: ConvTasNet for Music Vocal Separation\n# https://arxiv.org/abs/2010.13154\n# Dataset : Musdb\n# ################################\n# Basic parameters\n# Seed needs to be set at top of yaml, before objects with parameters are made\n\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params\ndata_folder: !PLACEHOLDER\n\n\nexperiment_name: convtasnet\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\nresult_file_path: !PLACEHOLDER\n\n\n# Experiment params\nprecision: fp16 # bf16, fp16 or fp32\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True # Save estimated sources on disk\nsample_rate: 8000\nn_audio_to_save: 10\nchunk_size: 20\n\n\n####################### WaveDrop Params ########################################\n\nuse_wavedrop: True\n# Frequency drop: randomly drops a number of frequency bands to zero.\ndrop_freq_low: 0  # Min frequency band dropout probability\ndrop_freq_high: 1  # Max frequency band dropout probability\ndrop_freq_count_low: 1  # Min number of frequency bands to drop\ndrop_freq_count_high: 3  # Max number of frequency bands to drop\ndrop_freq_width: 0.05  # Width of frequency bands to drop\n\ndrop_freq: !new:speechbrain.augment.time_domain.DropFreq\n    drop_freq_low: !ref <drop_freq_low>\n    drop_freq_high: !ref <drop_freq_high>\n    drop_freq_count_low: !ref <drop_freq_count_low>\n    drop_freq_count_high: !ref <drop_freq_count_high>\n    drop_freq_width: !ref <drop_freq_width>\n\n# Time drop: randomly drops a number of temporal chunks.\ndrop_chunk_count_low: 1  # Min number of audio chunks to drop\ndrop_chunk_count_high: 5  # Max number of audio chunks to drop\ndrop_chunk_length_low: 1000  # Min length of audio chunks to drop\ndrop_chunk_length_high: 2000  # Max length of audio chunks to drop\n\ndrop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n    drop_length_low: !ref <drop_chunk_length_low>\n    drop_length_high: !ref <drop_chunk_length_high>\n    drop_count_low: !ref <drop_chunk_count_low>\n    drop_count_high: !ref <drop_chunk_count_high>\n\n####################### Training Parameters ####################################\nN_epochs: 15\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999  # this is the upper limit for an acceptable loss\nnum_sources: 2\n\n\n# loss thresholding -- this thresholds the training loss\nthreshold_byloss: True\nthreshold: -30\n\n# Encoder parameters\nN_encoder_out: 256\n# out_channels: 256\nkernel_size: 16\nkernel_stride: 8\n\n# Dataloader options\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 1\n\n\n# Specifying the network\nEncoder: !new:speechbrain.lobes.models.dual_path.Encoder\n    kernel_size: !ref <kernel_size>\n    out_channels: !ref <N_encoder_out>\n\n\nMaskNet: !new:speechbrain.lobes.models.conv_tasnet.MaskNet\n    N: 256\n    B: 256\n    H: 512\n    P: 3\n    X: 6\n    R: 4\n    C: !ref <num_sources>\n    norm_type: 'gLN'\n    causal: True\n    mask_nonlinear: 'relu'\n\n\nDecoder: !new:speechbrain.lobes.models.dual_path.Decoder\n    in_channels: !ref <N_encoder_out>\n    out_channels: 1\n    kernel_size: !ref <kernel_size>\n    stride: !ref <kernel_stride>\n    bias: False\n\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder: !ref <Encoder>\n    decoder: !ref <Decoder>\n    masknet: !ref <MaskNet>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder: !ref <Encoder>\n        decoder: !ref <Decoder>\n        masknet: !ref <MaskNet>\n        counter: !ref <epoch_counter>\n        lr_scheduler: !ref <lr_scheduler>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:30:34.557515Z","iopub.execute_input":"2025-04-18T14:30:34.557782Z","iopub.status.idle":"2025-04-18T14:30:34.582278Z","shell.execute_reply.started":"2025-04-18T14:30:34.557759Z","shell.execute_reply":"2025-04-18T14:30:34.581608Z"}},"outputs":[{"name":"stdout","text":"Overwriting convtasnet-hparams.yaml\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"%%file convtasnet-train.py\n#!/usr/bin/env/python3\n\"\"\"\nRecipe for vocal separation using convtasnet\n\"\"\"\n\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nimport pdb\n\nimport musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\nfrom dataset import MusDBDataset\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.core import AMPConfig\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nimport time\nfrom torch.utils.data import DataLoader\n\nimport musdb\n\n\n# Define training procedure\nclass Separation(sb.Brain):\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix       \n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)      \n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_sources)],\n            dim=-1,\n        ).to(self.device)\n\n        if stage == sb.Stage.TRAIN:\n            with torch.no_grad():\n    \n                if self.hparams.use_wavedrop:\n                    mix = self.hparams.drop_chunk(mix, mix_lens)\n                    mix = self.hparams.drop_freq(mix)\n\n\n        \n        # Separation\n        mix_w = self.hparams.Encoder(mix)\n        est_mask = self.hparams.MaskNet(mix_w)\n        mix_w = torch.stack([mix_w] * self.hparams.num_sources)\n        sep_h = mix_w * est_mask\n        \n        # Decoding\n        est_source = torch.cat(\n            [\n                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n                for i in range(self.hparams.num_sources)\n            ],\n            dim=-1,\n        )\n\n        # pad estimates as per requirement \n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n        # print(\"INSIDE FIT BATCH\")\n        \n        amp = AMPConfig.from_name(self.precision)\n        should_step = (self.step % self.grad_accumulation_factor) == 0\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig] #mix_sig, voc_sig, inst_sig\n       \n        \n        predictions, targets = self.compute_forward(\n            mixture, targets, sb.Stage.TRAIN\n        )\n        loss = self.compute_objectives(predictions, targets)\n\n        if self.hparams.threshold_byloss:\n            th = self.hparams.threshold\n            loss = loss[loss > th]\n            if loss.nelement() > 0:\n                loss = loss.mean()\n        else:\n            loss = loss.mean()\n\n        if (\n            loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        ):  # the fix for computational problems\n            loss.backward()\n            if self.hparams.clip_grad_norm >= 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.optimizer.step()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_loader):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n\n        def is_silent(source, threshold=1e-6):\n            return np.max(np.abs(source[0])) < threshold or np.max(np.abs(source[1])) < threshold\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n            skip_cnt = 0\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n                   \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_sources, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n                    \n     \n                    if not is_silent(targets[0].t().cpu().numpy()) and not is_silent(predictions[0].t().detach().cpu().numpy()) and not is_silent(mixture_signal[0].t().detach().cpu().numpy()):\n                        \n                    \n                        sdr, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            predictions[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_baseline, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            mixture_signal[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_i = sdr.mean() - sdr_baseline.mean()\n    \n                        # Saving on a csv file\n                        row = {\n                            \"snt_id\": snt_id[0],\n                            \"sdr\": sdr.mean(),\n                            \"sdr_i\": sdr_i,\n                            \"si-snr\": -sisnr.item(),\n                            \"si-snr_i\": -sisnr_i.item(),\n                        }\n                        writer.writerow(row)\n    \n                        # Metric Accumulation\n                        all_sdrs.append(sdr.mean())\n                        all_sdrs_i.append(sdr_i.mean())\n                        all_sisnrs.append(-sisnr.item())\n                        all_sisnrs_i.append(-sisnr_i.item())\n    \n                else:\n                    skip_cnt += 1\n                    print(f\"Warning: skipping silent target, this has happened {skip_cnt} times\")\n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n        \n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n\n        if(self.hparams[\"result_file_path\"] is not None and self.hparams[\"result_file_path\"] != \"\"):\n            with open(self.hparams.result_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as metrics_csv:\n                writer = csv.DictWriter(metrics_csv, fieldnames=[\"model_name\", \"n_epochs\", \"learning_rate\", \"chunk_size\", \"sample_rate\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"])\n                row = {\n                        \"model_name\": self.hparams[\"experiment_name\"],\n                        \"learning_rate\": self.hparams[\"lr\"],\n                        \"n_epochs\": self.hparams[\"N_epochs\"],\n                        \"chunk_size\":self.hparams[\"chunk_size\"],\n                        \"sample_rate\":self.hparams[\"sample_rate\"],\n                        \"sdr\": np.array(all_sdrs).mean(),\n                        \"sdr_i\": np.array(all_sdrs_i).mean(),\n                        \"si-snr\": np.array(all_sisnrs).mean(),\n                        \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                    }\n                writer.writerow(row)\n            \n        \n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_sources):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n   \n\n    # Brain class initialization\n    separator = Separation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n \n    # Training\n    # Usage with SpeechBrain\n    train_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    valid_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    test_data = MusDBDataset(hparams[\"db_path\"], subset=\"test\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    \n    print(hparams[\"result_file_path\"])\n    \n    # Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    test_loader = sb.dataio.dataloader.make_dataloader(\n        test_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n    # print(\"STARTING FIT\")\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # # Eval\n    separator.evaluate(test_loader, min_key=\"si-snr\")\n    separator.save_results(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:33:32.071219Z","iopub.execute_input":"2025-04-18T14:33:32.071861Z","iopub.status.idle":"2025-04-18T14:33:32.083060Z","shell.execute_reply.started":"2025-04-18T14:33:32.071838Z","shell.execute_reply":"2025-04-18T14:33:32.082149Z"}},"outputs":[{"name":"stdout","text":"Overwriting convtasnet-train.py\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# !python convtasnet-train.py convtasnet-hparams.yaml --data_folder=db_path --device \"cpu\"\n!torchrun --standalone --nproc_per_node=2 convtasnet-train.py convtasnet-hparams.yaml --data_folder={db_path} --result_file_path={result_file_path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:33:33.351802Z","iopub.execute_input":"2025-04-18T14:33:33.352510Z","execution_failed":"2025-04-18T14:37:22.208Z"}},"outputs":[{"name":"stdout","text":"W0418 14:33:35.119000 360483 torch/distributed/run.py:793] \nW0418 14:33:35.119000 360483 torch/distributed/run.py:793] *****************************************\nW0418 14:33:35.119000 360483 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0418 14:33:35.119000 360483 torch/distributed/run.py:793] *****************************************\nspeechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/convtasnet/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `True`\nspeechbrain.core - Using training precision: `--precision=fp16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 6.6M\n* Total Number of Parameters: 6.6M\n* Trainable Parameters represent 100.0000% of the total size.\nresult_file_path\nresult_file_path\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-18+05-48-06+00\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n/usr/local/lib/python3.11/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/convtasnet/1234/save/CKPT+2025-04-18+05-48-06+00\n 75%|          | 33/44 [03:16<01:07,  6.18s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## DPRNN (Dual-Path Recurrent Neural Network)\nKey Idea:\nDPRNN is designed to model extremely long sequences efficiently by splitting input signals into smaller chunks and processing them through intra-chunk (local) and inter-chunk (global) RNNs. This dual-path approach addresses the limitations of traditional RNNs in handling long-term dependencies while maintaining low computational complexity.\n\nArchitecture\n- Encoder: Converts input waveforms (or time-frequency representations) into latent features using 1D convolutions (time-domain) or STFT (TF-domain).\n- Decoder: Reconstructs separated signals using transposed convolutions or overlap-add operations.\n\nDual-Path Processing:\n- Intra-chunk RNN: Models local patterns within short segments (e.g., 5.75-ms windows) 11.\n- Inter-chunk RNN: Captures global dependencies across segments, enabling utterance-level modeling 612.\n- Mask Estimation: Learns soft masks in the latent space to separate sources, similar to ConvTasNet but with RNN-based temporal modeling.\n\nAdvantages\n- Long-Sequence Modeling: Superior to 1D CNNs and vanilla RNNs for sequences with >100k time steps.\n- Versatility: Adaptable to time-domain (e.g., TasNet) and TF-domain tasks (e.g., speech enhancement).\n\nLimitations\n- Computational Overhead: RNNs may introduce higher latency compared to fully convolutional models like ConvTasNet.\n- Fixed Chunking: Performance depends on optimal chunk size selection.\n\nPersonal Interests\nDPRNNs innovative dual-path mechanism achieves state-of-the-art separation quality (e.g., WSJ0-2mix dataset) with minimal parameters, making it ideal for real-time applications like hearing aids and telecommunication. Its extension to variable speaker counts (e.g., Multi-Decoder DPRNN) further showcases its flexibility.","metadata":{}},{"cell_type":"code","source":"%%file dprnn-hparams.yaml\n# ################################\n# Model: DPRNN for Music vocal separation\n# https://arxiv.org/abs/2010.13154\n# Dataset : MusDB\n# ################################\n# Basic parameters\n# Seed needs to be set at top of yaml, before objects with parameters are made\n\n\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params\n\ndata_folder: !PLACEHOLDER\n\n\nexperiment_name: dprnn\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\nresult_file_path: !PLACEHOLDER\n\n# Experiment params\nprecision: fp16 # bf16, fp16 or fp32\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True # Save estimated sources on disk\nsample_rate: 8000\nn_audio_to_save: 10\nchunk_size: 20\n\n####################### Training Parameters ####################################\nN_epochs: 15\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999  # this is the upper limit for an acceptable loss\nnum_sources: 2\n\n\n\n# loss thresholding -- this thresholds the training loss\nthreshold_byloss: True\nthreshold: -30\n\n# Encoder parameters\nN_encoder_out: 256\nout_channels: 256\nkernel_size: 16\nkernel_stride: 8\n\n# Dataloader options\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 3\n\n\n# Specifying the network\nEncoder: !new:speechbrain.lobes.models.dual_path.Encoder\n    kernel_size: !ref <kernel_size>\n    out_channels: !ref <N_encoder_out>\n\nintra: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n    num_layers: 1\n    input_size: !ref <out_channels>\n    hidden_channels: !ref <out_channels>\n    dropout: 0\n    bidirectional: True\n\ninter: !new:speechbrain.lobes.models.dual_path.SBRNNBlock\n    num_layers: 1\n    input_size: !ref <out_channels>\n    hidden_channels: !ref <out_channels>\n    dropout: 0\n    bidirectional: True\n\nMaskNet: !new:speechbrain.lobes.models.dual_path.Dual_Path_Model\n    num_spks: !ref <num_sources>\n    in_channels: !ref <N_encoder_out>\n    out_channels: !ref <out_channels>\n    num_layers: 6\n    K: 250\n    intra_model: !ref <intra>\n    inter_model: !ref <inter>\n    norm: ln\n    linear_layer_after_inter_intra: True\n    skip_around_intra: True\n\nDecoder: !new:speechbrain.lobes.models.dual_path.Decoder\n    in_channels: !ref <N_encoder_out>\n    out_channels: 1\n    kernel_size: !ref <kernel_size>\n    stride: !ref <kernel_stride>\n    bias: False\n\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder: !ref <Encoder>\n    decoder: !ref <Decoder>\n    masknet: !ref <MaskNet>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder: !ref <Encoder>\n        decoder: !ref <Decoder>\n        masknet: !ref <MaskNet>\n        counter: !ref <epoch_counter>\n        lr_scheduler: !ref <lr_scheduler>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:59:28.183308Z","iopub.execute_input":"2025-04-18T05:59:28.183662Z","iopub.status.idle":"2025-04-18T05:59:28.190743Z","shell.execute_reply.started":"2025-04-18T05:59:28.183629Z","shell.execute_reply":"2025-04-18T05:59:28.190165Z"}},"outputs":[{"name":"stdout","text":"Overwriting dprnn-hparams.yaml\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"%%file dprnn-train.py\n#!/usr/bin/env/python3\n\"\"\"\nRecipe for vocal separation using convtasnet\n\"\"\"\n\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nimport pdb\n\nimport musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\nfrom dataset import MusDBDataset\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.core import AMPConfig\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nimport time\nfrom torch.utils.data import DataLoader\n\nimport musdb\n\n\n# Define training procedure\nclass Separation(sb.Brain):\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix       \n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)      \n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_sources)],\n            dim=-1,\n        ).to(self.device)\n        \n        # Separation\n        mix_w = self.hparams.Encoder(mix)\n        est_mask = self.hparams.MaskNet(mix_w)\n        mix_w = torch.stack([mix_w] * self.hparams.num_sources)\n        sep_h = mix_w * est_mask\n        \n        # Decoding\n        est_source = torch.cat(\n            [\n                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n                for i in range(self.hparams.num_sources)\n            ],\n            dim=-1,\n        )\n\n        # pad estimates as per requirement \n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n        # print(\"INSIDE FIT BATCH\")\n        \n        amp = AMPConfig.from_name(self.precision)\n        should_step = (self.step % self.grad_accumulation_factor) == 0\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig] #mix_sig, voc_sig, inst_sig\n       \n        \n        predictions, targets = self.compute_forward(\n            mixture, targets, sb.Stage.TRAIN\n        )\n        loss = self.compute_objectives(predictions, targets)\n\n        if self.hparams.threshold_byloss:\n            th = self.hparams.threshold\n            loss = loss[loss > th]\n            if loss.nelement() > 0:\n                loss = loss.mean()\n        else:\n            loss = loss.mean()\n\n        if (\n            loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        ):  # the fix for computational problems\n            loss.backward()\n            if self.hparams.clip_grad_norm >= 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.optimizer.step()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_loader):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n\n        def is_silent(source, threshold=1e-6):\n            return np.max(np.abs(source[0])) < threshold or np.max(np.abs(source[1])) < threshold\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n            skip_cnt = 0\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n                   \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_sources, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n                    \n     \n                    if not is_silent(targets[0].t().cpu().numpy()) and not is_silent(predictions[0].t().detach().cpu().numpy()) and not is_silent(mixture_signal[0].t().detach().cpu().numpy()):\n                        \n                    \n                        sdr, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            predictions[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_baseline, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            mixture_signal[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_i = sdr.mean() - sdr_baseline.mean()\n    \n                        # Saving on a csv file\n                        row = {\n                            \"snt_id\": snt_id[0],\n                            \"sdr\": sdr.mean(),\n                            \"sdr_i\": sdr_i,\n                            \"si-snr\": -sisnr.item(),\n                            \"si-snr_i\": -sisnr_i.item(),\n                        }\n                        writer.writerow(row)\n    \n                        # Metric Accumulation\n                        all_sdrs.append(sdr.mean())\n                        all_sdrs_i.append(sdr_i.mean())\n                        all_sisnrs.append(-sisnr.item())\n                        all_sisnrs_i.append(-sisnr_i.item())\n    \n                    \n                else:\n                    skip_cnt += 1\n                    print(f\"Warning: skipping silent target, this has happened {skip_cnt} times\")\n                \n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n        if(self.hparams.result_file_path is not None and self.hparams.result_file_path != \"\"):\n            with open(self.hparams.result_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as metrics_csv:\n                writer = csv.DictWriter(metrics_csv, fieldnames=[\"model_name\", \"n_epochs\", \"learning_rate\", \"chunk_size\", \"sample_rate\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"])\n                row = {\n                        \"model_name\": self.hparams.experiment_name,\n                        \"learning_rate\": self.hparams.lr,\n                        \"n_epochs\": self.hparams.N_epochs,\n                        \"chunk_size\":self.hparams.chunk_size,\n                        \"sample_rate\":self.hparams.sample_rate,\n                        \"sdr\": np.array(all_sdrs).mean(),\n                        \"sdr_i\": np.array(all_sdrs_i).mean(),\n                        \"si-snr\": np.array(all_sisnrs).mean(),\n                        \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                    }\n                writer.writerow(row)\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_sources):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n   \n\n    # Brain class initialization\n    separator = Separation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n \n    # Training\n        \n    # Usage with SpeechBrain\n    train_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    valid_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    test_data = MusDBDataset(hparams[\"db_path\"], subset=\"test\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    \n\n    # Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    test_loader = sb.dataio.dataloader.make_dataloader(\n        test_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n    # print(\"STARTING FIT\")\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # # Eval\n    separator.evaluate(test_loader, min_key=\"si-snr\")\n    separator.save_results(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:59:28.191480Z","iopub.execute_input":"2025-04-18T05:59:28.192073Z","iopub.status.idle":"2025-04-18T05:59:28.217992Z","shell.execute_reply.started":"2025-04-18T05:59:28.192047Z","shell.execute_reply":"2025-04-18T05:59:28.217291Z"}},"outputs":[{"name":"stdout","text":"Overwriting dprnn-train.py\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"!python dprnn-train.py dprnn-hparams.yaml --data_folder=db_path\n!torchrun --standalone --nproc_per_node=2 dprnn-train.py dprnn-hparams.yaml --data_folder=db_path --result_file_path={result_file_path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:59:28.219521Z","iopub.execute_input":"2025-04-18T05:59:28.219781Z","iopub.status.idle":"2025-04-18T08:50:58.452329Z","shell.execute_reply.started":"2025-04-18T05:59:28.219762Z","shell.execute_reply":"2025-04-18T08:50:58.451315Z"}},"outputs":[{"name":"stdout","text":"speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/dprnn/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `True`\nspeechbrain.core - Using training precision: `--precision=fp16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 14.6M\n* Total Number of Parameters: 14.6M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\nspeechbrain.utils.epoch_loop - Going into epoch 1\n100%|| 81/81 [08:49<00:00,  6.54s/it, train_loss=12]\n100%|| 13/13 [01:19<00:00,  6.08s/it]\nspeechbrain.utils.train_logger - epoch: 1, lr: 1.50e-04 - train si-snr: 11.95 - valid si-snr: 8.28\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-10-01+00\nspeechbrain.utils.epoch_loop - Going into epoch 2\n100%|| 81/81 [08:39<00:00,  6.42s/it, train_loss=9.65]\n100%|| 13/13 [01:18<00:00,  6.07s/it]\nspeechbrain.utils.train_logger - epoch: 2, lr: 1.50e-04 - train si-snr: 9.65 - valid si-snr: 7.58\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-19-59+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-10-01+00\nspeechbrain.utils.epoch_loop - Going into epoch 3\n100%|| 81/81 [08:41<00:00,  6.44s/it, train_loss=8.82]\n100%|| 13/13 [01:19<00:00,  6.15s/it]\nspeechbrain.utils.train_logger - epoch: 3, lr: 1.50e-04 - train si-snr: 8.82 - valid si-snr: 7.38\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-30-01+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-19-59+00\nspeechbrain.utils.epoch_loop - Going into epoch 4\n100%|| 81/81 [08:41<00:00,  6.44s/it, train_loss=8.27]\n100%|| 13/13 [01:19<00:00,  6.09s/it]\nspeechbrain.utils.train_logger - epoch: 4, lr: 1.50e-04 - train si-snr: 8.27 - valid si-snr: 7.31\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-40-02+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-30-01+00\nspeechbrain.utils.epoch_loop - Going into epoch 5\n100%|| 81/81 [08:40<00:00,  6.42s/it, train_loss=7.85]\n100%|| 13/13 [01:19<00:00,  6.14s/it]\nspeechbrain.utils.train_logger - epoch: 5, lr: 1.50e-04 - train si-snr: 7.85 - valid si-snr: 7.17\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-50-02+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-40-02+00\nspeechbrain.utils.epoch_loop - Going into epoch 6\n100%|| 81/81 [08:42<00:00,  6.46s/it, train_loss=7.39]\n100%|| 13/13 [01:20<00:00,  6.18s/it]\nspeechbrain.utils.train_logger - epoch: 6, lr: 1.50e-04 - train si-snr: 7.39 - valid si-snr: 6.68\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-00-06+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+06-50-02+00\nspeechbrain.utils.epoch_loop - Going into epoch 7\n100%|| 81/81 [08:52<00:00,  6.57s/it, train_loss=6.94]\n100%|| 13/13 [01:20<00:00,  6.19s/it]\nspeechbrain.utils.train_logger - epoch: 7, lr: 1.50e-04 - train si-snr: 6.94 - valid si-snr: 6.26\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-10-19+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-00-06+00\nspeechbrain.utils.epoch_loop - Going into epoch 8\n100%|| 81/81 [08:50<00:00,  6.55s/it, train_loss=6.54]\n100%|| 13/13 [01:21<00:00,  6.24s/it]\nspeechbrain.utils.train_logger - epoch: 8, lr: 1.50e-04 - train si-snr: 6.54 - valid si-snr: 5.90\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-20-31+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-10-19+00\nspeechbrain.utils.epoch_loop - Going into epoch 9\n100%|| 81/81 [08:48<00:00,  6.52s/it, train_loss=6.19]\n100%|| 13/13 [01:20<00:00,  6.19s/it]\nspeechbrain.utils.train_logger - epoch: 9, lr: 1.50e-04 - train si-snr: 6.19 - valid si-snr: 6.00\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-30-40+00\nspeechbrain.utils.epoch_loop - Going into epoch 10\n100%|| 81/81 [08:48<00:00,  6.53s/it, train_loss=5.71]\n100%|| 13/13 [01:20<00:00,  6.20s/it]\nspeechbrain.utils.train_logger - epoch: 10, lr: 1.50e-04 - train si-snr: 5.71 - valid si-snr: 5.27\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-40-50+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-30-40+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-20-31+00\nspeechbrain.utils.epoch_loop - Going into epoch 11\n100%|| 81/81 [08:45<00:00,  6.49s/it, train_loss=5.22]\n100%|| 13/13 [01:19<00:00,  6.14s/it]\nspeechbrain.utils.train_logger - epoch: 11, lr: 1.50e-04 - train si-snr: 5.22 - valid si-snr: 5.35\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-50-56+00\nspeechbrain.utils.epoch_loop - Going into epoch 12\n100%|| 81/81 [08:49<00:00,  6.54s/it, train_loss=4.92]\n100%|| 13/13 [01:19<00:00,  6.10s/it]\nspeechbrain.utils.train_logger - epoch: 12, lr: 1.50e-04 - train si-snr: 4.92 - valid si-snr: 5.24\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-01-05+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-50-56+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+07-40-50+00\nspeechbrain.utils.epoch_loop - Going into epoch 13\n100%|| 81/81 [08:49<00:00,  6.54s/it, train_loss=4.69]\n100%|| 13/13 [01:19<00:00,  6.15s/it]\nspeechbrain.utils.train_logger - epoch: 13, lr: 1.50e-04 - train si-snr: 4.69 - valid si-snr: 4.91\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-11-15+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-01-05+00\nspeechbrain.utils.epoch_loop - Going into epoch 14\n100%|| 81/81 [08:45<00:00,  6.49s/it, train_loss=4.46]\n100%|| 13/13 [01:20<00:00,  6.21s/it]\nspeechbrain.utils.train_logger - epoch: 14, lr: 1.50e-04 - train si-snr: 4.46 - valid si-snr: 5.57\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-21-22+00\nspeechbrain.utils.epoch_loop - Going into epoch 15\n100%|| 81/81 [08:46<00:00,  6.50s/it, train_loss=4.11]\n100%|| 13/13 [01:19<00:00,  6.14s/it]\nspeechbrain.utils.train_logger - epoch: 15, lr: 1.50e-04 - train si-snr: 4.11 - valid si-snr: 5.02\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-31-28+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-21-22+00\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-11-15+00\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n100%|| 44/44 [04:25<00:00,  6.03s/it]\nspeechbrain.utils.train_logger - Epoch loaded: 13 - test si-snr: 12.44\n  0%|                                                    | 0/44 [00:00<?, ?it/s]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  2%|                                           | 1/44 [00:05<04:10,  5.83s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  5%|                                          | 2/44 [00:11<04:06,  5.87s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  7%|                                         | 3/44 [00:17<04:04,  5.97s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  9%|                                        | 4/44 [00:25<04:31,  6.79s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 16%|                                     | 7/44 [00:44<03:57,  6.42s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 23%|                                 | 10/44 [01:02<03:20,  5.89s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 32%|                             | 14/44 [01:30<03:30,  7.02s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 36%|                           | 16/44 [01:41<03:01,  6.49s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 39%|                          | 17/44 [01:48<02:54,  6.47s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 43%|                        | 19/44 [02:00<02:33,  6.13s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 45%|                       | 20/44 [02:06<02:30,  6.27s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 48%|                      | 21/44 [02:13<02:29,  6.49s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 50%|                     | 22/44 [02:22<02:34,  7.01s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 52%|                    | 23/44 [02:27<02:15,  6.47s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 55%|                   | 24/44 [02:31<01:53,  5.68s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 59%|                 | 26/44 [02:42<01:42,  5.67s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 61%|                | 27/44 [02:50<01:46,  6.28s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 66%|              | 29/44 [03:03<01:35,  6.40s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 68%|             | 30/44 [03:10<01:30,  6.45s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 70%|            | 31/44 [03:15<01:19,  6.11s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 73%|           | 32/44 [03:24<01:22,  6.87s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 75%|          | 33/44 [03:30<01:14,  6.76s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 77%|         | 34/44 [03:37<01:09,  6.91s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 80%|        | 35/44 [03:45<01:05,  7.26s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 82%|       | 36/44 [03:52<00:56,  7.00s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 86%|     | 38/44 [04:05<00:39,  6.66s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 89%|     | 39/44 [04:12<00:35,  7.01s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 91%|    | 40/44 [04:19<00:27,  6.92s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 93%|   | 41/44 [04:25<00:19,  6.49s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n100%|| 44/44 [04:42<00:00,  6.42s/it]\nWarning: skipping silent target, this has happened 1 times\n__main__ - Mean SISNR is -2.7040036876996356\n__main__ - Mean SISNRi is 0.41061601241429646\n__main__ - Mean SDR is 2.434394763027718\n__main__ - Mean SDRi is 1.8126941386310313\nW0418 08:40:39.955000 198347 torch/distributed/run.py:793] \nW0418 08:40:39.955000 198347 torch/distributed/run.py:793] *****************************************\nW0418 08:40:39.955000 198347 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0418 08:40:39.955000 198347 torch/distributed/run.py:793] *****************************************\nspeechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/dprnn/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `True`\nspeechbrain.core - Using training precision: `--precision=fp16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 14.6M\n* Total Number of Parameters: 14.6M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-31-28+00\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n/usr/local/lib/python3.11/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/dprnn/1234/save/CKPT+2025-04-18+08-11-15+00\n100%|| 44/44 [04:47<00:00,  6.54s/it]\nspeechbrain.utils.train_logger - Epoch loaded: 13 - test si-snr: 12.44\n  0%|                                                    | 0/44 [00:00<?, ?it/s]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  2%|                                           | 1/44 [00:06<04:35,  6.40s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  5%|                                          | 2/44 [00:12<04:25,  6.31s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  7%|                                         | 3/44 [00:19<04:25,  6.48s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  9%|                                        | 4/44 [00:28<05:04,  7.62s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 16%|                                     | 7/44 [00:48<04:15,  6.92s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 23%|                                 | 10/44 [01:07<03:40,  6.50s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n 25%|                                | 11/44 [01:13<03:24,  6.21s/it]/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 32%|                             | 14/44 [01:38<03:47,  7.58s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 36%|                           | 16/44 [01:50<03:12,  6.88s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 39%|                          | 17/44 [01:56<03:03,  6.81s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 43%|                        | 19/44 [02:10<02:45,  6.61s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 45%|                       | 20/44 [02:17<02:41,  6.73s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n 48%|                      | 21/44 [02:24<02:39,  6.91s/it]/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 48%|                      | 21/44 [02:24<02:40,  6.96s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n 50%|                     | 22/44 [02:33<02:43,  7.45s/it]/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 50%|                     | 22/44 [02:33<02:45,  7.54s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 52%|                    | 23/44 [02:38<02:22,  6.79s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 52%|                    | 23/44 [02:38<02:24,  6.90s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 55%|                   | 24/44 [02:42<01:58,  5.90s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 55%|                   | 24/44 [02:42<02:00,  6.02s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n 57%|                  | 25/44 [02:49<01:57,  6.18s/it]/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 59%|                 | 26/44 [02:55<01:49,  6.08s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 61%|                | 27/44 [03:02<01:53,  6.65s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 61%|                | 27/44 [03:03<01:56,  6.84s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 64%|               | 28/44 [03:10<01:51,  6.99s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 66%|              | 29/44 [03:18<01:43,  6.88s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 68%|             | 30/44 [03:24<01:36,  6.92s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 68%|             | 30/44 [03:25<01:37,  6.96s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 70%|            | 31/44 [03:29<01:25,  6.58s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 70%|            | 31/44 [03:31<01:26,  6.67s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 73%|           | 32/44 [03:40<01:31,  7.65s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 73%|           | 32/44 [03:40<01:30,  7.54s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 75%|          | 33/44 [03:47<01:22,  7.46s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 75%|          | 33/44 [03:47<01:21,  7.45s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 77%|         | 34/44 [03:55<01:16,  7.69s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 77%|         | 34/44 [03:56<01:16,  7.64s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 80%|        | 35/44 [04:03<01:11,  7.92s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 80%|        | 35/44 [04:04<01:11,  7.90s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 82%|       | 36/44 [04:11<01:01,  7.70s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 82%|       | 36/44 [04:11<01:01,  7.69s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 84%|      | 37/44 [04:18<00:52,  7.50s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 86%|     | 38/44 [04:25<00:42,  7.17s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 89%|     | 39/44 [04:32<00:37,  7.49s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 89%|     | 39/44 [04:33<00:37,  7.50s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 91%|    | 40/44 [04:39<00:29,  7.43s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 91%|    | 40/44 [04:40<00:29,  7.47s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 93%|   | 41/44 [04:45<00:20,  6.92s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 93%|   | 41/44 [04:46<00:20,  6.90s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 95%|  | 42/44 [04:52<00:13,  6.91s/it]/kaggle/working/dprnn-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/dprnn-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n100%|| 44/44 [05:04<00:00,  6.92s/it]\nWarning: skipping silent target, this has happened 1 times\n100%|| 44/44 [05:04<00:00,  6.93s/it]\nWarning: skipping silent target, this has happened 1 times\n__main__ - Mean SISNR is -2.7040036876996356\n__main__ - Mean SISNRi is 0.41061601241429646\n__main__ - Mean SDR is 2.434394763027716\n__main__ - Mean SDRi is 1.8126941386310336\n[rank0]:[W418 08:50:56.577992892 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"## Sepformer (Transformer-based Source Separation)\n \nKey Idea\nSepFormer introduces a transformer-based architecture for speech separation, combining the advantages of self-attention mechanisms with dual-path processing. It achieves state-of-the-art performance by modeling both local and global dependencies through stacked transformer layers, outperforming RNN-based approaches like DPRNN in terms of scale and accuracy.\n\nArchitecture\nEncoder-Decoder:\n- Encoder: 1D convolutions to project waveforms into a latent space (similar to DPRNN).\n- Decoder: Transposed convolutions to reconstruct separated sources.\n\nDual-Path Transformer:\n- Intra-Chunk Transformer: Processes local segments (e.g., 25-ms windows) with self-attention.\n- Inter-Chunk Transformer: Captures global dependencies across segments via cross-attention.\n- Mask Estimation: Learns multiplicative masks in the latent space (like DPRNN but with transformer-based attention).\n\nKey Innovations\n- Dual-Path Self-Attention: Replaces RNNs with transformers for better parallelization and long-range modeling.\n- Layer Normalization: Stabilizes training in deep transformer stacks.\n\nOverlap-Add Handling: Uses segmentation/stitching to manage long sequences.\n\nAdvantages\n- Higher SDR: Achieves >20 dB SI-SNRi on WSJ0-2mix (vs. ~18 dB for DPRNN).\n- Parallelizability: No sequential RNN constraints; faster training on GPUs.\n- Scalability: Handles variable-length inputs better than fixed-chunk RNN approaches.\n\nLimitations\n- Memory Usage: Quadratic attention complexity limits maximum sequence length.\n- Compute Cost: Requires more FLOPs than DPRNN for real-time applications.\n\nPersonal Insights\nSepFormer demonstrates that transformers can surpass RNNs in speech separation when combined with dual-path processing. Its hybrid local/global attention design bridges the gap between computational efficiency and separation quality. In order to run the transformer efficiently, we train on two heads only","metadata":{}},{"cell_type":"code","source":"%%file sepformer-hparams.yaml\n# ################################\n# Model: DPRNN for Music vocal separation\n# https://arxiv.org/abs/2010.13154\n# Dataset : MusDB\n# ################################\n# Basic parameters\n# Seed needs to be set at top of yaml, before objects with parameters are made\n\n\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params\n\ndata_folder: !PLACEHOLDER\n\n\nexperiment_name: sepformer\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\nresult_file_path: !PLACEHOLDER\n\n\n# Experiment params\nprecision: fp16 # bf16, fp16 or fp32\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True # Save estimated sources on disk\nsample_rate: 8000\nn_audio_to_save: 10\nchunk_size: 20\n\n####################### Training Parameters ####################################\nN_epochs: 15\nbatch_size: 1\nlr: 0.00015\nclip_grad_norm: 5\nloss_upper_lim: 999999  # this is the upper limit for an acceptable loss\nnum_sources: 2\n\n\n\n# loss thresholding -- this thresholds the training loss\nthreshold_byloss: True\nthreshold: -30\n\n# Encoder parameters\nN_encoder_out: 256\nout_channels: 256\nkernel_size: 16\nkernel_stride: 8\n\n# Dataloader options\n# Set num_workers: 0 on MacOS due to behavior of the multiprocessing library\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 3\n\n\n# Specifying the network\nEncoder: !new:speechbrain.lobes.models.dual_path.Encoder\n    kernel_size: !ref <kernel_size>\n    out_channels: !ref <N_encoder_out>\n\n\nSBtfintra: !new:speechbrain.lobes.models.dual_path.SBTransformerBlock\n    num_layers: 2\n    d_model: !ref <out_channels>\n    nhead: 4\n    d_ffn: 1024\n    dropout: 0\n    use_positional_encoding: True\n    norm_before: True\n\nSBtfinter: !new:speechbrain.lobes.models.dual_path.SBTransformerBlock\n    num_layers: 2\n    d_model: !ref <out_channels>\n    nhead: 4\n    d_ffn: 1024\n    dropout: 0\n    use_positional_encoding: True\n    norm_before: True\n\nMaskNet: !new:speechbrain.lobes.models.dual_path.Dual_Path_Model\n    num_spks: !ref <num_sources>\n    in_channels: !ref <N_encoder_out>\n    out_channels: !ref <out_channels>\n    num_layers: 2\n    K: 250\n    intra_model: !ref <SBtfintra>\n    inter_model: !ref <SBtfinter>\n    norm: ln\n    linear_layer_after_inter_intra: False\n    skip_around_intra: True\n\nDecoder: !new:speechbrain.lobes.models.dual_path.Decoder\n    in_channels: !ref <N_encoder_out>\n    out_channels: 1\n    kernel_size: !ref <kernel_size>\n    stride: !ref <kernel_stride>\n    bias: False\n\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 85\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder: !ref <Encoder>\n    decoder: !ref <Decoder>\n    masknet: !ref <MaskNet>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder: !ref <Encoder>\n        decoder: !ref <Decoder>\n        masknet: !ref <MaskNet>\n        counter: !ref <epoch_counter>\n        lr_scheduler: !ref <lr_scheduler>\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T08:50:58.466368Z","iopub.execute_input":"2025-04-18T08:50:58.466665Z","iopub.status.idle":"2025-04-18T08:50:58.490676Z","shell.execute_reply.started":"2025-04-18T08:50:58.466643Z","shell.execute_reply":"2025-04-18T08:50:58.490117Z"}},"outputs":[{"name":"stdout","text":"Overwriting sepformer-hparams.yaml\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"%%file sepformer-train.py\n#!/usr/bin/env/python3\n\"\"\"\nRecipe for vocal separation using convtasnet\n\"\"\"\n\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nimport pdb\n\nimport musdb\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport speechbrain as sb\nimport psutil\nfrom dataset import MusDBDataset\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.core import AMPConfig\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nimport time\nfrom torch.utils.data import DataLoader\n\nimport musdb\n\n\n# Define training procedure\nclass Separation(sb.Brain):\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix       \n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)      \n\n        # Convert targets to tensor\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_sources)],\n            dim=-1,\n        ).to(self.device)\n        \n        # Separation\n        mix_w = self.hparams.Encoder(mix)\n        est_mask = self.hparams.MaskNet(mix_w)\n        mix_w = torch.stack([mix_w] * self.hparams.num_sources)\n        sep_h = mix_w * est_mask\n        \n        # Decoding\n        est_source = torch.cat(\n            [\n                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n                for i in range(self.hparams.num_sources)\n            ],\n            dim=-1,\n        )\n\n        # pad estimates as per requirement \n        T_origin = mix.size(1)\n        T_est = est_source.size(1)\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, :T_origin, :]\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        return self.hparams.loss(targets, predictions)\n\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n        # print(\"INSIDE FIT BATCH\")\n        \n        amp = AMPConfig.from_name(self.precision)\n        should_step = (self.step % self.grad_accumulation_factor) == 0\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig] #mix_sig, voc_sig, inst_sig\n       \n        \n        predictions, targets = self.compute_forward(\n            mixture, targets, sb.Stage.TRAIN\n        )\n        loss = self.compute_objectives(predictions, targets)\n\n        if self.hparams.threshold_byloss:\n            th = self.hparams.threshold\n            loss = loss[loss > th]\n            if loss.nelement() > 0:\n                loss = loss.mean()\n        else:\n            loss = loss.mean()\n\n        if (\n            loss.nelement() > 0 and loss < self.hparams.loss_upper_lim\n        ):  # the fix for computational problems\n            loss.backward()\n            if self.hparams.clip_grad_norm >= 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.optimizer.step()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id[0], mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id[0], mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def cut_signals(self, mixture, targets):\n        \"\"\"This function selects a random segment of a given length within the mixture.\n        The corresponding targets are selected accordingly\"\"\"\n        randstart = torch.randint(\n            0,\n            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n            (1,),\n        ).item()\n        targets = targets[\n            :, randstart : randstart + self.hparams.training_signal_len, :\n        ]\n        mixture = mixture[\n            :, randstart : randstart + self.hparams.training_signal_len\n        ]\n        return mixture, targets\n\n    def reset_layer_recursively(self, layer):\n        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n        if hasattr(layer, \"reset_parameters\"):\n            layer.reset_parameters()\n        for child_layer in layer.modules():\n            if layer != child_layer:\n                self.reset_layer_recursively(child_layer)\n\n    def save_results(self, test_loader):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n\n        def is_silent(source, threshold=1e-6):\n            return np.max(np.abs(source[0])) < threshold or np.max(np.abs(source[1])) < threshold\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n            skip_cnt = 0\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n                   \n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    sisnr = self.compute_objectives(predictions, targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_sources, dim=-1\n                    )\n                    mixture_signal = mixture_signal.to(targets.device)\n                    sisnr_baseline = self.compute_objectives(\n                        mixture_signal, targets\n                    )\n                    sisnr_i = sisnr - sisnr_baseline\n                    \n     \n                    if not is_silent(targets[0].t().cpu().numpy()) and not is_silent(predictions[0].t().detach().cpu().numpy()) and not is_silent(mixture_signal[0].t().detach().cpu().numpy()):\n                        \n                    \n                        sdr, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            predictions[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_baseline, _, _, _ = bss_eval_sources(\n                            targets[0].t().cpu().numpy(),\n                            mixture_signal[0].t().detach().cpu().numpy(),\n                            compute_permutation=False\n                        )\n    \n                        sdr_i = sdr.mean() - sdr_baseline.mean()\n    \n                        # Saving on a csv file\n                        row = {\n                            \"snt_id\": snt_id[0],\n                            \"sdr\": sdr.mean(),\n                            \"sdr_i\": sdr_i,\n                            \"si-snr\": -sisnr.item(),\n                            \"si-snr_i\": -sisnr_i.item(),\n                        }\n                        writer.writerow(row)\n    \n                        # Metric Accumulation\n                        all_sdrs.append(sdr.mean())\n                        all_sdrs_i.append(sdr_i.mean())\n                        all_sisnrs.append(-sisnr.item())\n                        all_sisnrs_i.append(-sisnr_i.item())\n    \n                    \n                else:\n                    skip_cnt += 1\n                    print(f\"Warning: skipping silent target, this has happened {skip_cnt} times\")\n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n        if(self.hparams.result_file_path is not None and self.hparams.result_file_path != \"\"):\n            with open(self.hparams.result_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as metrics_csv:\n                writer = csv.DictWriter(metrics_csv, fieldnames=[\"model_name\", \"n_epochs\", \"learning_rate\", \"chunk_size\", \"sample_rate\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"])\n                row = {\n                        \"model_name\": self.hparams.experiment_name,\n                        \"learning_rate\": self.hparams.lr,\n                        \"n_epochs\": self.hparams.N_epochs,\n                        \"chunk_size\":self.hparams.chunk_size,\n                        \"sample_rate\":self.hparams.sample_rate,\n                        \"sdr\": np.array(all_sdrs).mean(),\n                        \"sdr_i\": np.array(all_sdrs_i).mean(),\n                        \"si-snr\": np.array(all_sisnrs).mean(),\n                        \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                    }\n                writer.writerow(row)\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        for ns in range(self.hparams.num_sources):\n            # Estimated source\n            signal = predictions[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n   \n\n    # Brain class initialization\n    separator = Separation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n \n    # Training\n        \n    # Usage with SpeechBrain\n    train_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    valid_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    test_data = MusDBDataset(hparams[\"db_path\"], subset=\"test\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    \n\n    # Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    test_loader = sb.dataio.dataloader.make_dataloader(\n        test_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n    # print(\"STARTING FIT\")\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # # Eval\n    separator.evaluate(test_loader, min_key=\"si-snr\")\n    separator.save_results(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T08:50:58.453629Z","iopub.execute_input":"2025-04-18T08:50:58.453916Z","iopub.status.idle":"2025-04-18T08:50:58.465494Z","shell.execute_reply.started":"2025-04-18T08:50:58.453889Z","shell.execute_reply":"2025-04-18T08:50:58.464630Z"}},"outputs":[{"name":"stdout","text":"Overwriting sepformer-train.py\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# !python sepformer-train.py sepformer-hparams.yaml --data_folder=db_path\n!torchrun --standalone --nproc_per_node=2 sepformer-train.py sepformer-hparams.yaml --data_folder={db_path} --result_file_path={result_file_path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T08:50:58.491464Z","iopub.execute_input":"2025-04-18T08:50:58.491688Z","iopub.status.idle":"2025-04-18T11:27:02.137714Z","shell.execute_reply.started":"2025-04-18T08:50:58.491668Z","shell.execute_reply":"2025-04-18T11:27:02.136872Z"}},"outputs":[{"name":"stdout","text":"W0418 08:51:00.272000 203938 torch/distributed/run.py:793] \nW0418 08:51:00.272000 203938 torch/distributed/run.py:793] *****************************************\nW0418 08:51:00.272000 203938 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0418 08:51:00.272000 203938 torch/distributed/run.py:793] *****************************************\nspeechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/sepformer/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `True`\nspeechbrain.core - Using training precision: `--precision=fp16`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - Separation Model Statistics:\n* Total Number of Trainable Parameters: 6.7M\n* Total Number of Parameters: 6.7M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\nspeechbrain.utils.epoch_loop - Going into epoch 1\n100%|| 81/81 [08:21<00:00,  6.20s/it, train_loss=11.8]\n100%|| 13/13 [01:23<00:00,  6.45s/it]\nspeechbrain.utils.train_logger - epoch: 1, lr: 1.50e-04 - train si-snr: 11.80 - valid si-snr: 7.78\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-01-08+00\nspeechbrain.utils.epoch_loop - Going into epoch 2\n100%|| 81/81 [08:13<00:00,  6.09s/it, train_loss=8.96]\n100%|| 13/13 [01:22<00:00,  6.37s/it]\nspeechbrain.utils.train_logger - epoch: 2, lr: 1.50e-04 - train si-snr: 8.96 - valid si-snr: 7.25\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-10-45+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-01-08+00\nspeechbrain.utils.epoch_loop - Going into epoch 3\n100%|| 81/81 [08:17<00:00,  6.14s/it, train_loss=8.24]\n100%|| 13/13 [01:23<00:00,  6.40s/it]\nspeechbrain.utils.train_logger - epoch: 3, lr: 1.50e-04 - train si-snr: 8.24 - valid si-snr: 7.33\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-20-27+00\nspeechbrain.utils.epoch_loop - Going into epoch 4\n100%|| 81/81 [08:17<00:00,  6.14s/it, train_loss=7.7]\n100%|| 13/13 [01:22<00:00,  6.37s/it]\nspeechbrain.utils.train_logger - epoch: 4, lr: 1.50e-04 - train si-snr: 7.70 - valid si-snr: 6.51\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-30-10+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-10-45+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-20-27+00\nspeechbrain.utils.epoch_loop - Going into epoch 5\n100%|| 81/81 [08:15<00:00,  6.12s/it, train_loss=7.32]\n100%|| 13/13 [01:22<00:00,  6.35s/it]\nspeechbrain.utils.train_logger - epoch: 5, lr: 1.50e-04 - train si-snr: 7.32 - valid si-snr: 6.27\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-39-52+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-30-10+00\nspeechbrain.utils.epoch_loop - Going into epoch 6\n100%|| 81/81 [08:15<00:00,  6.11s/it, train_loss=7.03]\n100%|| 13/13 [01:22<00:00,  6.38s/it]\nspeechbrain.utils.train_logger - epoch: 6, lr: 1.50e-04 - train si-snr: 7.03 - valid si-snr: 6.23\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-49-34+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-39-52+00\nspeechbrain.utils.epoch_loop - Going into epoch 7\n100%|| 81/81 [08:16<00:00,  6.13s/it, train_loss=6.79]\n100%|| 13/13 [01:25<00:00,  6.56s/it]\nspeechbrain.utils.train_logger - epoch: 7, lr: 1.50e-04 - train si-snr: 6.79 - valid si-snr: 5.99\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-59-19+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-49-34+00\nspeechbrain.utils.epoch_loop - Going into epoch 8\n100%|| 81/81 [08:24<00:00,  6.22s/it, train_loss=6.48]\n100%|| 13/13 [01:26<00:00,  6.64s/it]\nspeechbrain.utils.train_logger - epoch: 8, lr: 1.50e-04 - train si-snr: 6.48 - valid si-snr: 5.62\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-09-10+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+09-59-19+00\nspeechbrain.utils.epoch_loop - Going into epoch 9\n100%|| 81/81 [08:23<00:00,  6.22s/it, train_loss=6.11]\n100%|| 13/13 [01:23<00:00,  6.46s/it]\nspeechbrain.utils.train_logger - epoch: 9, lr: 1.50e-04 - train si-snr: 6.11 - valid si-snr: 5.39\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-18-58+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-09-10+00\nspeechbrain.utils.epoch_loop - Going into epoch 10\n100%|| 81/81 [08:22<00:00,  6.20s/it, train_loss=5.87]\n100%|| 13/13 [01:24<00:00,  6.53s/it]\nspeechbrain.utils.train_logger - epoch: 10, lr: 1.50e-04 - train si-snr: 5.87 - valid si-snr: 5.47\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-28-45+00\nspeechbrain.utils.epoch_loop - Going into epoch 11\n100%|| 81/81 [08:22<00:00,  6.21s/it, train_loss=5.67]\n100%|| 13/13 [01:24<00:00,  6.53s/it]\nspeechbrain.utils.train_logger - epoch: 11, lr: 1.50e-04 - train si-snr: 5.67 - valid si-snr: 5.25\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-38-33+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-28-45+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-18-58+00\nspeechbrain.utils.epoch_loop - Going into epoch 12\n100%|| 81/81 [08:21<00:00,  6.19s/it, train_loss=5.38]\n100%|| 13/13 [01:23<00:00,  6.46s/it]\nspeechbrain.utils.train_logger - epoch: 12, lr: 1.50e-04 - train si-snr: 5.38 - valid si-snr: 5.43\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-48-18+00\nspeechbrain.utils.epoch_loop - Going into epoch 13\n100%|| 81/81 [08:18<00:00,  6.16s/it, train_loss=5.13]\n100%|| 13/13 [01:22<00:00,  6.35s/it]\nspeechbrain.utils.train_logger - epoch: 13, lr: 1.50e-04 - train si-snr: 5.13 - valid si-snr: 5.11\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-58-00+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-38-33+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-48-18+00\nspeechbrain.utils.epoch_loop - Going into epoch 14\n100%|| 81/81 [08:16<00:00,  6.12s/it, train_loss=4.71]\n100%|| 13/13 [01:23<00:00,  6.45s/it]\nspeechbrain.utils.train_logger - epoch: 14, lr: 1.50e-04 - train si-snr: 4.71 - valid si-snr: 4.91\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+11-07-40+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+10-58-00+00\nspeechbrain.utils.epoch_loop - Going into epoch 15\n100%|| 81/81 [08:25<00:00,  6.24s/it, train_loss=4.3]\n100%|| 13/13 [01:25<00:00,  6.54s/it]\nspeechbrain.utils.train_logger - epoch: 15, lr: 1.50e-04 - train si-snr: 4.30 - valid si-snr: 4.96\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+11-17-31+00\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/sepformer/1234/save/CKPT+2025-04-18+11-07-40+00\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n/usr/local/lib/python3.11/dist-packages/speechbrain/nnet/schedulers.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(path)\n100%|| 44/44 [04:32<00:00,  6.20s/it]\nspeechbrain.utils.train_logger - Epoch loaded: 14 - test si-snr: 11.86\n  0%|                                                    | 0/44 [00:00<?, ?it/s]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  2%|                                           | 1/44 [00:05<04:17,  5.99s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  2%|                                           | 1/44 [00:06<04:27,  6.23s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  5%|                                          | 2/44 [00:11<04:08,  5.92s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  5%|                                          | 2/44 [00:12<04:10,  5.97s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  7%|                                         | 3/44 [00:18<04:13,  6.19s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  7%|                                         | 3/44 [00:18<04:13,  6.18s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  9%|                                        | 4/44 [00:27<04:46,  7.16s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n  9%|                                        | 4/44 [00:26<04:43,  7.10s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 11%|                                       | 5/44 [00:33<04:28,  6.90s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 16%|                                     | 7/44 [00:45<03:59,  6.47s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n 18%|                                    | 8/44 [00:53<04:03,  6.76s/it]/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 23%|                                 | 10/44 [01:04<03:29,  6.17s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 25%|                                | 11/44 [01:09<03:16,  5.95s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 32%|                             | 14/44 [01:33<03:39,  7.33s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 34%|                            | 15/44 [01:38<03:09,  6.54s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 36%|                           | 16/44 [01:45<03:06,  6.65s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 39%|                          | 17/44 [01:52<03:00,  6.70s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 39%|                          | 17/44 [01:52<02:59,  6.66s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 41%|                         | 18/44 [01:59<02:56,  6.79s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 43%|                        | 19/44 [02:04<02:39,  6.37s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 45%|                       | 20/44 [02:11<02:34,  6.43s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 45%|                       | 20/44 [02:11<02:35,  6.49s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 48%|                      | 21/44 [02:18<02:34,  6.72s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 48%|                      | 21/44 [02:18<02:35,  6.76s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 50%|                     | 22/44 [02:27<02:44,  7.46s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 50%|                     | 22/44 [02:27<02:43,  7.43s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 52%|                    | 23/44 [02:32<02:22,  6.80s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 52%|                    | 23/44 [02:32<02:21,  6.72s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 55%|                   | 24/44 [02:36<01:56,  5.85s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 55%|                   | 24/44 [02:36<01:56,  5.82s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 57%|                  | 25/44 [02:43<01:55,  6.07s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 59%|                 | 26/44 [02:49<01:47,  5.97s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 61%|                | 27/44 [02:56<01:51,  6.57s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 61%|                | 27/44 [02:57<01:54,  6.75s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 64%|               | 28/44 [03:04<01:49,  6.87s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 66%|              | 29/44 [03:11<01:41,  6.77s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 68%|             | 30/44 [03:17<01:34,  6.78s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 68%|             | 30/44 [03:18<01:34,  6.77s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 70%|            | 31/44 [03:22<01:22,  6.35s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 70%|            | 31/44 [03:23<01:22,  6.36s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 73%|           | 32/44 [03:32<01:27,  7.28s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 73%|           | 32/44 [03:32<01:26,  7.23s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 75%|          | 33/44 [03:39<01:18,  7.17s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 75%|          | 33/44 [03:39<01:19,  7.20s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 77%|         | 34/44 [03:47<01:13,  7.36s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 77%|         | 34/44 [03:47<01:13,  7.35s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 80%|        | 35/44 [03:55<01:08,  7.65s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 80%|        | 35/44 [03:56<01:09,  7.68s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 82%|       | 36/44 [04:02<00:59,  7.45s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 82%|       | 36/44 [04:03<00:59,  7.46s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 84%|      | 37/44 [04:08<00:50,  7.19s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 86%|     | 38/44 [04:16<00:41,  6.94s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 89%|     | 39/44 [04:23<00:36,  7.23s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 89%|     | 39/44 [04:24<00:36,  7.28s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 91%|    | 40/44 [04:29<00:28,  7.10s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 91%|    | 40/44 [04:31<00:29,  7.25s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 93%|   | 41/44 [04:35<00:20,  6.67s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 93%|   | 41/44 [04:36<00:20,  6.71s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n 95%|  | 42/44 [04:42<00:13,  6.64s/it]/kaggle/working/sepformer-train.py:266: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr, _, _, _ = bss_eval_sources(\n/kaggle/working/sepformer-train.py:272: FutureWarning: mir_eval.separation.bss_eval_sources\n\tDeprecated as of mir_eval version 0.8.\n\tIt will be removed in mir_eval version 0.9.\n  sdr_baseline, _, _, _ = bss_eval_sources(\n100%|| 44/44 [04:53<00:00,  6.67s/it]\nWarning: skipping silent target, this has happened 1 times\n__main__ - Mean SISNR is -2.3113911112149554\n__main__ - Mean SISNRi is 0.8032285730044048\n__main__ - Mean SDR is 2.958899161501011\n__main__ - Mean SDRi is 2.3371985371043293\n[rank0]:[W418 11:26:58.110454573 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n100%|| 44/44 [04:54<00:00,  6.69s/it]\nWarning: skipping silent target, this has happened 1 times\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"## Demucs v2 (Music Source Separation in the Waveform Domain)\nKey Idea\nDemucs v2 is a waveform-to-waveform U-Net model designed for music source separation, leveraging bidirectional LSTMs and time-domain convolutions to outperform spectrogram-based methods. It introduces pitch/tempo augmentation and overlap-add reconstruction to reduce artifacts at segment boundaries 102.\n\nArchitecture\n- Encoder: Strided 1D convolutions to compress the waveform into latent features.\n- Decoder: Transposed convolutions to reconstruct separated stems (drums, bass, vocals, etc.) 10.\n- Bidirectional LSTM: Bridges the encoder and decoder to capture long-term dependencies 10.\n\nKey Innovations in v2\n- DiffQ Quantization: Reduces model size to 120MB without quality loss 10.\n- Overlap-Add: Linear crossfading between segments to minimize boundary artifacts 2.\n- Pitch/Tempo Augmentation: Improves SDR by +0.5 dB via time-stretching and pitch-shifting during training 10.\n\nAdvantages\n- State-of-the-Art SDR: Achieves 6.3 dB on MUSDB (vs. 5.7 dB for Conv-Tasnet) without extra data 10.\n- Natural Sounding Output: Human evaluations (MOS 3.2) favor Demucs over spectrogram methods 10.\n\n\nLimitations\nBleeding Artifacts: Vocals often leak into other stems 10.\n\n\nPersonal Insights\nDemucs v2s waveform-centric approach demonstrates that time-domain models can surpass spectrogram methods when combined with LSTMs and clever augmentation. It also feels empowering to develop a 250 million parameter model. ","metadata":{}},{"cell_type":"code","source":"%%file demucsModels.py\n\nimport torch\nfrom torch import nn\nfrom speechbrain.nnet.CNN import Conv1d, ConvTranspose1d\n# from speechbrain.nnet.activations import GLU\nfrom speechbrain.lobes.models.beats import GLU_Linear\nfrom torch.nn import GLU\nfrom speechbrain.nnet.RNN import LSTM\nfrom speechbrain.nnet.linear import Linear\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = Conv1d(\n            out_channels=out_channels,\n            in_channels=in_channels,\n            kernel_size=8,\n            stride=4,\n            # default_padding=2,\n            skip_transpose=True,\n        )\n        self.glu_conv = Conv1d(\n            out_channels=2*out_channels,\n            in_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            skip_transpose=True,\n        )\n        self.relu = torch.nn.ReLU()\n        self.glu = GLU(dim=1)\n\n    def forward(self, x):\n        # print(x.size())\n        x = self.relu(self.conv(x))\n        # print(\"#########checking encoder\")\n        # print(x.size())\n        # print(self.glu_conv(x).size())\n        # print(\"#########encoder ended\")\n        x = self.glu(self.glu_conv(x))\n        # print(x.size())\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.glu_conv = Conv1d(\n            out_channels=2*in_channels,\n            in_channels=in_channels,\n            kernel_size=1,\n            stride=1,\n            skip_transpose=True,\n        )\n        self.conv_tr = ConvTranspose1d(\n            out_channels=out_channels,\n            in_channels=in_channels,  # After GLU split\n            kernel_size=8,\n            stride=4,\n            # padding=2,\n            # output_padding=2,\n            skip_transpose=True,\n        )\n        self.glu = GLU(dim=1)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x, skip=None):\n\n        if skip is not None:\n\n            # T changed after conv1d in encoder, fix it here\n            T_x = x.size(-1)\n            T_skip = skip.size(-1)\n\n            # # Case 1: Decoder output is longer\n            # if T_skip > T_x:\n            #     # Center-trim decoder output\n            #     start = (T_skip - T_x) // 2\n            #     skip = skip[..., start : start + T_x]\n\n            # # Case 2: Skip is longer\n            # elif T_skip < T_x:\n            #     # Center-pad decoder output\n            #     pad = T_x - T_skip\n            #     skip = nn.functional.pad(skip, (pad // 2, pad - pad // 2))\n\n            if T_skip > T_x:\n                # Trim from the end of 'skip' to match 'x'\n                skip = skip[..., :T_x]  # Keeps the first T_x samples\n\n            # Case 2: Skip is shorter than target (x)\n            elif T_skip < T_x:\n                # Pad zeros at the end of 'skip' to match 'x'\n                pad = T_x - T_skip\n                skip = nn.functional.pad(skip, (0, pad))\n\n\n            x = x + skip\n\n\n\n\n        x = self.glu(self.glu_conv(x))\n\n        x = self.relu(self.conv_tr(x))\n\n        return x\n\n\n\n\nclass SourceSeparator(nn.Module):\n    def __init__(self, in_channels, out_channels=2, num_sources=4):\n        \"\"\"\n        Args:\n            C_in: Input channels from last decoder (typically 8)\n            C_out: Output channels per source (2 for stereo)\n            num_sources: Number of sources to separate (e.g. 4 for vocals, drums, bass, other)\n        \"\"\"\n        super().__init__()\n        # Final linear layer (no activation)\n        self.output_proj = Linear(\n            input_size=in_channels,\n            n_neurons=num_sources * out_channels,  # 4 sources * 2 channels = 8\n            bias=True\n        )\n        self.num_sources = num_sources\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        \"\"\"\n        Input: [batch, C_in, time]\n        Output: [batch, num_sources, out_channels, time]\n        \"\"\"\n        # Permute to [batch, time, features]\n        # print(x.size())\n        x = x.permute(0, 2, 1)\n\n        # Project to source waveforms\n        x = self.output_proj(x)  # [batch, time, num_sources*out_channels]\n        # print(x.size())\n\n        # Reshape to separated sources\n        # x = x.view(x.size(0), -1, self.num_sources, self.out_channels)\n        x = x.permute(0, 2, 1)\n        # print(x.size())\n        # Return as [batch, sources, time]\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:09:34.354766Z","iopub.execute_input":"2025-04-18T12:09:34.355520Z","iopub.status.idle":"2025-04-18T12:09:34.362373Z","shell.execute_reply.started":"2025-04-18T12:09:34.355493Z","shell.execute_reply":"2025-04-18T12:09:34.361701Z"}},"outputs":[{"name":"stdout","text":"Writing demucsModels.py\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"%%file demucs-hparams.yaml\n\n# ################################\n# Model: Demucs for source separation\n# https://hal.science/hal-02379796/document\n# Dataset : Musdb\n# ################################\n# Basic parameters\nseed: 1234\n__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n\n# Data params (unchanged from DPRNN)\ndata_folder: !PLACEHOLDER\n\nexperiment_name: demucs\noutput_folder: !ref /kaggle/working/results/<experiment_name>/<seed>\ntrain_log: !ref <output_folder>/train_log.txt\nsave_folder: !ref <output_folder>/save\ntrain_data: !ref <output_folder>/train.json\nvalid_data: !ref <output_folder>/valid.json\ntest_data: !ref <output_folder>/test.json\nskip_prep: False\ndb_path: '/kaggle/input/musdb18-music-source-separation-dataset'\n\nresult_file_path: !PLACEHOLDER\n\n# Experiment params\nprecision: fp32\nnum_sources: 2\n\ninstrumental_classification: False\nnoprogressbar: False\nsave_audio: True\nsample_rate: 8000\nn_audio_to_save: 10\nchunk_size: 30\n\n####################### Training Parameters ####################################\n\nN_epochs: 15\nbatch_size: 1\nlr: 0.0005\nclip_grad_norm: 5\nloss_upper_lim: 999999\nlimit_training_signal_len: False\ntraining_signal_len: 32000000\n\n\n# Data augmentation (unchanged)\nuse_wavedrop: False\nuse_rand_shift: False\nmin_shift: -8000\nmax_shift: 8000\n\n\n# Frequency/time drop (unchanged)\ndrop_freq: !new:speechbrain.augment.time_domain.DropFreq\n    drop_freq_low: 0\n    drop_freq_high: 1\n    drop_freq_count_low: 1\n    drop_freq_count_high: 3\n    drop_freq_width: 0.05\n\ndrop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n    drop_length_low: 1000\n    drop_length_high: 2000\n    drop_count_low: 1\n    drop_count_high: 5\n\nthreshold_byloss: True\nthreshold: -30\n\n################ Demucs Specific Parameters #############################\n## for Demucs V3/4\n# # Fourier Transform Parameters\n# n_fft: 2048\n# hop_length: 512\n\n\nkernel_size: 16\n# kernel_stride: 8\n\n# Dataloader options (unchanged)\ndataloader_opts:\n    batch_size: !ref <batch_size>\n    num_workers: 3\n\n######################## Network Definition ####################################\n\n\nEncoder1: !new:demucsModels.EncoderBlock\n    in_channels: 1\n    # kernel_size: !ref <kernel_size>\n    out_channels: 64\n\n\nEncoder2: !new:demucsModels.EncoderBlock\n    in_channels: 64\n    out_channels: 128\n\n\nEncoder3: !new:demucsModels.EncoderBlock\n    in_channels: 128\n    out_channels: 256\n\n\nEncoder4: !new:demucsModels.EncoderBlock\n    in_channels: 256\n    out_channels: 512\n\n\nEncoder5: !new:demucsModels.EncoderBlock\n    in_channels: 512\n    out_channels: 1024\n\n\nEncoder6: !new:demucsModels.EncoderBlock\n    in_channels: 1024\n    out_channels: 2048\n\n\n\n\nDecoder6: !new:demucsModels.DecoderBlock\n    in_channels: 2048\n    out_channels: 1024\n\n\nDecoder5: !new:demucsModels.DecoderBlock\n    in_channels: 1024\n    out_channels: 512\n\n\nDecoder4: !new:demucsModels.DecoderBlock\n    in_channels: 512\n    out_channels: 256\n\n\nDecoder3: !new:demucsModels.DecoderBlock\n    in_channels: 256\n    out_channels: 128\n\n\nDecoder2: !new:demucsModels.DecoderBlock\n    in_channels: 128\n    out_channels: 64\n\n\nDecoder1: !new:demucsModels.DecoderBlock\n    in_channels: 64\n    out_channels: 4\n\n\nLinear: !new:speechbrain.nnet.linear.Linear\n    input_size: 4096\n    bias: False\n    n_neurons: 2048\n\nBiLSTM: !new:speechbrain.nnet.RNN.LSTM\n    hidden_size: 2048\n    input_size: 2048\n    num_layers: 2\n    bidirectional: True\n    # batch_first: True\n\nLinearSeparator: !new:demucsModels.SourceSeparator\n    in_channels: 4\n    out_channels: 1\n    num_sources: !ref <num_sources>\n\n\n######################## Remaining Config ######################################\noptimizer: !name:torch.optim.Adam\n    lr: !ref <lr>\n    weight_decay: 0\n\n# loss: !name:speechbrain.nnet.losses.mse_loss\nloss: !name:speechbrain.nnet.losses.get_si_snr_with_pitwrapper\n# loss: !name:speechbrain.nnet.losses.l1_loss\n\n\nlr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau\n    factor: 0.5\n    patience: 2\n    dont_halve_until_epoch: 50\n\nepoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n    limit: !ref <N_epochs>\n\nmodules:\n    encoder1: !ref <Encoder1>\n    encoder2: !ref <Encoder2>\n    encoder3: !ref <Encoder3>\n    encoder4: !ref <Encoder4>\n    encoder5: !ref <Encoder5>\n    encoder6: !ref <Encoder6>\n    lstm: !ref <BiLSTM>\n    linear: !ref <Linear>\n    decoder6: !ref <Decoder6>\n    decoder5: !ref <Decoder5>\n    decoder4: !ref <Decoder4>\n    decoder3: !ref <Decoder3>\n    decoder2: !ref <Decoder2>\n    decoder1: !ref <Decoder1>\n    linearSeparator: !ref <LinearSeparator>\n\ncheckpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n    checkpoints_dir: !ref <save_folder>\n    recoverables:\n        encoder1: !ref <Encoder1>\n        encoder2: !ref <Encoder2>\n        encoder3: !ref <Encoder3>\n        encoder4: !ref <Encoder4>\n        encoder5: !ref <Encoder5>\n        encoder6: !ref <Encoder6>\n        lstm: !ref <BiLSTM>\n        linear: !ref <Linear>\n        decoder6: !ref <Decoder6>\n        decoder5: !ref <Decoder5>\n        decoder4: !ref <Decoder4>\n        decoder3: !ref <Decoder3>\n        decoder2: !ref <Decoder2>\n        decoder1: !ref <Decoder1>\n        linearSeparator: !ref <LinearSeparator>\n        counter: !ref <epoch_counter>\n\n\ntrain_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n    save_file: !ref <train_log>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:32:51.829869Z","iopub.execute_input":"2025-04-18T13:32:51.830526Z","iopub.status.idle":"2025-04-18T13:32:51.838383Z","shell.execute_reply.started":"2025-04-18T13:32:51.830492Z","shell.execute_reply":"2025-04-18T13:32:51.837820Z"}},"outputs":[{"name":"stdout","text":"Overwriting demucs-hparams.yaml\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"%%file demucs-train.py\n#!/usr/bin/env/python3\n\"\"\"Recipe for training a neural speech separation system on the wsjmix\ndataset. The system employs an encoder, a decoder, and a masking network.\n\nTo run this recipe, do the following:\n> python train.py hparams/sepformer.yaml\n> python train.py hparams/dualpath_rnn.yaml\n> python train.py hparams/convtasnet.yaml\n\nThe experiment file is flexible enough to support different neural\nnetworks. By properly changing the parameter files, you can try\ndifferent architectures. The script supports both wsj2mix and\nwsj3mix.\n\n\nAuthors\n * Cem Subakan 2020\n * Mirco Ravanelli 2020\n * Samuele Cornell 2020\n * Mirko Bronzi 2020\n * Jianyuan Zhong 2020\n\"\"\"\n## CHECKPOINT\nimport csv\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\n\n\n\nimport speechbrain as sb\nimport speechbrain.nnet.schedulers as schedulers\nfrom speechbrain.utils.distributed import run_on_main\nfrom speechbrain.utils.logger import get_logger\nfrom speechbrain.nnet.CNN import Conv1d, ConvTranspose1d\n# from speechbrain.nnet.activations import GLU\nfrom speechbrain.lobes.models.beats import GLU_Linear\nfrom torch.nn import GLU\nfrom speechbrain.nnet.RNN import LSTM\nfrom speechbrain.nnet.linear import Linear\nfrom demucsModels import EncoderBlock, DecoderBlock\nfrom speechbrain.nnet.losses import get_si_snr_with_pitwrapper\nfrom dataset import MusDBDataset\n\nfrom torch.utils.data import Dataset\nimport musdb\nnp.float_ = np.float64\n\n\n\n\n\n# Define training procedure\nclass DemucsSeparation(sb.Brain):\n    # def on_fit_start(self):\n\n\n    def compute_forward(self, mix, targets, stage, noise=None):\n        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n\n        # Unpack lists and put tensors in the right device\n        mix, mix_lens = mix\n        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)\n\n        # Convert targets to tensor\n        # print([targets[i][0].size() for i in range(self.hparams.num_sources)])\n        targets = torch.cat(\n            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_sources)],\n            dim=-1,\n        ).to(self.device)\n        \n        mix=mix.unsqueeze(1)\n        targets=targets.permute(0,2,1)\n       \n\n        mix_enc_1 = self.modules.encoder1(mix)\n        mix_enc_2 = self.modules.encoder2(mix_enc_1)\n        mix_enc_3 = self.modules.encoder3(mix_enc_2)\n        mix_enc_4 = self.modules.encoder4(mix_enc_3)\n        mix_enc_5 = self.modules.encoder5(mix_enc_4)\n        mix_enc_6 = self.modules.encoder6(mix_enc_5)\n\n        lstm_in = mix_enc_6.permute(0,2,1)\n        lstm_out, _ = self.modules.lstm(lstm_in) # outputs both -- outputs as well as hidden states -- we dont need hidden states\n        lin_out = self.modules.linear(lstm_out)\n        lin_out = lin_out.permute(0,2,1)\n\n        mix_dec_6 = self.modules.decoder6(lin_out, skip=mix_enc_6)\n        mix_dec_5 = self.modules.decoder5(mix_dec_6, skip=mix_enc_5)\n        mix_dec_4 = self.modules.decoder4(mix_dec_5, skip=mix_enc_4)\n        mix_dec_3 = self.modules.decoder3(mix_dec_4, skip=mix_enc_3)\n        mix_dec_2 = self.modules.decoder2(mix_dec_3, skip=mix_enc_2)\n        mix_dec_1 = self.modules.decoder1(mix_dec_2, skip=mix_enc_1)\n\n        mix_out = self.modules.linearSeparator(mix_dec_1)\n        est_source = mix_out\n\n\n\n        # T changed after conv1d in encoder, fix it here\n        T_origin = targets.size(2)\n        T_est = est_source.size(2)\n\n        if T_origin > T_est:\n            est_source = F.pad(est_source, (0, 0, T_origin - T_est))\n        else:\n            est_source = est_source[:, : , :T_origin]\n\n        return est_source, targets\n\n    def compute_objectives(self, predictions, targets):\n        \"\"\"Computes the sinr loss\"\"\"\n        # print(\"comp obj\")\n        targets = targets.permute(0,2,1)\n        predictions = predictions.permute(0,2,1)\n        return self.hparams.loss(targets, predictions) # for pitwrapper\n## CHECKPOINT\n    def fit_batch(self, batch):\n        \"\"\"Trains one batch\"\"\"\n\n        # Unpacking batch list\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n        with self.training_ctx:\n            predictions, targets = self.compute_forward(\n                mixture, targets, sb.Stage.TRAIN\n            )\n\n            loss = self.compute_objectives(predictions, targets)\n\n            # hard threshold the easy dataitems\n            if self.hparams.threshold_byloss:\n                th = self.hparams.threshold\n                loss = loss[loss > th]\n                if loss.nelement() > 0:\n                    loss = loss.mean()\n            else:\n                loss = loss.mean()\n\n        if loss.nelement() > 0 and loss < self.hparams.loss_upper_lim:\n            self.scaler.scale(loss).backward()\n            if self.hparams.clip_grad_norm >= 0:\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(\n                    self.modules.parameters(),\n                    self.hparams.clip_grad_norm,\n                )\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n        else:\n            self.nonfinite_count += 1\n            logger.info(\n                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n                    self.nonfinite_count\n                )\n            )\n            loss.data = torch.tensor(0.0).to(self.device)\n        self.optimizer.zero_grad()\n\n        return loss.detach().cpu()\n\n    def evaluate_batch(self, batch, stage):\n        \"\"\"Computations needed for validation/test batches\"\"\"\n        snt_id = batch.track_id\n        mixture = batch.mix_sig\n        targets = [batch.voc_sig, batch.inst_sig]\n\n\n        with torch.no_grad():\n            predictions, targets = self.compute_forward(mixture, targets, stage)\n            loss = self.compute_objectives(predictions, targets)\n\n        # Manage audio file saving\n        if stage == sb.Stage.TEST and self.hparams.save_audio:\n            if hasattr(self.hparams, \"n_audio_to_save\"):\n                if self.hparams.n_audio_to_save > 0:\n                    self.save_audio(snt_id, mixture, targets, predictions)\n                    self.hparams.n_audio_to_save += -1\n            else:\n                self.save_audio(snt_id, mixture, targets, predictions)\n\n        return loss.mean().detach()\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        \"\"\"Gets called at the end of a epoch.\"\"\"\n        # Compute/store important stats\n        stage_stats = {\"si-snr\": stage_loss}\n        if stage == sb.Stage.TRAIN:\n            self.train_stats = stage_stats\n\n        # Perform end-of-iteration things, like annealing, logging, etc.\n        if stage == sb.Stage.VALID:\n            # Learning rate annealing\n            if isinstance(\n                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n            ):\n                current_lr, next_lr = self.hparams.lr_scheduler(\n                    [self.optimizer], epoch, stage_loss\n                )\n                schedulers.update_learning_rate(self.optimizer, next_lr)\n            else:\n                # if we do not use the reducelronplateau, we do not change the lr\n                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n                train_stats=self.train_stats,\n                valid_stats=stage_stats,\n            )\n            self.checkpointer.save_and_keep_only(\n                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n            )\n        elif stage == sb.Stage.TEST:\n            self.hparams.train_logger.log_stats(\n                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n                test_stats=stage_stats,\n            )\n\n\n    def save_results(self, test_loader):\n        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n        them into a csv file\"\"\"\n\n        # This package is required for SDR computation\n        from mir_eval.separation import bss_eval_sources\n\n        # Create folders where to store audio\n        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n\n        # Variable init\n        all_sdrs = []\n        all_sdrs_i = []\n        all_sisnrs = []\n        all_sisnrs_i = []\n        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n\n        # test_loader = sb.dataio.dataloader.make_dataloader(\n        #     test_data, **self.hparams.dataloader_opts\n        # )\n\n        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n            writer.writeheader()\n\n            # Loop over all test sentence\n            with tqdm(test_loader, dynamic_ncols=True) as t:\n                for i, batch in enumerate(t):\n                    # Apply Separation\n                    mixture, mix_len = batch.mix_sig\n                    snt_id = batch.track_id\n                    targets = [batch.voc_sig, batch.inst_sig]\n\n\n                    with torch.no_grad():\n                        predictions, targets = self.compute_forward(\n                            batch.mix_sig, targets, sb.Stage.TEST\n                        )\n\n                    # Compute SI-SNR\n                    # predictions = predictions.permute(0,2,1)\n                    # targets = targets.permute(0,2,1)\n                    print(predictions.size())\n                    print(targets.size())\n                    # sisnr = self.compute_objectives(predictions, targets)\n                    sisnr = get_si_snr_with_pitwrapper(predictions,targets)\n\n                    # Compute SI-SNR improvement\n                    mixture_signal = torch.stack(\n                        [mixture] * self.hparams.num_sources, dim=-1\n                    )\n                    #.permute(0,2,1)\n                    print(\"---------------------\")\n                    # print(mixture.size())\n                    print(mixture_signal.size())\n\n                    mixture_signal = mixture_signal.to(targets.device)\n                    # sisnr_baseline = self.compute_objectives(\n                    #     mixture_signal, targets\n                    # )\n                    sisnr_baseline = get_si_snr_with_pitwrapper(mixture_signal, targets)\n                    sisnr_i = sisnr - sisnr_baseline\n\n                    # Compute SDR\n                    sdr, _, _, _ = bss_eval_sources(\n                        targets[0].mean(dim=1).t().cpu().numpy(),\n                        predictions[0].mean(dim=1).t().detach().cpu().numpy(),\n                    )\n\n                    sdr_baseline, _, _, _ = bss_eval_sources(\n                        targets[0].mean(dim=1).t().cpu().numpy(),\n                        mixture_signal[0].mean(dim=1).t().detach().cpu().numpy(),\n                    )\n\n                    sdr_i = sdr.mean() - sdr_baseline.mean()\n\n                    # Saving on a csv file\n                    row = {\n                        \"snt_id\": snt_id[0],\n                        \"sdr\": sdr.mean(),\n                        \"sdr_i\": sdr_i,\n                        \"si-snr\": -sisnr.item(),\n                        \"si-snr_i\": -sisnr_i.item(),\n                    }\n                    writer.writerow(row)\n\n                    # Metric Accumulation\n                    all_sdrs.append(sdr.mean())\n                    all_sdrs_i.append(sdr_i.mean())\n                    all_sisnrs.append(-sisnr.item())\n                    all_sisnrs_i.append(-sisnr_i.item())\n\n                row = {\n                    \"snt_id\": \"avg\",\n                    \"sdr\": np.array(all_sdrs).mean(),\n                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n                    \"si-snr\": np.array(all_sisnrs).mean(),\n                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                }\n                writer.writerow(row)\n\n        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n        if(self.hparams.result_file_path is not None and self.hparams.result_file_path != \"\"):\n            with open(self.hparams.result_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as metrics_csv:\n                writer = csv.DictWriter(metrics_csv, fieldnames=[\"model_name\", \"n_epochs\", \"learning_rate\", \"chunk_size\", \"sample_rate\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"])\n                row = {\n                        \"model_name\": self.hparams.experiment_name,\n                        \"learning_rate\": self.hparams.lr,\n                        \"n_epochs\": self.hparams.N_epochs,\n                        \"chunk_size\":self.hparams.chunk_size,\n                        \"sample_rate\":self.hparams.sample_rate,\n                        \"sdr\": np.array(all_sdrs).mean(),\n                        \"sdr_i\": np.array(all_sdrs_i).mean(),\n                        \"si-snr\": np.array(all_sisnrs).mean(),\n                        \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n                    }\n                writer.writerow(row)\n\n    def save_audio(self, snt_id, mixture, targets, predictions):\n        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n\n        # Create output folder\n\n        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n        targets = targets.permute(0,2,1)\n        predictions = predictions.permute(0,2,1)\n        for ns in range(self.hparams.num_sources):\n            # Estimated source\\\n            \n            print(\"------- in here --------------\")\n            print(predictions.size())\n            signal = predictions[0, :, ns]\n            print(signal.size())\n            signal = signal / signal.abs().max()\n            # signal = signal / signal.abs().max(dim=1, keepdim=True)[0]\n            print(signal.size())\n            save_file = os.path.join(\n                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n            # Original source\n            signal = targets[0, :, ns]\n            signal = signal / signal.abs().max()\n            save_file = os.path.join(\n                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n            )\n            torchaudio.save(\n                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n            )\n\n        # Mixture\n        signal = mixture[0][0, :]\n        signal = signal / signal.abs().max()\n        print(signal.size())\n        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n        torchaudio.save(\n            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n        )\n\n\n\nif __name__ == \"__main__\":\n    # Load hyperparameters file with command-line overrides\n    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n    with open(hparams_file, encoding=\"utf-8\") as fin:\n        hparams = load_hyperpyyaml(fin, overrides)\n\n    # Initialize ddp (useful only for multi-GPU DDP training)\n    sb.utils.distributed.ddp_init_group(run_opts)\n\n    # Logger info\n    logger = get_logger(__name__)\n\n    # Create experiment directory\n    sb.create_experiment_directory(\n        experiment_directory=hparams[\"output_folder\"],\n        hyperparams_to_save=hparams_file,\n        overrides=overrides,\n    )\n\n    # Update precision to bf16 if the device is CPU and precision is fp16\n    if run_opts.get(\"device\") == \"cpu\" and hparams.get(\"precision\") == \"fp16\":\n        hparams[\"precision\"] = \"bf16\"\n\n\n\n        # Usage with SpeechBrain\n    train_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"train\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    valid_data = MusDBDataset(hparams[\"db_path\"], subset=\"train\", split=\"valid\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n    test_data = MusDBDataset(hparams[\"db_path\"], subset=\"test\", target_sr=hparams[\"sample_rate\"], chunk_size=hparams[\"chunk_size\"])\n\n\n    # Create DataLoader\n    train_loader = sb.dataio.dataloader.make_dataloader(\n        train_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    valid_loader = sb.dataio.dataloader.make_dataloader(\n        valid_data,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n    test_loader = sb.dataio.dataloader.make_dataloader(\n        test_data,\n        batch_size=hparams[\"batch_size\"],\n        collate_fn=sb.dataio.batch.PaddedBatch  # Handles variable lengths\n    )\n\n\n    # Brain class initialization\n    separator = DemucsSeparation(\n        modules=hparams[\"modules\"],\n        opt_class=hparams[\"optimizer\"],\n        hparams=hparams,\n        run_opts=run_opts,\n        checkpointer=hparams[\"checkpointer\"],\n    )\n\n\n    # Training\n    separator.fit(\n        separator.hparams.epoch_counter,\n        train_loader,\n        valid_loader,\n        train_loader_kwargs=hparams[\"dataloader_opts\"],\n        valid_loader_kwargs=hparams[\"dataloader_opts\"],\n    )\n\n    # Eval\n    separator.evaluate(test_loader, min_key=\"si-snr\")\n    separator.save_results(test_loader)\n    ## CHECKPOINT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:32:55.810363Z","iopub.execute_input":"2025-04-18T13:32:55.811068Z","iopub.status.idle":"2025-04-18T13:32:55.821190Z","shell.execute_reply.started":"2025-04-18T13:32:55.811045Z","shell.execute_reply":"2025-04-18T13:32:55.820612Z"}},"outputs":[{"name":"stdout","text":"Overwriting demucs-train.py\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# !python3 demucs-train.py demucs-hparams.yaml --data_folder=db_path --device \"cpu\"\n!torchrun --standalone --nproc_per_node=2 demucs-train.py demucs-hparams.yaml --data_folder=db_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:32:57.150385Z","iopub.execute_input":"2025-04-18T13:32:57.151119Z","iopub.status.idle":"2025-04-18T14:30:34.547035Z","shell.execute_reply.started":"2025-04-18T13:32:57.151096Z","shell.execute_reply":"2025-04-18T14:30:34.545974Z"}},"outputs":[{"name":"stdout","text":"W0418 13:32:59.199000 325359 torch/distributed/run.py:793] \nW0418 13:32:59.199000 325359 torch/distributed/run.py:793] *****************************************\nW0418 13:32:59.199000 325359 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0418 13:32:59.199000 325359 torch/distributed/run.py:793] *****************************************\nspeechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nspeechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\nspeechbrain.core - Beginning experiment!\nspeechbrain.core - Experiment folder: /kaggle/working/results/demucs/1234\nspeechbrain.core - Info: precision arg from hparam file is used\nspeechbrain.core - Info: noprogressbar arg from hparam file is used\nspeechbrain.core - Gradscaler enabled: `False`\nspeechbrain.core - Using training precision: `--precision=fp32`\nspeechbrain.core - Using evaluation precision: `--eval_precision=fp32`\nspeechbrain.core - DemucsSeparation Model Statistics:\n* Total Number of Trainable Parameters: 243.3M\n* Total Number of Parameters: 243.3M\n* Trainable Parameters represent 100.0000% of the total size.\nspeechbrain.utils.checkpoints - Loading a checkpoint from /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+12-57-44+00\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\nspeechbrain.utils.epoch_loop - Going into epoch 6\n100%|| 81/81 [07:54<00:00,  5.86s/it, train_loss=4.47]\n100%|| 13/13 [01:20<00:00,  6.19s/it]\nspeechbrain.utils.train_logger - epoch: 6, lr: 5.00e-04 - train si-snr: 4.47 - valid si-snr: 5.21\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+13-42-53+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+12-57-44+00\nspeechbrain.utils.epoch_loop - Going into epoch 7\n100%|| 81/81 [07:47<00:00,  5.78s/it, train_loss=4.11]\n100%|| 13/13 [01:18<00:00,  6.07s/it]\nspeechbrain.utils.train_logger - epoch: 7, lr: 5.00e-04 - train si-snr: 4.11 - valid si-snr: 4.73\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+13-52-06+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+13-42-53+00\nspeechbrain.utils.epoch_loop - Going into epoch 8\n100%|| 81/81 [07:42<00:00,  5.71s/it, train_loss=4.03]\n100%|| 13/13 [01:18<00:00,  6.07s/it]\nspeechbrain.utils.train_logger - epoch: 8, lr: 5.00e-04 - train si-snr: 4.03 - valid si-snr: 4.70\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+14-01-14+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+13-52-06+00\nspeechbrain.utils.epoch_loop - Going into epoch 9\n100%|| 81/81 [07:47<00:00,  5.77s/it, train_loss=3.7]\n100%|| 13/13 [01:19<00:00,  6.15s/it]\nspeechbrain.utils.train_logger - epoch: 9, lr: 5.00e-04 - train si-snr: 3.70 - valid si-snr: 4.76\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+14-10-28+00\nspeechbrain.utils.epoch_loop - Going into epoch 10\n100%|| 81/81 [07:51<00:00,  5.82s/it, train_loss=3.75]\n100%|| 13/13 [01:19<00:00,  6.12s/it]\nspeechbrain.utils.train_logger - epoch: 10, lr: 5.00e-04 - train si-snr: 3.75 - valid si-snr: 4.81\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+14-19-45+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+14-10-28+00\nspeechbrain.utils.epoch_loop - Going into epoch 11\n100%|| 81/81 [07:47<00:00,  5.77s/it, train_loss=3.69]\n100%|| 13/13 [01:20<00:00,  6.16s/it]\nspeechbrain.utils.train_logger - epoch: 11, lr: 5.00e-04 - train si-snr: 3.69 - valid si-snr: 4.62\nspeechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+14-28-59+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+14-01-14+00\nspeechbrain.utils.checkpoints - Deleted checkpoint in /kaggle/working/results/demucs/1234/save/CKPT+2025-04-18+14-19-45+00\nspeechbrain.utils.epoch_loop - Going into epoch 12\n 19%|                     | 15/81 [01:25<06:10,  5.62s/it, train_loss=5.47]W0418 14:30:33.841000 325359 torch/distributed/elastic/agent/server/api.py:704] Received 2 death signal, shutting down workers\nW0418 14:30:33.842000 325359 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 325362 closing signal SIGINT\nW0418 14:30:33.842000 325359 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 325363 closing signal SIGINT\n 19%|                     | 15/81 [01:27<06:25,  5.84s/it, train_loss=5.47]\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/working/demucs-train.py\", line 445, in <module>\n[rank1]:     separator.fit(\n[rank1]:   File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1585, in fit\n[rank1]:     self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n[rank1]:   File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1410, in _fit_train\n[rank1]:     loss = self.fit_batch(batch)\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/kaggle/working/demucs-train.py\", line 167, in fit_batch\n[rank1]:     return loss.detach().cpu()\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: KeyboardInterrupt\nspeechbrain.core - Exception:\nTraceback (most recent call last):\n  File \"/kaggle/working/demucs-train.py\", line 445, in <module>\n    separator.fit(\n  File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1585, in fit\n    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n  File \"/usr/local/lib/python3.11/dist-packages/speechbrain/core.py\", line 1410, in _fit_train\n    loss = self.fit_batch(batch)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/demucs-train.py\", line 167, in fit_batch\n    return loss.detach().cpu()\n           ^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Result Decleration","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\n\nif result_file_path is not None:\n    df = pd.read_csv(result_file_path)\nelse:\n    # Sample data \n    data = {\n        'model_name': ['demucs'],\n        'batch_size': [1],\n        'n_epochs': [30],\n        'n_samples': [8000],\n        'chunk_size': [30],\n        'snt_id': [-11.316875255705217],\n        'sdr': [-45.328893867866654],\n        'sdr_i': [-19.785856680436567],\n        'si-snr': [-14.166560108011419],\n        'si-snr_i': [-14.166560108011419]\n    }\n    df = pd.DataFrame(data)\n\n\ndef create_table(df):\n    \n    formatted_df = df.copy()\n    float_cols = df.select_dtypes(include=['float64']).columns\n    formatted_df[float_cols] = formatted_df[float_cols].round(2)\n    table = tabulate(formatted_df, headers='keys', tablefmt='grid', showindex=False)\n    return table\n\ndef create_plots(df):\n    plt.figure(figsize=(15, 8))\n    \n    metrics = ['sdr', 'sdr_i', 'si-snr', 'si-snr_i']\n    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n    \n    for i, metric in enumerate(metrics):\n        plt.subplot(2, 2, i+1)\n        plt.bar(df['model_name'], df[metric], color=colors[i])\n        plt.title(f'{metric.upper()} Comparison', fontweight='bold')\n        plt.ylabel('dB')\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        \n        for j, val in enumerate(df[metric]):\n            plt.text(j, val/2, f'{val:.2f}', ha='center', color='white', fontweight='bold')\n    \n    plt.tight_layout()\n    return plt\n\n# Display results\nprint(\"=== Formatted Table ===\")\nprint(create_table(df))\n\nprint(\"\\n=== Generating Plots ===\")\nplot = create_plots(df)\nplot.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:34:11.663223Z","iopub.execute_input":"2025-04-18T15:34:11.663818Z","iopub.status.idle":"2025-04-18T15:34:12.276566Z","shell.execute_reply.started":"2025-04-18T15:34:11.663793Z","shell.execute_reply":"2025-04-18T15:34:12.275840Z"}},"outputs":[{"name":"stdout","text":"=== Formatted Table ===\n+--------------+------------+-----------------+--------------+---------------+-------+---------+----------+------------+\n| model_name   | n_epochs   | learning_rate   | chunk_size   | sample_rate   | sdr   | sdr_i   | si-snr   | si-snr_i   |\n+==============+============+=================+==============+===============+=======+=========+==========+============+\n+--------------+------------+-----------------+--------------+---------------+-------+---------+----------+------------+\n\n=== Generating Plots ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x800 with 4 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":7}]}